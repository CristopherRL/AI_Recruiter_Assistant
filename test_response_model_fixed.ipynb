{"cells":[{"cell_type":"markdown","metadata":{"id":"84FEhWkQv6kz"},"source":["# **Test Models Response Format - FIXED VERSION** üß™\n","\n","Testing notebook with Gemma-specific fixes to avoid CUDA errors.\n","\n","## Purpose\n","- Download and test Llama 3 and Gemma models\n","- Examine response structure and content\n","- Verify output format for guardrail development\n","- **FIXED**: Gemma-specific configuration to avoid CUDA errors\n","\n","## Models to Test\n","- **Meta-Llama-3-8B-Instruct**: For output guardrail validation\n","- **google/gemma-3-4b-it**: Alternative option (FIXED)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"CK6HJqsJv6k2"},"source":["## **Setup and Installation**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHn6yOHBv6k3","executionInfo":{"status":"ok","timestamp":1753618094537,"user_tz":-120,"elapsed":7088,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"618809a1-4edd-4515-b6a9-ba41930b6166"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ All dependencies loaded!\n","üî• CUDA available: False\n"]}],"source":["# Install required packages\n","%pip install transformers>=4.36.0 torch>=2.0.0 bitsandbytes>=0.41.0 accelerate>=0.24.0\n","\n","# Core imports\n","import os, json, torch, time\n","from typing import Dict, Any\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","print(\"‚úÖ All dependencies loaded!\")\n","print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"]},{"cell_type":"markdown","metadata":{"id":"v5g7VThjv6k4"},"source":["## **Setup Paths**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Lh6thCfv6k4","executionInfo":{"status":"ok","timestamp":1753618105750,"user_tz":-120,"elapsed":11211,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"967cc320-9dc2-488a-fd60-efb59c03b729"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ Google Drive mounted\n","üìÅ Project path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\n","üóÇÔ∏è Cache path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/huggingface_cache\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up project paths for caching\n","project_path = \"/content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\"\n","cache_path = f\"{project_path}/huggingface_cache\"\n","\n","# Create directories\n","os.makedirs(cache_path, exist_ok=True)\n","\n","print(f\"‚úÖ Google Drive mounted\")\n","print(f\"üìÅ Project path: {project_path}\")\n","print(f\"üóÇÔ∏è Cache path: {cache_path}\")"]},{"cell_type":"markdown","metadata":{"id":"pQJuNrwnv6k4"},"source":["## **Model Loading Functions**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbfWEhSJv6k5","executionInfo":{"status":"ok","timestamp":1753618105786,"user_tz":-120,"elapsed":34,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"f1d5c729-67e4-4c22-86e3-bbdd366f3363"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Model loading functions ready!\n"]}],"source":["def check_cached_models(cache_path):\n","    \"\"\"Check for cached models\"\"\"\n","    cached_models = []\n","    if not os.path.exists(cache_path):\n","        return cached_models\n","\n","    try:\n","        items = os.listdir(cache_path)\n","        for item in items:\n","            item_path = os.path.join(cache_path, item)\n","            if os.path.isdir(item_path) and not item.startswith('.'):\n","                try:\n","                    contents = os.listdir(item_path)\n","                    has_models_folder = any(f.startswith('models--') for f in contents if os.path.isdir(os.path.join(item_path, f)))\n","                    if has_models_folder:\n","                        cached_models.append(item)\n","                except Exception:\n","                    continue\n","    except Exception:\n","        pass\n","\n","    return cached_models\n","\n","def create_model_cache_dir(model_name: str, cache_path: str) -> str:\n","    \"\"\"Create clean cache directory for a model\"\"\"\n","    model_folder = model_name.replace('/', ' ')\n","    model_cache_dir = os.path.join(cache_path, model_folder)\n","    os.makedirs(model_cache_dir, exist_ok=True)\n","    return model_cache_dir\n","\n","def load_model_with_quantization(model_name: str, cache_path: str):\n","    \"\"\"Load model with cache detection and quantization\"\"\"\n","    cached_models = check_cached_models(cache_path)\n","    model_folder_space = model_name.replace('/', ' ')\n","\n","    if model_folder_space in cached_models:\n","        print(f\"‚ö° Loading {model_name} from cache...\")\n","        model_cache_dir = os.path.join(cache_path, model_folder_space)\n","    else:\n","        print(f\"üì• Downloading {model_name}...\")\n","        model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","\n","    # Quantization for efficiency\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16,\n","        bnb_4bit_use_double_quant=False\n","    )\n","\n","    try:\n","        # Load tokenizer\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n","        if tokenizer.pad_token is None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","        # Load model\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            quantization_config=bnb_config,\n","            device_map=\"auto\",\n","            trust_remote_code=True,\n","            cache_dir=model_cache_dir\n","        )\n","\n","        print(f\"‚úÖ {model_name} loaded successfully!\")\n","        return model, tokenizer\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error loading {model_name}: {str(e)}\")\n","        raise\n","\n","print(\"‚úÖ Model loading functions ready!\")"]},{"cell_type":"markdown","metadata":{"id":"Y4Dhyhzdv6k5"},"source":["## **Test Response Generation Function - FIXED FOR GEMMA**"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCWDOqXJv6k5","executionInfo":{"status":"ok","timestamp":1753618105811,"user_tz":-120,"elapsed":24,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"5c135df2-6723-4302-91ca-022e0f6a10f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Response testing function ready with Gemma fixes!\n"]}],"source":["# Test response generation function with Gemma-specific fixes\n","def test_model_response(model, tokenizer, model_name: str, prompt: str, max_new_tokens: int = 150):\n","    \"\"\"Test model response and show detailed structure\"\"\"\n","    print(f\"\\nüß™ TESTING MODEL: {model_name}\")\n","    print(\"=\" * 60)\n","    print(f\"üìù INPUT PROMPT:\")\n","    print(f'\"{prompt}\"')\n","    print(\"-\" * 60)\n","\n","    try:\n","        # Tokenize input\n","        inputs = tokenizer(\n","            prompt,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=1024\n","        )\n","\n","        print(f\"üìä TOKENIZATION INFO:\")\n","        print(f\"   Input IDs shape: {inputs['input_ids'].shape}\")\n","        print(f\"   Input tokens count: {inputs['input_ids'].shape[1]}\")\n","\n","        # Move to device\n","        device = next(model.parameters()).device\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        print(f\"   Device: {device}\")\n","\n","        # Generate response with model-specific settings\n","        start_time = time.time()\n","\n","        with torch.no_grad():\n","            # Different generation parameters for Gemma vs other models\n","            if \"gemma\" in model_name.lower():\n","                # Gemma-specific settings to avoid CUDA errors\n","                print(f\"   üîß Using Gemma-specific generation settings\")\n","                outputs = model.generate(\n","                    **inputs,\n","                    max_new_tokens=max_new_tokens,\n","                    do_sample=False,  # Use greedy decoding for Gemma\n","                    temperature=1.0,\n","                    top_p=1.0,\n","                    pad_token_id=tokenizer.eos_token_id,\n","                    eos_token_id=tokenizer.eos_token_id,\n","                    return_dict_in_generate=True,\n","                    output_scores=False  # Disable scores for Gemma\n","                )\n","            else:\n","                # Standard settings for other models\n","                print(f\"   üîß Using standard generation settings\")\n","                outputs = model.generate(\n","                    **inputs,\n","                    max_new_tokens=max_new_tokens,\n","                    do_sample=True,\n","                    temperature=0.7,\n","                    top_p=0.9,\n","                    pad_token_id=tokenizer.eos_token_id,\n","                    return_dict_in_generate=True,\n","                    output_scores=True\n","                )\n","\n","        generation_time = time.time() - start_time\n","\n","        print(f\"\\n‚ö° GENERATION INFO:\")\n","        print(f\"   Generation time: {generation_time:.2f}s\")\n","        print(f\"   Output type: {type(outputs)}\")\n","        print(f\"   Output keys: {list(outputs.keys()) if hasattr(outputs, 'keys') else 'No keys (tensor)'}\")\n","\n","        # Extract sequences\n","        if hasattr(outputs, 'sequences'):\n","            sequences = outputs.sequences\n","            print(f\"   Sequences shape: {sequences.shape}\")\n","        else:\n","            sequences = outputs\n","            print(f\"   Sequences shape: {sequences.shape}\")\n","\n","        # Decode full response\n","        full_response = tokenizer.decode(sequences[0], skip_special_tokens=True)\n","\n","        # Extract only new tokens (response without prompt)\n","        generated_response = full_response.replace(prompt, \"\").strip()\n","\n","        print(f\"\\nüí¨ RESPONSE ANALYSIS:\")\n","        print(f\"   Full response length: {len(full_response)} chars\")\n","        print(f\"   Generated response length: {len(generated_response)} chars\")\n","        print(f\"   New tokens generated: {sequences.shape[1] - inputs['input_ids'].shape[1]}\")\n","\n","        print(f\"\\nüìÑ FULL RESPONSE:\")\n","        print(\"<<START_FULL>>\")\n","        print(full_response)\n","        print(\"<<END_FULL>>\")\n","\n","        print(f\"\\n‚ú® GENERATED ONLY:\")\n","        print(\"<<START_GENERATED>>\")\n","        print(generated_response)\n","        print(\"<<END_GENERATED>>\")\n","\n","        # Show token structure for debugging\n","        print(f\"\\nüîç TOKEN ANALYSIS:\")\n","        input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","        output_tokens = tokenizer.convert_ids_to_tokens(sequences[0])\n","        new_tokens = output_tokens[len(input_tokens):]\n","\n","        print(f\"   Input tokens: {input_tokens[:5]}...{input_tokens[-5:]}\")\n","        print(f\"   New tokens: {new_tokens[:10]}\")\n","\n","        # Return structured result\n","        return {\n","            \"model_name\": model_name,\n","            \"prompt\": prompt,\n","            \"full_response\": full_response,\n","            \"generated_response\": generated_response,\n","            \"generation_time\": generation_time,\n","            \"input_tokens_count\": inputs['input_ids'].shape[1],\n","            \"output_tokens_count\": sequences.shape[1],\n","            \"new_tokens_count\": sequences.shape[1] - inputs['input_ids'].shape[1],\n","            \"device\": str(device)\n","        }\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error testing {model_name}: {str(e)}\")\n","        return {\"error\": str(e)}\n","\n","print(\"‚úÖ Response testing function ready with Gemma fixes!\")"]},{"cell_type":"markdown","metadata":{"id":"48NeIc_4v6k6"},"source":["## **Test Prompts**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-u9JuvEv6k6","executionInfo":{"status":"ok","timestamp":1753618105842,"user_tz":-120,"elapsed":20,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"7e9ccaa0-38dd-48cd-db1d-2448f06bee77"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Test prompts defined!\n","üìä Total prompts: 3\n","   1. Simple Validation\n","   2. Simple Correction\n","   3. Basic Question\n"]}],"source":["# Define test prompts to examine response structure\n","test_prompts = [\n","    {\n","        \"name\": \"Simple Validation\",\n","        \"prompt\": \"\"\"You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"]\"\"\"\n","    },\n","    {\n","        \"name\": \"Simple Correction\",\n","        \"prompt\": \"\"\"Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version:\"\"\"\n","    },\n","    {\n","        \"name\": \"Basic Question\",\n","        \"prompt\": \"What is 2+2? Answer briefly.\"\n","    }\n","]\n","\n","print(\"‚úÖ Test prompts defined!\")\n","print(f\"üìä Total prompts: {len(test_prompts)}\")\n","for i, prompt in enumerate(test_prompts, 1):\n","    print(f\"   {i}. {prompt['name']}\")"]},{"cell_type":"markdown","metadata":{"id":"d30_7U5lv6k6"},"source":["## **Test Llama 3 Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["60316703033449968a343d270d549971","4650c68b464b4b39b9cfa6624d98444d","41adbfc7f8904faea1053ccec9145184","7465473647534852a6e1dd9e946cd082","cc942dc2e5fa401bbcc97217a10f106c","170a2b84dce54060ae93c95be7550169","5637cec0427e420c9f8b039482d08a6c","60bb4cfaf02d4ba08ebeee128884e037","16e3046d3b7b4ccf928719f534774492","eab103855743472f854ae873d220972b","87f37876952e47af9046731d772e371c","3d74347dc90640d4b3f9e912579f758b","dc9a637cd1194cff8920a2a4b2b64648","e9ad75e939a4442d9cb9b27c8aef384a","d74c470457af4c88a4a63a9e8cd3dbd7","ccae6437023f435b877ed4ec55440075","58efb4e42b6f41c4872c09ec7a53e539","1ba40b042ba04f20bb4331f05b609b12","01f8762f801c468fa483a7379ab933fb","bdf22f82fb804fc0944a5a92cf793cfd","57337c76b34b424d864e5805f356e81d","d62c88599dd34ba8b3a7ef60921134a9"]},"id":"Q9ksKTlqv6k6","outputId":"c97256fc-4301-43b5-c4e2-a84fc97fb070"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ TESTING LLAMA 3 MODEL\n","================================================================================\n","‚ö° Loading meta-llama/Meta-Llama-3-8B-Instruct from cache...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60316703033449968a343d270d549971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d74347dc90640d4b3f9e912579f758b"}},"metadata":{}}],"source":["# Test Meta-Llama-3-8B-Instruct\n","llama_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","print(f\"üöÄ TESTING LLAMA 3 MODEL\")\n","print(\"=\" * 80)\n","\n","try:\n","    # Load Llama 3 model\n","    llama_model, llama_tokenizer = load_model_with_quantization(llama_model_name, cache_path)\n","\n","    print(f\"\\nüìã MODEL INFO:\")\n","    print(f\"   Model: {llama_model_name}\")\n","    print(f\"   Tokenizer vocab size: {llama_tokenizer.vocab_size}\")\n","    print(f\"   EOS token: {llama_tokenizer.eos_token}\")\n","    print(f\"   PAD token: {llama_tokenizer.pad_token}\")\n","\n","    # Test all prompts\n","    llama_results = []\n","\n","    for prompt_info in test_prompts:\n","        result = test_model_response(\n","            llama_model,\n","            llama_tokenizer,\n","            \"Llama-3-8B\",\n","            prompt_info[\"prompt\"],\n","            max_new_tokens=100\n","        )\n","        result[\"prompt_name\"] = prompt_info[\"name\"]\n","        llama_results.append(result)\n","\n","        print(\"\\n\" + \"=\"*80)\n","\n","    print(f\"\\n‚úÖ LLAMA 3 TESTING COMPLETE!\")\n","    print(f\"üìä Tested {len(llama_results)} prompts\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Failed to test Llama 3: {str(e)}\")\n","    llama_results = []"]},{"cell_type":"markdown","metadata":{"id":"Mrqg5J_4v6k7"},"source":["## **Test Gemma Model - FIXED**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH1ld4Glv6k7"},"outputs":[],"source":["# Test google/gemma-3-4b-it with fixes\n","gemma_model_name = \"google/gemma-3-4b-it\"\n","\n","print(f\"üöÄ TESTING GEMMA MODEL (FIXED VERSION)\")\n","print(\"=\" * 80)\n","\n","try:\n","    # Load Gemma model\n","    gemma_model, gemma_tokenizer = load_model_with_quantization(gemma_model_name, cache_path)\n","\n","    print(f\"\\nüìã MODEL INFO:\")\n","    print(f\"   Model: {gemma_model_name}\")\n","    print(f\"   Tokenizer vocab size: {gemma_tokenizer.vocab_size}\")\n","    print(f\"   EOS token: {gemma_tokenizer.eos_token}\")\n","    print(f\"   PAD token: {gemma_tokenizer.pad_token}\")\n","\n","    # Test all prompts\n","    gemma_results = []\n","\n","    for prompt_info in test_prompts:\n","        result = test_model_response(\n","            gemma_model,\n","            gemma_tokenizer,\n","            \"Gemma-3-4B\",\n","            prompt_info[\"prompt\"],\n","            max_new_tokens=100\n","        )\n","        result[\"prompt_name\"] = prompt_info[\"name\"]\n","        gemma_results.append(result)\n","\n","        print(\"\\n\" + \"=\"*80)\n","\n","    print(f\"\\n‚úÖ GEMMA TESTING COMPLETE!\")\n","    print(f\"üìä Tested {len(gemma_results)} prompts\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Failed to test Gemma: {str(e)}\")\n","    gemma_results = []"]},{"cell_type":"markdown","metadata":{"id":"dqQ3oLwiv6k7"},"source":["## **Compare Model Responses**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLejuctIv6k7"},"outputs":[],"source":["# Compare responses from both models\n","print(\"üìä MODEL COMPARISON SUMMARY\")\n","print(\"=\" * 80)\n","\n","if llama_results and gemma_results:\n","    for i, prompt_info in enumerate(test_prompts):\n","        print(f\"\\nüîç PROMPT: {prompt_info['name']}\")\n","        print(\"-\" * 50)\n","\n","        if i < len(llama_results) and i < len(gemma_results):\n","            llama_result = llama_results[i]\n","            gemma_result = gemma_results[i]\n","\n","            print(f\"ü¶ô LLAMA 3 ({llama_result.get('generation_time', 0):.2f}s):\")\n","            print(f\"   Response: {llama_result.get('generated_response', 'ERROR')[:200]}...\")\n","\n","            print(f\"\\nüíé GEMMA ({gemma_result.get('generation_time', 0):.2f}s):\")\n","            print(f\"   Response: {gemma_result.get('generated_response', 'ERROR')[:200]}...\")\n","\n","            # Performance comparison\n","            if 'generation_time' in llama_result and 'generation_time' in gemma_result:\n","                faster = \"Llama 3\" if llama_result['generation_time'] < gemma_result['generation_time'] else \"Gemma\"\n","                print(f\"\\n‚ö° Faster: {faster}\")\n","\n","        print(\"\\n\" + \"=\"*50)\n","\n","    # Overall performance summary\n","    print(f\"\\nüèÜ OVERALL PERFORMANCE:\")\n","\n","    if llama_results:\n","        avg_llama_time = sum(r.get('generation_time', 0) for r in llama_results) / len(llama_results)\n","        print(f\"   ü¶ô Llama 3 avg time: {avg_llama_time:.2f}s\")\n","\n","    if gemma_results:\n","        avg_gemma_time = sum(r.get('generation_time', 0) for r in gemma_results) / len(gemma_results)\n","        print(f\"   üíé Gemma avg time: {avg_gemma_time:.2f}s\")\n","\n","else:\n","    print(\"‚ùå Could not compare - one or both models failed to load/test\")\n","\n","print(f\"\\n‚úÖ TESTING COMPLETE!\")"]},{"cell_type":"markdown","metadata":{"id":"WHXJ3x9Lv6k7"},"source":["## **Save Results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Adi8cND3v6k7"},"outputs":[],"source":["# Save detailed results to file for analysis\n","import json\n","from datetime import datetime\n","\n","results_data = {\n","    \"timestamp\": datetime.now().isoformat(),\n","    \"test_info\": {\n","        \"purpose\": \"Model response format testing - FIXED VERSION\",\n","        \"models_tested\": [llama_model_name, gemma_model_name],\n","        \"prompts_count\": len(test_prompts),\n","        \"fixes_applied\": [\n","            \"Gemma-specific generation settings\",\n","            \"Greedy decoding for Gemma\",\n","            \"Disabled output_scores for Gemma\",\n","            \"Added eos_token_id for Gemma\"\n","        ]\n","    },\n","    \"llama_results\": llama_results,\n","    \"gemma_results\": gemma_results,\n","    \"test_prompts\": test_prompts\n","}\n","\n","# Save to file\n","results_file = f\"{project_path}/model_response_test_results_FIXED.json\"\n","try:\n","    with open(results_file, 'w', encoding='utf-8') as f:\n","        json.dump(results_data, f, indent=2, ensure_ascii=False)\n","    print(f\"‚úÖ Results saved to: {results_file}\")\n","except Exception as e:\n","    print(f\"‚ùå Could not save results: {str(e)}\")\n","\n","print(f\"\\nüéØ ANALYSIS COMPLETE!\")\n","print(f\"üìä Models tested: {len([r for r in [llama_results, gemma_results] if r])}\")\n","print(f\"üìù Total responses generated: {len(llama_results) + len(gemma_results)}\")\n","print(f\"\\nüí° Use this data to improve the output guardrail implementation!\")\n","print(f\"üîß FIXES APPLIED: Gemma-specific generation settings to avoid CUDA errors\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"60316703033449968a343d270d549971":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4650c68b464b4b39b9cfa6624d98444d","IPY_MODEL_41adbfc7f8904faea1053ccec9145184","IPY_MODEL_7465473647534852a6e1dd9e946cd082"],"layout":"IPY_MODEL_cc942dc2e5fa401bbcc97217a10f106c"}},"4650c68b464b4b39b9cfa6624d98444d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_170a2b84dce54060ae93c95be7550169","placeholder":"‚Äã","style":"IPY_MODEL_5637cec0427e420c9f8b039482d08a6c","value":"Fetching‚Äá4‚Äáfiles:‚Äá‚Äá‚Äá0%"}},"41adbfc7f8904faea1053ccec9145184":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_60bb4cfaf02d4ba08ebeee128884e037","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16e3046d3b7b4ccf928719f534774492","value":0}},"7465473647534852a6e1dd9e946cd082":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eab103855743472f854ae873d220972b","placeholder":"‚Äã","style":"IPY_MODEL_87f37876952e47af9046731d772e371c","value":"‚Äá0/4‚Äá[00:00&lt;?,‚Äá?it/s]"}},"cc942dc2e5fa401bbcc97217a10f106c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"170a2b84dce54060ae93c95be7550169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5637cec0427e420c9f8b039482d08a6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60bb4cfaf02d4ba08ebeee128884e037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16e3046d3b7b4ccf928719f534774492":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eab103855743472f854ae873d220972b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87f37876952e47af9046731d772e371c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d74347dc90640d4b3f9e912579f758b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc9a637cd1194cff8920a2a4b2b64648","IPY_MODEL_e9ad75e939a4442d9cb9b27c8aef384a","IPY_MODEL_d74c470457af4c88a4a63a9e8cd3dbd7"],"layout":"IPY_MODEL_ccae6437023f435b877ed4ec55440075"}},"dc9a637cd1194cff8920a2a4b2b64648":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58efb4e42b6f41c4872c09ec7a53e539","placeholder":"‚Äã","style":"IPY_MODEL_1ba40b042ba04f20bb4331f05b609b12","value":"model-00002-of-00004.safetensors:‚Äá‚Äá62%"}},"e9ad75e939a4442d9cb9b27c8aef384a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_01f8762f801c468fa483a7379ab933fb","max":4999802720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdf22f82fb804fc0944a5a92cf793cfd","value":3123162671}},"d74c470457af4c88a4a63a9e8cd3dbd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57337c76b34b424d864e5805f356e81d","placeholder":"‚Äã","style":"IPY_MODEL_d62c88599dd34ba8b3a7ef60921134a9","value":"‚Äá3.12G/5.00G‚Äá[03:29&lt;00:43,‚Äá43.1MB/s]"}},"ccae6437023f435b877ed4ec55440075":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58efb4e42b6f41c4872c09ec7a53e539":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ba40b042ba04f20bb4331f05b609b12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01f8762f801c468fa483a7379ab933fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdf22f82fb804fc0944a5a92cf793cfd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57337c76b34b424d864e5805f356e81d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d62c88599dd34ba8b3a7ef60921134a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}