{"cells":[{"cell_type":"markdown","metadata":{"id":"B5E0iaKT7cPG"},"source":["# **AI Recruiter Assistant ü§ñ**\n","\n","A conversational chatbot to pre-screen job offers from recruiters using advanced AI techniques.\n","\n","## Project Overview\n","- **Goal**: Automate initial screening of job offers from recruiters\n","- **Technology**: Fine-tuned open-source LLM with RAG pipeline\n","- **Interface**: Gradio web application for real-time conversations\n","- **Timeline**: 1 week structured development\n","- **Methodology**: Following systematic Generative AI project lifecycle\n","\n","## üéØ **AI Project Methodology**\n","\n","This project follows a **structured 4-stage Generative AI lifecycle** for systematic development and evaluation:\n","\n","### **Stage 1: Define the Scope** ‚úÖ COMPLETED\n","- **Problem Identification**: Manual screening of recruiter messages is time-consuming and inconsistent\n","- **Desired Outcome**: Automated system that analyzes job offers and responds appropriately based on profile match\n","- **Data Requirements**: CV, job expectations, LinkedIn conversation history for fine-tuning\n","- **Feasibility**: Generative AI is ideal for this conversational task with contextual decision-making\n","\n","### **Stage 2: Select Models** üîÑ CURRENT STAGE\n","- **Research Models**: Compare 4 open-source models from Hugging Face\n","- **Benchmark Performance**: Test speed, memory usage, and response quality\n","- **Model Selection**: Choose optimal model based on performance metrics\n","- **Cache Management**: Efficient model storage and loading from Google Drive\n","\n","### **Stage 3: Adapt & Align Model** üîÆ UPCOMING\n","- **Data Preprocessing**: Process LinkedIn conversation data for fine-tuning\n","- **Prompt Engineering**: Optimize prompts for recruiter interaction scenarios\n","- **QLoRA Fine-tuning**: Specialize selected model with conversation style\n","- **Evaluation**: Validate model performance on real recruiter scenarios\n","\n","### **Stage 4: Application Integration** üîÆ FINAL\n","- **Gradio Interface**: Deploy web application for real-time testing\n","- **End-to-end Testing**: Comprehensive system validation\n","- **Performance Optimization**: Final tuning and deployment preparation\n","- **Documentation**: Complete project summary and next steps\n","\n","## üèóÔ∏è **System Architecture**\n","\n","```mermaid\n","graph LR\n","    A[üì® Recruiter Message] --> B[üîç Intent Detection]\n","    B --> C[üìä RAG Analysis]\n","    C --> D[üéØ Match Scoring]\n","    D --> E[üß† State Management]\n","    E --> F[üí¨ Response Generation]\n","```\n","\n","---\n","\n","**üöÄ Currently working on Stage 2: Model Selection with cache optimization**\n"]},{"cell_type":"markdown","metadata":{"id":"hyj76RMWpRAk"},"source":["# **Stage 1: Define the Scope**"]},{"cell_type":"markdown","metadata":{"id":"_uEAC7f_pdh7"},"source":["## ***Phase 1:*** Configuration"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104039,"status":"ok","timestamp":1753578969644,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"5p_hhDIH7cPJ","outputId":"7906024b-d870-465d-cdae-bd6bf40e530d"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ All dependencies loaded!\n","üî• CUDA available: True\n","üéÆ GPU: NVIDIA A100-SXM4-40GB\n","üíæ GPU Memory: 42.5 GB\n"]}],"source":["# Install required packages\n","!pip install transformers>=4.36.0 torch>=2.0.0 peft>=0.7.0 bitsandbytes>=0.41.0 accelerate>=0.24.0\n","!pip install langchain>=0.1.0 langchain-community>=0.0.10 faiss-cpu>=1.7.4 sentence-transformers>=2.2.0\n","!pip install gradio>=4.0.0 pandas>=2.0.0 numpy>=1.24.0 tqdm>=4.65.0 datasets>=2.14.0\n","\n","# Core imports\n","import os, json, torch, pandas as pd, numpy as np, time, psutil\n","from pathlib import Path\n","from typing import List, Dict, Tuple, Optional\n","from dataclasses import dataclass\n","from enum import Enum\n","\n","# ML/AI imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, GenerationConfig\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# RAG imports\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.schema import Document\n","\n","print(\"‚úÖ All dependencies loaded!\")\n","print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","\n","# # Disk space tip\n","# print(f\"\\\\nüí° DISK SPACE TIP (100GB+ usage):\")\n","# print(f\"!rm -rf /tmp/* ~/.cache/pip/* ~/.cache/huggingface/*\")\n"]},{"cell_type":"markdown","metadata":{"id":"m6YQjLRYuTUM"},"source":["## ***Phase 2:*** Data Requirements"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96728,"status":"ok","timestamp":1753579066381,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"vsPjCQa47cPK","outputId":"0463d11e-c232-4d15-b041-843c811cd5d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ Google Drive mounted\n","Main paths\n","üìÅ Project path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\n","üóÇÔ∏è Cache path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/huggingface_cache\n","Loaded documents\n","‚úÖ CV loaded: 7032 characters\n","‚úÖ Job expectations loaded: 326 characters\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up project paths\n","project_path = \"/content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\"\n","cache_path = f\"{project_path}/huggingface_cache\"\n","\n","# Create directories\n","os.makedirs(cache_path, exist_ok=True)\n","os.makedirs(f\"{project_path}/data\", exist_ok=True)\n","\n","print(f\"‚úÖ Google Drive mounted\")\n","print(f\"Main paths\")\n","print(f\"üìÅ Project path: {project_path}\")\n","print(f\"üóÇÔ∏è Cache path: {cache_path}\")\n","\n","# Load data files\n","def load_documents():\n","    try:\n","        with open(f'{project_path}/RAG/cv.md', 'r', encoding='utf-8') as f:\n","            cv_content = f.read()\n","        with open(f'{project_path}/RAG/job_expectations.md', 'r', encoding='utf-8') as f:\n","            expectations_content = f.read()\n","\n","        print(f\"Loaded documents\")\n","        print(f\"‚úÖ CV loaded: {len(cv_content)} characters\")\n","        print(f\"‚úÖ Job expectations loaded: {len(expectations_content)} characters\")\n","        return cv_content, expectations_content\n","    except FileNotFoundError as e:\n","        print(f\"‚ùå Error loading documents: {e}\")\n","        return None, None\n","\n","cv_content, expectations_content = load_documents()\n"]},{"cell_type":"markdown","metadata":{"id":"U8cWQ1hcpXZt"},"source":["# **Stage 2: Select Models**"]},{"cell_type":"markdown","metadata":{"id":"iYVdYpjEpl07"},"source":["## ***--> Phase 1:*** check cached models"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1753579066445,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"x5bPz0Mm7cPK","outputId":"cb3f8f5b-09f8-474a-b90f-67dc0527ee16"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Cache detection functions ready\n"]}],"source":["def check_cached_models(cache_path):\n","    \"\"\"Check for cached models\"\"\"\n","    cached_models = []\n","    if not os.path.exists(cache_path):\n","        return cached_models\n","\n","    try:\n","        items = os.listdir(cache_path)\n","        for item in items:\n","            item_path = os.path.join(cache_path, item)\n","            if os.path.isdir(item_path) and not item.startswith('.'):\n","                try:\n","                    contents = os.listdir(item_path)\n","                    # Simple check: does it contain ANY folder starting with \"models--\"?\n","                    has_models_folder = any(f.startswith('models--') for f in contents if os.path.isdir(os.path.join(item_path, f)))\n","                    if has_models_folder:\n","                        cached_models.append(item)\n","                except Exception:\n","                    continue\n","    except Exception:\n","        pass\n","\n","    return cached_models\n","\n","def create_model_cache_dir(model_name: str, cache_path: str) -> str:\n","    \"\"\"Create clean cache directory for a model\"\"\"\n","    model_folder = model_name.replace('/', ' ')\n","    model_cache_dir = os.path.join(cache_path, model_folder)\n","    os.makedirs(model_cache_dir, exist_ok=True)\n","    return model_cache_dir\n","\n","def display_cache_status(candidate_models):\n","    \"\"\"Display cached models status\"\"\"\n","    print(\"\\nüîç CACHE STATUS:\")\n","    print(\"=\"*80 )\n","    cached_models = check_cached_models(cache_path)\n","\n","    if cached_models:\n","        print(f\"‚úÖ Found {len(cached_models)} cached models:\")\n","        for model in cached_models:\n","            model_name = model.replace(' ', '/')\n","            print(f\"\\t‚ö° {model_name}\")\n","    else:\n","        print(\"üì≠ No cached models found\")\n","\n","    # Show download vs cache status for candidate models\n","    print(f\"\\n‚¨áÔ∏èDownload vs üíªCache status:\")\n","    for model_name in candidate_models:\n","        model_folder_space = model_name.replace('/', ' ')\n","        if model_folder_space in cached_models:\n","            print(f\"\\t‚ö° {model_name} - üíª Will be load from cache\")\n","        else:\n","            print(f\"\\tüì• {model_name} - ‚¨áÔ∏è Will be download\")\n","\n","    return cached_models\n","\n","print(\"‚úÖ Cache detection functions ready\")\n"]},{"cell_type":"markdown","metadata":{"id":"KqVQ4-0xqBVJ"},"source":["## **--> Phase 2**: detect environment configuration, download required models, apply quantizacion, create benchmark process"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1753579066689,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"pNHpxHdk7cPK","outputId":"e78446ad-7790-4a21-b10c-48926d753b4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Multi-scenario benchmark ready!\n","üéØ 3 recruiter scenarios per model\n","üìä Test scenarios: ['Perfect Match', 'Generic Message', 'Wrong Match']\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4-1460874234.py:78: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  import pkg_resources\n"]}],"source":["class CacheAwareModelBenchmark:\n","\n","    def __init__(self):\n","        self.candidate_models = [\n","            \"mistralai/Mistral-7B-Instruct-v0.3\",\n","            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","            \"microsoft/Phi-3-mini-4k-instruct\",\n","            \"google/gemma-3-4b-it\"\n","        ]\n","\n","        self.model_specs = {\n","            \"mistralai/Mistral-7B-Instruct-v0.3\": {\"size\": \"7B\", \"context_length\": \"32K\"},\n","            \"meta-llama/Meta-Llama-3-8B-Instruct\": {\"size\": \"8B\", \"context_length\": \"8K\"},\n","            \"microsoft/Phi-3-mini-4k-instruct\": {\"size\": \"3.8B\", \"context_length\": \"4K\"},\n","            \"google/gemma-3-4b-it\": {\"size\": \"4B\", \"context_length\": \"8K\", \"features\": \"multimodal\"}\n","        }\n","\n","        self.results = []\n","\n","        # Benchmark iteration tracking\n","        from datetime import datetime\n","        self.benchmark_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.results_folder = f\"{project_path}/benchmark_iterations\"\n","        os.makedirs(self.results_folder, exist_ok=True)\n","\n","        # Capture environment configuration\n","        self.environment_config = self.detect_environment_config()\n","\n","        # System prompt for all interactions\n","        self.system_prompt = \"\"\"\n","        You are my highly intelligent personal assistant.\n","        Your mission is to engage in a continuous role-playing conversation where you will act as me.\n","        From this moment forward, the user will be playing the role of various recruiters contacting me.\n","        You must analyze the messages and generate appropriate responses as if you were me talking with a recruiter.\n","        Do not break character. Do not mention that you are an AI or an assistant in your replies to the recruiters.\n","        Analyze this job offer and tell me if it matches my profile:\n","        ***I am a Data and AI Engineer with extensive experience in building robust ELT pipelines and developing cutting-edge Generative AI solutions.\n","        My key skills include designing systems with Retrieval-Augmented Generation (RAG) and AI Agents using frameworks like Semantic Kernel and LangChain.\n","        I am proficient in data engineering with PySpark, SQL, and Apache Airflow,\n","        and highly skilled in cloud platforms, particularly Microsoft Azure (Data Factory, Databricks, AI Services) and GCP (BigQuery, Composer).\n","        I also possess strong backend development experience using Python and FastAPI to build and deploy services.***\n","        \"\"\"\n","\n","        # 3 different recruiter scenarios to test\n","        self.test_prompts = [\n","            {\n","            \"name\": \"Perfect Match\",\n","            \"message\": \"\"\"\n","            Hi! I hope you are doing well.\n","            I came across your profile and I am impressed by your background in AI and data engineering.\n","            I have an exciting opportunity for a Senior Data Engineer position at a fast-growing fintech company.\n","            The role involves working with Python, cloud technologies, and building ML pipelines.\n","            The salary range is ‚Ç¨60,000-65,000 and it is 100% remote.\n","            Would you be interested in learning more?\n","            \"\"\"\n","            },\n","            {\n","            \"name\": \"Generic Message\",\n","            \"message\": \"\"\"\n","            Hello, are you currently open to new opportunities?\n","            \"\"\"\n","            },\n","            {\n","            \"name\": \"Wrong Match\",\n","            \"message\": \"\"\"\n","            We have a Java developer position available at our company.\n","            5 years experience required, on-site work in London, competitive salary.\n","            Interested?\n","            \"\"\"\n","            }\n","        ]\n","\n","    def detect_environment_config(self):\n","        \"\"\"Detect comprehensive environment configuration for Google Colab\"\"\"\n","        import platform\n","        import sys\n","        import subprocess\n","        import pkg_resources\n","        import re\n","\n","        config = {\n","            \"platform\": {\n","                \"system\": platform.system(),\n","                \"release\": platform.release(),\n","                \"machine\": platform.machine(),\n","                \"processor\": platform.processor(),\n","                \"python_version\": sys.version.split()[0],\n","                \"python_implementation\": platform.python_implementation()\n","            },\n","            \"hardware\": {},\n","            \"software\": {},\n","            \"colab_specific\": {}\n","        }\n","\n","        # GPU Information\n","        if torch.cuda.is_available():\n","            gpu_count = torch.cuda.device_count()\n","            config[\"hardware\"][\"gpu\"] = {\n","                \"available\": True,\n","                \"count\": gpu_count,\n","                \"devices\": []\n","            }\n","\n","            for i in range(gpu_count):\n","                gpu_props = torch.cuda.get_device_properties(i)\n","                gpu_info = {\n","                    \"index\": i,\n","                    \"name\": gpu_props.name,\n","                    \"memory_total_gb\": round(gpu_props.total_memory / 1e9, 2),\n","                    \"memory_total_mb\": gpu_props.total_memory // (1024 * 1024),\n","                    \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\",\n","                    \"multiprocessor_count\": gpu_props.multi_processor_count\n","                }\n","                config[\"hardware\"][\"gpu\"][\"devices\"].append(gpu_info)\n","\n","            # Current GPU memory usage\n","            try:\n","                config[\"hardware\"][\"gpu\"][\"current_memory_allocated_gb\"] = round(torch.cuda.memory_allocated() / 1e9, 2)\n","                config[\"hardware\"][\"gpu\"][\"current_memory_reserved_gb\"] = round(torch.cuda.memory_reserved() / 1e9, 2)\n","            except:\n","                pass\n","        else:\n","            config[\"hardware\"][\"gpu\"] = {\"available\": False}\n","\n","        # CPU Information\n","        try:\n","            cpu_count = os.cpu_count()\n","            config[\"hardware\"][\"cpu\"] = {\n","                \"count\": cpu_count,\n","                \"architecture\": platform.architecture()[0]\n","            }\n","\n","            # Try to get more detailed CPU info\n","            try:\n","                with open('/proc/cpuinfo', 'r') as f:\n","                    cpuinfo = f.read()\n","                    if 'model name' in cpuinfo:\n","                        cpu_model = cpuinfo.split(\"model name\")[1].split(':')[1].split('\\n')[0].strip()\n","                        config[\"hardware\"][\"cpu\"][\"model\"] = cpu_model\n","            except:\n","                pass\n","        except:\n","            config[\"hardware\"][\"cpu\"] = {\"count\": \"unknown\"}\n","\n","        # Memory Information\n","        try:\n","            import psutil\n","            memory = psutil.virtual_memory()\n","            config[\"hardware\"][\"memory\"] = {\n","                \"total_gb\": round(memory.total / 1e9, 2),\n","                \"available_gb\": round(memory.available / 1e9, 2),\n","                \"used_gb\": round(memory.used / 1e9, 2),\n","                \"percentage_used\": memory.percent\n","            }\n","        except ImportError:\n","            # Fallback without psutil\n","            try:\n","                with open('/proc/meminfo', 'r') as f:\n","                    meminfo = f.read()\n","                    total_match = re.search(r'MemTotal:\\s+(\\d+)', meminfo)\n","                    if total_match:\n","                        total_kb = int(total_match.group(1))\n","                        config[\"hardware\"][\"memory\"] = {\n","                            \"total_gb\": round(total_kb / 1e6, 2),\n","                            \"source\": \"proc_meminfo\"\n","                        }\n","            except:\n","                config[\"hardware\"][\"memory\"] = {\"total_gb\": \"unknown\"}\n","\n","        # CUDA Information\n","        if torch.cuda.is_available():\n","            config[\"software\"][\"cuda\"] = {\n","                \"version\": torch.version.cuda,\n","                \"cudnn_version\": torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None,\n","                \"cudnn_available\": torch.backends.cudnn.is_available()\n","            }\n","\n","        # Key Library Versions\n","        key_libraries = ['torch', 'transformers', 'accelerate', 'bitsandbytes', 'peft', 'numpy', 'pandas']\n","        config[\"software\"][\"libraries\"] = {}\n","\n","        for lib in key_libraries:\n","            try:\n","                version = pkg_resources.get_distribution(lib).version\n","                config[\"software\"][\"libraries\"][lib] = version\n","            except:\n","                config[\"software\"][\"libraries\"][lib] = \"not_found\"\n","\n","        # Google Colab Specific Detection\n","        try:\n","            # Check if running in Colab\n","            import google.colab\n","            config[\"colab_specific\"][\"environment\"] = \"google_colab\"\n","            config[\"colab_specific\"][\"is_colab\"] = True\n","\n","            # Try to get Colab VM info\n","            try:\n","                result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n","                                      capture_output=True, text=True, timeout=10)\n","                if result.returncode == 0:\n","                    gpu_info = result.stdout.strip().split(\", \")\n","                    if len(gpu_info) >= 2:\n","                        config[\"colab_specific\"][\"nvidia_smi\"] = {\n","                            \"gpu_name\": gpu_info[0],\n","                            \"memory_total_mb\": gpu_info[1]\n","                        }\n","            except:\n","                pass\n","\n","            # Detect Colab GPU type from GPU name\n","            if config[\"hardware\"][\"gpu\"][\"available\"]:\n","                gpu_name = config[\"hardware\"][\"gpu\"][\"devices\"][0][\"name\"].lower()\n","                if \"tesla t4\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"T4\"\n","                elif \"tesla k80\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"K80\"\n","                elif \"tesla v100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"V100\"\n","                elif \"tesla p4\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"P4\"\n","                elif \"tesla p100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"P100\"\n","                elif \"a100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"A100\"\n","                else:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"Unknown\"\n","\n","        except ImportError:\n","            config[\"colab_specific\"][\"is_colab\"] = False\n","            config[\"colab_specific\"][\"environment\"] = \"local_or_other\"\n","\n","        # Disk Space (for cache management)\n","        try:\n","            import shutil\n","            cache_disk_usage = shutil.disk_usage(cache_path)\n","            config[\"hardware\"][\"disk\"] = {\n","                \"cache_path_total_gb\": round(cache_disk_usage.total / 1e9, 2),\n","                \"cache_path_free_gb\": round(cache_disk_usage.free / 1e9, 2),\n","                \"cache_path_used_gb\": round((cache_disk_usage.total - cache_disk_usage.free) / 1e9, 2)\n","            }\n","        except:\n","            config[\"hardware\"][\"disk\"] = {\"status\": \"unable_to_detect\"}\n","\n","        return config\n","\n","    def display_environment_config(self):\n","\n","        print(\"\\nüñ•Ô∏è ENVIRONMENT CONFIGURATION:\")\n","        print(\"=\" * 60)\n","\n","        \"\"\"Display current environment configuration\"\"\"\n","        config = self.environment_config\n","        # GPU Information\n","        if config[\"hardware\"][\"gpu\"][\"available\"]:\n","            gpu = config['hardware']['gpu']['devices'][0]\n","            print(f\"üéÆ GPU: {gpu['name']}\")\n","            print(f\"üíæ GPU Memory: {gpu['memory_total_gb']} GB ({gpu['memory_total_mb']} MB)\")\n","            print(f\"üîß Compute Capability: {gpu['compute_capability']}\")\n","\n","            if config[\"colab_specific\"][\"is_colab\"]:\n","                colab_gpu = config[\"colab_specific\"].get(\"colab_gpu_type\", \"Unknown\")\n","                print(f\"‚òÅÔ∏è Colab GPU Type: {colab_gpu}\")\n","        else:\n","            print(\"‚ùå GPU: Not available\")\n","\n","        # CPU & Memory\n","        print(f\"üß† CPU: {config['hardware']['cpu']['count']} cores\")\n","        if 'model' in config['hardware']['cpu']:\n","            print(f\"üîç CPU Model: {config['hardware']['cpu']['model']}\")\n","\n","        if 'memory' in config['hardware']:\n","            mem = config['hardware']['memory']\n","            print(f\"üíø RAM: {mem['total_gb']} GB total\")\n","            if 'available_gb' in mem:\n","                print(f\"üìä RAM Usage: {mem['used_gb']}/{mem['total_gb']} GB ({mem['percentage_used']:.1f}%)\")\n","\n","        # Software\n","        print(f\"üêç Python: {config['platform']['python_version']}\")\n","        print(f\"üî• PyTorch: {config['software']['libraries']['torch']}\")\n","        print(f\"ü§ó Transformers: {config['software']['libraries']['transformers']}\")\n","\n","        if config['hardware']['gpu']['available']:\n","            print(f\"‚ö° CUDA: {config['software']['cuda']['version']}\")\n","\n","        # Environment\n","        env_type = \"Google Colab\" if config[\"colab_specific\"][\"is_colab\"] else \"Local/Other\"\n","        print(f\"üåê Environment: {env_type}\")\n","\n","    def load_model_with_quantization(self, model_name: str):\n","        \"\"\"Load model with cache detection, corruption handling, and optimized configurations\"\"\"\n","        cached_models = check_cached_models(cache_path)\n","        model_folder_space = model_name.replace('/', ' ')\n","\n","        is_cached = model_folder_space in cached_models\n","        cache_corrupted = False  # Flag to track if we need to retry\n","\n","        if is_cached:\n","            print(f\"‚ö° Loading {model_name} from cache...\")\n","            model_cache_dir = os.path.join(cache_path, model_folder_space)\n","        else:\n","            print(f\"üì• Downloading {model_name} ...\")\n","            model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        def _attempt_load():\n","            \"\"\"Helper function to attempt model loading\"\"\"\n","            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n","            if tokenizer.pad_token is None:\n","                tokenizer.pad_token = tokenizer.eos_token\n","\n","            # Model-specific configurations to avoid warnings\n","            model_kwargs = {\n","                \"quantization_config\": bnb_config,\n","                \"device_map\": \"auto\",\n","                \"trust_remote_code\": True,\n","                \"cache_dir\": model_cache_dir\n","            }\n","\n","            # Fix Phi-3 flash-attention warnings\n","            if \"phi-3\" in model_name.lower():\n","                model_kwargs[\"attn_implementation\"] = \"eager\"\n","                print(f\"   üîß Using eager attention for Phi-3 (avoiding flash-attention warnings)\")\n","\n","            model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n","            return model, tokenizer\n","\n","        try:\n","            model, tokenizer = _attempt_load()\n","            print(f\"\\t‚úÖ {model_name} loaded successfully!\")\n","            return model, tokenizer\n","\n","        except Exception as e:\n","            error_msg = str(e).lower()\n","\n","            # Check for cache corruption errors\n","            if any(corruption_keyword in error_msg for corruption_keyword in\n","                   ['headertoosmall', 'header too small', 'corrupt', 'safetensors',\n","                    'invalid', 'decode', 'deserializing']):\n","\n","                if is_cached and not cache_corrupted:\n","                    print(f\"\\tüîß Cache corruption detected: {str(e)}\")\n","                    print(f\"\\tüóëÔ∏è Clearing corrupted cache and retrying download...\")\n","\n","                    # Remove corrupted cache directory\n","                    import shutil\n","                    try:\n","                        if os.path.exists(model_cache_dir):\n","                            shutil.rmtree(model_cache_dir)\n","                            print(f\"\\t‚úÖ Corrupted cache removed: {model_cache_dir}\")\n","                    except Exception as remove_error:\n","                        print(f\"\\t‚ö†Ô∏è Could not remove cache: {str(remove_error)}\")\n","\n","                    # Create new cache directory and retry download\n","                    model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","                    print(f\"\\tüì• Re-downloading {model_name}...\")\n","                    cache_corrupted = True  # Mark as corrupted to avoid infinite retry\n","\n","                    try:\n","                        model, tokenizer = _attempt_load()\n","                        print(f\"\\t‚úÖ {model_name} loaded successfully after cache cleanup!\")\n","                        return model, tokenizer\n","                    except Exception as retry_error:\n","                        print(f\"\\t‚ùå Failed even after cache cleanup: {str(retry_error)}\")\n","                        raise retry_error\n","                else:\n","                    print(f\"\\t‚ùå Cache corruption persists or already retried: {str(e)}\")\n","                    raise\n","            else:\n","                print(f\"\\t‚ùå Error loading {model_name}: {str(e)}\")\n","                raise\n","\n","    def test_single_prompt(self, model, tokenizer, model_name: str, test_prompt: dict):\n","        \"\"\"Test a single prompt and return response\"\"\"\n","        # Combine system prompt with user message\n","        full_prompt = f\"{self.system_prompt}\\\\n\\\\nRecruiter: {test_prompt['message']}\\\\n\\\\nResponse:\"\n","\n","        try:\n","            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n","\n","            device = next(model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                inference_start = time.time()\n","\n","                if \"phi-3\" in model_name.lower():\n","                    # Fix for Phi-3 DynamicCache issue\n","                    outputs = model.generate(\n","                        **inputs,  # This already contains attention_mask\n","                        max_new_tokens=100,\n","                        do_sample=True,\n","                        temperature=0.7,\n","                        pad_token_id=tokenizer.eos_token_id,\n","                        use_cache=False  # Disable problematic cache\n","                    )\n","                else:\n","                    outputs = model.generate(\n","                        **inputs,  # This already contains attention_mask\n","                        max_new_tokens=100,\n","                        do_sample=True,\n","                        temperature=0.7,\n","                        pad_token_id=tokenizer.eos_token_id\n","                    )\n","                inference_time = time.time() - inference_start\n","\n","            # Decode response and clean up\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            # Extract only the response part (remove the prompt)\n","            response = response.replace(full_prompt, \"\").strip()\n","\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"inference_time\": inference_time,\n","                \"response\": response, # [:300] + \"...\" if len(response) > 300 else response,\n","                \"status\": \"success\"\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": str(e)\n","            }\n","\n","    def benchmark_model(self, model_name: str):\n","        \"\"\"Test model with multiple recruiter scenarios\"\"\"\n","        print(f\"\\nüß™ Testing {model_name}...\")\n","\n","        start_time = time.time()\n","\n","        try:\n","            model, tokenizer = self.load_model_with_quantization(model_name)\n","            load_time = time.time() - start_time\n","\n","            # Test all prompts\n","            prompt_results = []\n","            total_inference_time = 0\n","\n","            for i, test_prompt in enumerate(self.test_prompts):\n","                print(f\"\\tTesting scenario {i+1}/3: {test_prompt['name']}\")\n","                prompt_result = self.test_single_prompt(model, tokenizer, model_name, test_prompt)\n","                prompt_results.append(prompt_result)\n","                if prompt_result[\"status\"] == \"success\":\n","                    total_inference_time += prompt_result[\"inference_time\"]\n","\n","            # Calculate memory usage\n","            if torch.cuda.is_available():\n","                memory_used = torch.cuda.max_memory_allocated() / 1e9\n","                torch.cuda.reset_peak_memory_stats()\n","            else:\n","                memory_used = 0\n","\n","            device = next(model.parameters()).device\n","            avg_inference_time = total_inference_time / len([r for r in prompt_results if r[\"status\"] == \"success\"])\n","\n","            result = {\n","                \"model_name\": model_name,\n","                \"status\": \"success\",\n","                \"load_time\": load_time,\n","                \"avg_inference_time\": avg_inference_time,\n","                \"total_inference_time\": total_inference_time,\n","                \"memory_gb\": memory_used,\n","                \"device\": str(device),\n","                \"prompt_results\": prompt_results,\n","                \"specifications\": self.model_specs.get(model_name, {})\n","            }\n","\n","            successful_prompts = len([r for r in prompt_results if r[\"status\"] == \"success\"])\n","            print(f\"\\t‚úÖ Load: {load_time:.1f}s | Avg Inference: {avg_inference_time:.2f}s | Memory: {memory_used:.1f}GB\")\n","            print(f\"\\tüéØ Successful prompts: {successful_prompts}/3 | Device: {device}\")\n","\n","            # Clean up\n","            del model, tokenizer\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","            return result\n","\n","        except Exception as e:\n","            print(f\"\\t‚ö†Ô∏è Model failed to load: {str(e)}\")\n","            return {\n","                \"model_name\": model_name,\n","                \"status\": \"failed\",\n","                \"error\": str(e),\n","                \"specifications\": self.model_specs.get(model_name, {})\n","            }\n","\n","    def benchmark_all_models(self):\n","\n","        \"\"\"Benchmark all models with multiple scenarios\"\"\"\n","        print(\"üöÄ STARTING MULTI-SCENARIO ANALYSIS\")\n","        print(\"üéØ Testing 3 recruiter scenarios per model\")\n","\n","        for model_name in self.candidate_models:\n","            print(\"-\" * 60)\n","            result = self.benchmark_model(model_name)\n","            self.results.append(result)\n","\n","        print(\"\\n‚úÖ Multi-scenario benchmark complete!\")\n","\n","    def save_benchmark_iteration(self):\n","        \"\"\"Save current benchmark iteration with timestamp\"\"\"\n","        from datetime import datetime\n","\n","        # Create detailed iteration data\n","        iteration_data = {\n","            \"metadata\": {\n","                \"timestamp\": self.benchmark_timestamp,\n","                \"datetime_readable\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","                \"total_models\": len(self.candidate_models),\n","                \"successful_models\": len([r for r in self.results if r.get(\"status\") == \"success\"]),\n","                \"failed_models\": len([r for r in self.results if r.get(\"status\") == \"failed\"]),\n","                \"test_scenarios\": len(self.test_prompts)\n","            },\n","            \"environment_config\": self.environment_config,\n","            \"test_scenarios\": self.test_prompts,\n","            \"model_specifications\": self.model_specs,\n","            \"detailed_results\": self.results,\n","            \"summary\": self.generate_benchmark_summary()\n","        }\n","\n","        # Save detailed results\n","        iteration_file = f\"{self.results_folder}/benchmark_{self.benchmark_timestamp}.json\"\n","        with open(iteration_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(iteration_data, f, indent=2, default=str, ensure_ascii=False)\n","\n","        print(f\"\\nüíæ Benchmark iteration saved: {iteration_file}\")\n","\n","        # Update comparison history\n","        self.update_comparison_history(iteration_data)\n","\n","        return iteration_file\n","\n","    def generate_benchmark_summary(self):\n","        \"\"\"Generate concise summary of benchmark results\"\"\"\n","        successful_results = [r for r in self.results if r.get('status') == 'success']\n","\n","        if not successful_results:\n","            return {\"status\": \"no_successful_models\"}\n","\n","        # Performance rankings\n","        by_speed = sorted(successful_results, key=lambda x: x[\"avg_inference_time\"])\n","        by_memory = sorted(successful_results, key=lambda x: x[\"memory_gb\"])\n","        by_load_time = sorted(successful_results, key=lambda x: x[\"load_time\"])\n","\n","        summary = {\n","            \"rankings\": {\n","                \"fastest_inference\": {\n","                    \"model\": by_speed[0][\"model_name\"],\n","                    \"time_seconds\": by_speed[0][\"avg_inference_time\"]\n","                },\n","                \"least_memory\": {\n","                    \"model\": by_memory[0][\"model_name\"],\n","                    \"memory_gb\": by_memory[0][\"memory_gb\"]\n","                },\n","                \"fastest_loading\": {\n","                    \"model\": by_load_time[0][\"model_name\"],\n","                    \"load_time_seconds\": by_load_time[0][\"load_time\"]\n","                }\n","            },\n","            \"overall_stats\": {\n","                \"avg_inference_time\": sum(r[\"avg_inference_time\"] for r in successful_results) / len(successful_results),\n","                \"avg_memory_usage\": sum(r[\"memory_gb\"] for r in successful_results) / len(successful_results),\n","                \"avg_load_time\": sum(r[\"load_time\"] for r in successful_results) / len(successful_results),\n","                \"total_successful_prompts\": sum(len([p for p in r[\"prompt_results\"] if p[\"status\"] == \"success\"]) for r in successful_results)\n","            },\n","            \"model_performance_scores\": []\n","        }\n","\n","        # Calculate performance scores (lower is better)\n","        for result in successful_results:\n","            score = (\n","                result[\"avg_inference_time\"] * 0.4 +  # 40% weight on inference speed\n","                result[\"memory_gb\"] * 0.3 +           # 30% weight on memory efficiency\n","                result[\"load_time\"] / 100 * 0.3       # 30% weight on load time (scaled)\n","            )\n","\n","            summary[\"model_performance_scores\"].append({\n","                \"model\": result[\"model_name\"],\n","                \"performance_score\": round(score, 2),\n","                \"successful_prompts\": len([p for p in result[\"prompt_results\"] if p[\"status\"] == \"success\"])\n","            })\n","\n","        # Sort by performance score\n","        summary[\"model_performance_scores\"].sort(key=lambda x: x[\"performance_score\"])\n","\n","\n","        return summary\n","\n","    def update_comparison_history(self, current_iteration):\n","        \"\"\"Update master comparison file with historical data\"\"\"\n","        comparison_file = f\"{self.results_folder}/benchmark_comparison_history.json\"\n","\n","        # Load existing history\n","        if os.path.exists(comparison_file):\n","            with open(comparison_file, 'r', encoding='utf-8') as f:\n","                history = json.load(f)\n","        else:\n","            history = {\n","                \"iterations\": [],\n","                \"models_tracked\": list(set(self.candidate_models)),\n","                \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","            }\n","\n","        # Add current iteration summary\n","        history[\"iterations\"].append({\n","            \"timestamp\": current_iteration[\"metadata\"][\"timestamp\"],\n","            \"datetime\": current_iteration[\"metadata\"][\"datetime_readable\"],\n","            \"summary\": current_iteration[\"summary\"],\n","            \"metadata\": current_iteration[\"metadata\"]\n","        })\n","\n","        # Keep only last 10 iterations\n","        history[\"iterations\"] = history[\"iterations\"][-10:]\n","        history[\"last_updated\"] = current_iteration[\"metadata\"][\"datetime_readable\"]\n","\n","        # Save updated history\n","        with open(comparison_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(history, f, indent=2, default=str, ensure_ascii=False)\n","\n","        print(f\"üìà Comparison history updated: {comparison_file}\")\n","\n","    def display_detailed_results(self):\n","        \"\"\"Display comprehensive benchmark results and save iteration\"\"\"\n","        print(\"\\n\\nüìä DETAILED BENCHMARK RESULTS\")\n","        print(\"=\" * 60)\n","\n","        successful_results = [r for r in self.results if r.get('status') == 'success']\n","        failed_results = [r for r in self.results if r.get('status') == 'failed']\n","\n","        if successful_results:\n","            print(f\"\\n‚úÖ {len(successful_results)} models tested successfully:\")\n","\n","            # Model comparison table\n","            print(f\"\\n\\nüìà MODEL PERFORMANCE COMPARISON:\")\n","            print(\"-\" * 80)\n","            print(f\"{'Model':<30} {'Size':<8} {'Load(s)':<8} {'Avg Inf(s)':<10} {'Memory(GB)':<10} {'Features':<12}\")\n","            print(\"-\" * 80)\n","\n","            for result in successful_results:\n","                specs = result['specifications']\n","                features = specs.get('features', 'text-only')\n","                model_short = result['model_name'].split('/')[-1][:28]\n","\n","                print(f\"{model_short:<30} {specs['size']:<8} {result['load_time']:<8.1f} {result['avg_inference_time']:<10.2f} {result['memory_gb']:<10.1f} {features:<12}\")\n","\n","            # Find best models\n","            fastest_model = min(successful_results, key=lambda x: x['avg_inference_time'])\n","            least_memory = min(successful_results, key=lambda x: x['memory_gb'])\n","\n","            print(\"-\" * 80)\n","            print(f\"üèÜ FASTEST: {fastest_model['model_name'].split('/')[-1]} ({fastest_model['avg_inference_time']:.2f}s)\")\n","            print(f\"üíæ LEAST MEMORY: {least_memory['model_name'].split('/')[-1]} ({least_memory['memory_gb']:.1f}GB)\")\n","\n","            # Performance scoring\n","            summary = self.generate_benchmark_summary()\n","            print(f\"\\n\\nüéØ OVERALL PERFORMANCE RANKING:\")\n","            print(\"-\" * 80)\n","            print(\"\\nSCORE = 40% weight on inference speed + 30% weight on memory efficiency + 30% weight on load time (scaled)\\n\")\n","            for i, model_score in enumerate(summary['model_performance_scores'], 1):\n","                model_short = model_score['model'].split('/')[-1]\n","                print(f\"{i}. {model_short:<35} Score: {model_score['performance_score']:<6} ({model_score['successful_prompts']}/3 prompts)\")\n","\n","            # Detailed responses per scenario\n","            print(\"=\" * 60)\n","            print(f\"\\n\\nüé≠ RESPONSE QUALITY BY SCENARIO:\")\n","\n","            for i, scenario in enumerate(self.test_prompts):\n","                print(f\"\\n\\nüìù SCENARIO {i+1}: {scenario['name']}\")\n","                print(f\"Recruiter: {scenario['message'][:100]}...\")\n","                print(\"-\" * 60)\n","\n","                for result in successful_results:\n","                    if 'prompt_results' in result:\n","                        prompt_result = result['prompt_results'][i]\n","                        model_short = result['model_name'].split('/')[-1]\n","\n","                        if prompt_result['status'] == 'success':\n","                            print(f\"\\nü§ñ {model_short}:\")\n","                            print(f\"\\t‚ö° Time: {prompt_result['inference_time']:.2f}s\")\n","                            print(f\"\\tüí¨ Response:\\n<<START>>\\n {prompt_result['response']}\\n<<END>>\")\n","                        else:\n","                            print(f\"\\n‚ùå {model_short}: {prompt_result.get('error', 'Failed')}\")\n","\n","        if failed_results:\n","            print(f\"\\n‚ùå {len(failed_results)} models failed:\")\n","            for result in failed_results:\n","                print(f\"\\t‚Ä¢ {result['model_name']}: {result['error']}\")\n","\n","        # Save this iteration\n","        self.save_benchmark_iteration()\n","\n","benchmark = CacheAwareModelBenchmark()\n","print(\"‚úÖ Multi-scenario benchmark ready!\")\n","print(\"üéØ 3 recruiter scenarios per model\")\n","print(f\"üìä Test scenarios: {[p['name'] for p in benchmark.test_prompts]}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1753579066785,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"vAfe7CJ05Icl","outputId":"29fb26c6-b2a8-4729-adc9-2d0fd24989e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîß ROBUST FILE SAVING PATCH APPLIED!\n","‚úÖ Enhanced error handling for benchmark file operations\n","üíæ Recovery mechanisms activated for failed saves\n","üìä Disk space monitoring enabled\n"]}],"source":["# üîß ROBUST FILE SAVING PATCH\n","# Aplicar patch a las funciones de guardado existentes para manejo robusto de errores\n","\n","def robust_save_json(file_path, data, description=\"file\"):\n","    \"\"\"Helper function for robust JSON file saving with error handling\"\"\"\n","    import shutil\n","    from datetime import datetime\n","    import traceback\n","\n","    try:\n","        # Ensure directory exists\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","        # Check disk space\n","        disk_usage = shutil.disk_usage(os.path.dirname(file_path))\n","        free_gb = disk_usage.free / 1e9\n","\n","        if free_gb < 0.1:  # Less than 100MB\n","            print(f\"‚ö†Ô∏è Warning: Low disk space ({free_gb:.1f} GB)\")\n","\n","        # Write file with verification\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, default=str, ensure_ascii=False)\n","\n","        # Verify file was created and has content\n","        if os.path.exists(file_path):\n","            file_size = os.path.getsize(file_path)\n","            print(f\"‚úÖ {description} saved: {file_path} ({file_size} bytes)\")\n","            return True\n","        else:\n","            raise FileNotFoundError(f\"File was not created: {file_path}\")\n","\n","    except Exception as error:\n","        print(f\"‚ùå Error saving {description}: {str(error)}\")\n","\n","        # Try alternative filename with timestamp\n","        base_name, ext = os.path.splitext(file_path)\n","        alt_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        alt_file = f\"{base_name}_recovery_{alt_timestamp}{ext}\"\n","\n","        try:\n","            with open(alt_file, 'w', encoding='utf-8') as f:\n","                json.dump(data, f, indent=2, default=str, ensure_ascii=False)\n","            print(f\"‚úÖ {description} saved to recovery file: {alt_file}\")\n","            return True\n","        except Exception as alt_error:\n","            print(f\"‚ùå Recovery save also failed: {str(alt_error)}\")\n","            print(f\"üìç Full traceback: {traceback.format_exc()}\")\n","            return False\n","\n","# Patch the existing save methods with robust error handling\n","original_save_iteration = benchmark.save_benchmark_iteration\n","original_update_history = benchmark.update_comparison_history\n","\n","def patched_save_benchmark_iteration(self):\n","    \"\"\"Enhanced save_benchmark_iteration with robust error handling\"\"\"\n","    from datetime import datetime\n","\n","    try:\n","        # Create detailed iteration data (same as original)\n","        iteration_data = {\n","            \"metadata\": {\n","                \"timestamp\": self.benchmark_timestamp,\n","                \"datetime_readable\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","                \"total_models\": len(self.candidate_models),\n","                \"successful_models\": len([r for r in self.results if r.get(\"status\") == \"success\"]),\n","                \"failed_models\": len([r for r in self.results if r.get(\"status\") == \"failed\"]),\n","                \"test_scenarios\": len(self.test_prompts)\n","            },\n","            \"environment_config\": self.environment_config,\n","            \"test_scenarios\": self.test_prompts,\n","            \"model_specifications\": self.model_specs,\n","            \"detailed_results\": self.results,\n","            \"summary\": self.generate_benchmark_summary()\n","        }\n","\n","        # Use robust save function\n","        iteration_file = f\"{self.results_folder}/benchmark_{self.benchmark_timestamp}.json\"\n","        success = robust_save_json(iteration_file, iteration_data, \"Benchmark iteration\")\n","\n","        if success:\n","            # Update comparison history\n","            self.update_comparison_history(iteration_data)\n","            return iteration_file\n","        else:\n","            print(f\"‚ùå Failed to save benchmark iteration\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"‚ùå Critical error in patched_save_benchmark_iteration: {str(e)}\")\n","        return None\n","\n","def patched_update_comparison_history(self, current_iteration):\n","    \"\"\"Enhanced update_comparison_history with robust error handling\"\"\"\n","    comparison_file = f\"{self.results_folder}/benchmark_comparison_history.json\"\n","\n","    try:\n","        # Load existing history with error handling\n","        if os.path.exists(comparison_file):\n","            try:\n","                with open(comparison_file, 'r', encoding='utf-8') as f:\n","                    history = json.load(f)\n","                print(f\"üìö Loaded existing history with {len(history.get('iterations', []))} iterations\")\n","            except Exception as load_error:\n","                print(f\"‚ö†Ô∏è Error loading history: {str(load_error)}. Creating new history.\")\n","                history = {\n","                    \"iterations\": [],\n","                    \"models_tracked\": list(set(self.candidate_models)),\n","                    \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","                }\n","        else:\n","            print(f\"üìù Creating new history file\")\n","            history = {\n","                \"iterations\": [],\n","                \"models_tracked\": list(set(self.candidate_models)),\n","                \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","            }\n","\n","        # Add current iteration summary\n","        history[\"iterations\"].append({\n","            \"timestamp\": current_iteration[\"metadata\"][\"timestamp\"],\n","            \"datetime\": current_iteration[\"metadata\"][\"datetime_readable\"],\n","            \"summary\": current_iteration[\"summary\"],\n","            \"metadata\": current_iteration[\"metadata\"]\n","        })\n","\n","        # Keep only last 10 iterations\n","        history[\"iterations\"] = history[\"iterations\"][-10:]\n","        history[\"last_updated\"] = current_iteration[\"metadata\"][\"datetime_readable\"]\n","\n","        # Use robust save function\n","        robust_save_json(comparison_file, history, \"Comparison history\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Critical error in patched_update_comparison_history: {str(e)}\")\n","\n","# Apply patches to the benchmark instance\n","benchmark.save_benchmark_iteration = patched_save_benchmark_iteration.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark.update_comparison_history = patched_update_comparison_history.__get__(benchmark, CacheAwareModelBenchmark)\n","\n","print(\"üîß ROBUST FILE SAVING PATCH APPLIED!\")\n","print(\"‚úÖ Enhanced error handling for benchmark file operations\")\n","print(\"üíæ Recovery mechanisms activated for failed saves\")\n","print(\"üìä Disk space monitoring enabled\")\n"]},{"cell_type":"markdown","metadata":{"id":"qVZHz6ilu2_9"},"source":["## **--> Phase 3**: fix issues related with current models"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1753579066789,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"14Zl6tLXxNl5"},"outputs":[],"source":["# üîß COMPREHENSIVE GEMMA DIVISION BY ZERO FIX\n","import warnings\n","import numpy as np\n","\n","# Global numpy error state configuration\n","old_err_state = np.seterr(divide='ignore', invalid='ignore', over='ignore', under='ignore')\n","\n","# Comprehensive warning suppression for numerical issues\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*torch.utils.checkpoint.*\")\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# Monkey patch the test_single_prompt method with comprehensive Gemma fixes\n","original_test_single_prompt = benchmark.test_single_prompt\n","\n","def gemma_safe_test_single_prompt(self, model, tokenizer, model_name: str, test_prompt: dict):\n","    \"\"\"Comprehensive Gemma-safe version with multiple fallback strategies\"\"\"\n","    full_prompt = f\"{self.system_prompt}\\n\\nRecruiter: {test_prompt['message']}\\n\\nResponse:\"\n","\n","    try:\n","        # Comprehensive protection for all models with special handling for Gemma\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            with np.errstate(all='ignore'):  # Ignore all numpy warnings\n","                return self._safe_generate_response(model, tokenizer, model_name, test_prompt, full_prompt)\n","\n","    except Exception as e:\n","        # Enhanced error handling with type detection\n","        error_msg = str(e).lower()\n","        if any(keyword in error_msg for keyword in ['division', 'divide', 'zero', 'nan', 'inf']):\n","            print(f\"     üîß Numerical instability detected, applying fallback...\")\n","            try:\n","                return self._fallback_generation(model, tokenizer, model_name, test_prompt, full_prompt)\n","            except Exception as fallback_error:\n","                return {\n","                    \"prompt_name\": test_prompt[\"name\"],\n","                    \"status\": \"failed\",\n","                    \"error\": f\"Primary: {str(e)}, Fallback: {str(fallback_error)}\"\n","                }\n","        else:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": str(e)\n","            }\n","\n","def safe_generate_response(self, model, tokenizer, model_name: str, test_prompt: dict, full_prompt: str):\n","    \"\"\"Enhanced response generation with model-specific optimizations\"\"\"\n","\n","    # Enhanced tokenization with explicit parameters\n","    inputs = tokenizer(\n","        full_prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512,\n","        padding=True,\n","        add_special_tokens=True\n","    )\n","\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        inference_start = time.time()\n","\n","        # Enhanced model-specific generation parameters\n","        if \"phi-3\" in model_name.lower():\n","            generation_config = {\n","                **inputs,\n","                'max_new_tokens': 100,\n","                'do_sample': True,\n","                'temperature': 0.7,\n","                'pad_token_id': tokenizer.eos_token_id,\n","                'use_cache': False\n","            }\n","        elif \"gemma\" in model_name.lower():\n","            # Optimized Gemma configuration (cleaned invalid parameters)\n","            generation_config = {\n","                'input_ids': inputs['input_ids'],\n","                'attention_mask': inputs.get('attention_mask'),\n","                'max_new_tokens': 80,  # Reduced to avoid memory issues\n","                'do_sample': False,    # Use greedy decoding for stability\n","                'pad_token_id': tokenizer.pad_token_id or tokenizer.eos_token_id,\n","                'eos_token_id': tokenizer.eos_token_id,\n","                'repetition_penalty': 1.1,\n","                'use_cache': True     # Enable cache for Gemma\n","                # Removed: top_p, early_stopping (invalid for Gemma)\n","            }\n","        else:\n","            generation_config = {\n","                **inputs,\n","                'max_new_tokens': 100,\n","                'do_sample': True,\n","                'temperature': 0.7,\n","                'pad_token_id': tokenizer.eos_token_id\n","            }\n","\n","        # Generate with comprehensive error handling\n","        outputs = model.generate(**generation_config)\n","        inference_time = time.time() - inference_start\n","\n","    # Enhanced response decoding\n","    try:\n","        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        # Clean up response by removing prompt\n","        if full_prompt in response:\n","            response = response.replace(full_prompt, \"\").strip()\n","        elif response.startswith(full_prompt[:50]):  # Partial match fallback\n","            response = response[len(full_prompt):].strip()\n","    except Exception as decode_error:\n","        response = f\"[Decoding error: {str(decode_error)}]\"\n","\n","    return {\n","        \"prompt_name\": test_prompt[\"name\"],\n","        \"inference_time\": inference_time,\n","        \"response\": response, # [:300] + \"...\" if len(response) > 300 else response,\n","        \"status\": \"success\"\n","    }\n","\n","def fallback_generation(self, model, tokenizer, model_name: str, test_prompt: dict, full_prompt: str):\n","    \"\"\"Ultra-conservative fallback generation for problematic models\"\"\"\n","    print(f\"\\t\\tüÜò Using ultra-conservative fallback generation...\")\n","\n","    # Minimal tokenization\n","    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        inference_start = time.time()\n","\n","        # Ultra-conservative generation\n","        try:\n","            outputs = model.generate(\n","                inputs['input_ids'],\n","                max_new_tokens=50,     # Very limited output\n","                do_sample=False,       # Greedy only\n","                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n","                eos_token_id=tokenizer.eos_token_id,\n","                use_cache=False,       # Disable cache\n","                output_scores=False,   # Disable score computation\n","                return_dict_in_generate=False\n","            )\n","            inference_time = time.time() - inference_start\n","\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            response = response.replace(full_prompt, \"\").strip() or \"[Model generated empty response]\"\n","\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"inference_time\": inference_time,\n","                \"response\": f\"[FALLBACK] {response}\",\n","                \"status\": \"success\"\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": f\"Fallback failed: {str(e)}\"\n","            }\n","\n","# Apply comprehensive patches\n","benchmark.test_single_prompt = gemma_safe_test_single_prompt.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark._safe_generate_response = safe_generate_response.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark._fallback_generation = fallback_generation.__get__(benchmark, CacheAwareModelBenchmark)"]},{"cell_type":"markdown","metadata":{"id":"BoXIj5Huq2Ri"},"source":["## **--> Phase 4:** Execute benchmarking"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1753579067203,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"ZSD894ZJ7cPL","outputId":"82aea7da-caaf-4780-caeb-ac32de273c61"},"outputs":[{"output_type":"stream","name":"stdout","text":["====================================================================================================\n","========================================= START OF PROCESS =========================================\n","====================================================================================================\n","\n","üñ•Ô∏è ENVIRONMENT CONFIGURATION:\n","============================================================\n","üéÆ GPU: NVIDIA A100-SXM4-40GB\n","üíæ GPU Memory: 42.47 GB (40506 MB)\n","üîß Compute Capability: 8.0\n","‚òÅÔ∏è Colab GPU Type: A100\n","üß† CPU: 12 cores\n","üîç CPU Model: Intel(R) Xeon(R) CPU @ 2.20GHz\n","üíø RAM: 89.63 GB total\n","üìä RAM Usage: 2.47/89.63 GB (3.8%)\n","üêç Python: 3.11.13\n","üî• PyTorch: 2.6.0+cu124\n","ü§ó Transformers: 4.53.3\n","‚ö° CUDA: 12.4\n","üåê Environment: Google Colab\n","\n","üîç CACHE STATUS:\n","================================================================================\n","‚úÖ Found 4 cached models:\n","\t‚ö° microsoft/Phi-3-mini-4k-instruct\n","\t‚ö° google/gemma-3-4b-it\n","\t‚ö° meta-llama/Meta-Llama-3-8B-Instruct\n","\t‚ö° mistralai/Mistral-7B-Instruct-v0.3\n","\n","‚¨áÔ∏èDownload vs üíªCache status:\n","\t‚ö° mistralai/Mistral-7B-Instruct-v0.3 - üíª Will be load from cache\n","\t‚ö° meta-llama/Meta-Llama-3-8B-Instruct - üíª Will be load from cache\n","\t‚ö° microsoft/Phi-3-mini-4k-instruct - üíª Will be load from cache\n","\t‚ö° google/gemma-3-4b-it - üíª Will be load from cache\n","\n","\n","‚å®Ô∏è SYSTEM PROMPT:\n","============================================================\n","\n","        You are my highly intelligent personal assistant.\n","        Your mission is to engage in a continuous role-playing conversation where you will act as me.\n","        From this moment forward, the user will be playing the role of various recruiters contacting me.\n","        You must analyze the messages and generate appropriate responses as if you were me talking with a recruiter.\n","        Do not break character. Do not mention that you are an AI or an assistant in your replies to the recruiters.\n","        Analyze this job offer and tell me if it matches my profile:\n","        ***I am a Data and AI Engineer with extensive experience in building robust ELT pipelines and developing cutting-edge Generative AI solutions.\n","        My key skills include designing systems with Retrieval-Augmented Generation (RAG) and AI Agents using frameworks like Semantic Kernel and LangChain.\n","        I am proficient in data engineering with PySpark, SQL, and Apache Airflow,\n","        and highly skilled in cloud platforms, particularly Microsoft Azure (Data Factory, Databricks, AI Services) and GCP (BigQuery, Composer).\n","        I also possess strong backend development experience using Python and FastAPI to build and deploy services.***\n","        \n","\n","\n","üéØ BENCHMARK SCENARIOS:\n","============================================================\n","  1. Perfect Match: \n","            Hi! I hope you are doing well.\n","            I came across your profile and I am impressed by your background in AI and data engineering.\n","            I have an exciting opportunity for a Senior Data Engineer position at a fast-growing fintech company.\n","            The role involves working with Python, cloud technologies, and building ML pipelines.\n","            The salary range is ‚Ç¨60,000-65,000 and it is 100% remote.\n","            Would you be interested in learning more?\n","            \n","  2. Generic Message: \n","            Hello, are you currently open to new opportunities?\n","            \n","  3. Wrong Match: \n","            We have a Java developer position available at our company.\n","            5 years experience required, on-site work in London, competitive salary.\n","            Interested?\n","            \n","\n","‚è∏Ô∏è Set 'run_benchmark = True' above to start the OPTIMIZED benchmark\n","üéØ Multiple test scenarios: READY\n","üìä Detailed comparison: READY\n","üÜò Automatic fallback system: ACTIVE\n","üíæ Your cached models will save significant time!\n","\n","====================================================================================================\n","========================================= END OF PROCESS =========================================\n","====================================================================================================\n"]}],"source":["print(\"=\"*100 )\n","print(\"=\"*41 + \" START OF PROCESS \" + \"=\"*41 )\n","print(\"=\"*100 )\n","\n","# Display environment configuration\n","benchmark.display_environment_config()\n","\n","# Check cache status\n","cached_models = display_cache_status(benchmark.candidate_models)\n","\n","# üéØ SYSTEM PROMPT\n","print(f\"\\n\\n‚å®Ô∏è SYSTEM PROMPT:\")\n","print(\"=\" * 60)\n","print(benchmark.system_prompt)\n","\n","\n","# üéØ TEST SCENARIOS INFO\n","print(f\"\\n\\nüéØ BENCHMARK SCENARIOS:\")\n","print(\"=\" * 60)\n","\n","for i, prompt in enumerate(benchmark.test_prompts):\n","    print(f\"  {i+1}. {prompt['name']}: {prompt['message']}\")\n","\n","\n","######### üöÄ RUN COMPREHENSIVE BENCHMARK  #########\n","run_benchmark = False # ‚úÖ Set to True to test\n","\n","if run_benchmark:\n","    print(\"\\nüîß STARTING BENCHMARK\")\n","    print(f\"üìÅ Results will be saved to: {benchmark.results_folder}\")\n","    print(f\"üïí Benchmark timestamp: {benchmark.benchmark_timestamp}\")\n","\n","    benchmark.benchmark_all_models()\n","    benchmark.display_detailed_results()\n","\n","    print(f\"\\n‚úÖ BENCHMARK COMPLETE!\")\n","    print(f\"üìä Check {benchmark.results_folder}/ for detailed results and comparison history\")\n","else:\n","    print(\"\\n‚è∏Ô∏è Set 'run_benchmark = True' above to start the OPTIMIZED benchmark\")\n","\n","print(\"üéØ Multiple test scenarios: READY\")\n","print(\"üìä Detailed comparison: READY\")\n","print(\"üÜò Automatic fallback system: ACTIVE\")\n","print(\"üíæ Your cached models will save significant time!\\n\")\n","\n","\n","print(\"=\"*100 )\n","print(\"=\"*41 + \" END OF PROCESS \" + \"=\"*41 )\n","print(\"=\"*100 )"]},{"cell_type":"markdown","metadata":{"id":"CSZQ5tdSvOyk"},"source":["## **Phase 5:** Analyzing results"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1753579067244,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"SVENij8tw-2o","outputId":"098fc450-8ce1-4473-bcfa-8d0389799599"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîç BENCHMARK EXPLORER READY!\n"]}],"source":["# üìä BENCHMARK RESULTS EXPLORER\n","# Run this cell AFTER BECHMARKING to explore saved iterations\n","\n","# List all iterations\n","results_folder = f\"{project_path}/benchmark_iterations\"\n","iteration_files = [f for f in os.listdir(results_folder) if f.startswith('benchmark_') and f.endswith('.json')]\n","iteration_files.sort(reverse=True)  # Most recent first\n","\n","\n","def explore_benchmark_history():\n","    \"\"\"Explore saved benchmark iterations and history\"\"\"\n","\n","\n","    if not os.path.exists(results_folder):\n","        print(\"‚ùå No benchmark iterations found. Run the benchmark first!\")\n","        return\n","\n","    print(f\"üìÅ BENCHMARK ITERATIONS FOUND: {len(iteration_files)}\")\n","    print(\"-\" * 60)\n","\n","    if iteration_files:\n","        for i, filename in enumerate(iteration_files[:5], 1):  # Show last 5\n","            timestamp = filename.replace('benchmark_', '').replace('.json', '')\n","            # Parse timestamp\n","            from datetime import datetime\n","            try:\n","                dt = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')\n","                readable_date = dt.strftime('%Y-%m-%d %H:%M:%S')\n","                print(f\"{i}. {filename} ({readable_date})\")\n","            except:\n","                print(f\"{i}. {filename}\")\n","\n","    # Show comparison history if exists\n","    history_file = f\"{results_folder}/benchmark_comparison_history.json\"\n","    if os.path.exists(history_file):\n","        print(f\"\\nüìà COMPARISON HISTORY:\")\n","        with open(history_file, 'r', encoding='utf-8') as f:\n","            history = json.load(f)\n","\n","        print(f\"Total iterations tracked: {len(history['iterations'])}\")\n","        print(f\"Models tracked: {', '.join([m.split('/')[-1] for m in history['models_tracked']])}\")\n","        print(f\"Last updated: {history['last_updated']}\")\n","\n","        if history['iterations']:\n","            latest = history['iterations'][-1]\n","            if 'summary' in latest and 'model_performance_scores' in latest['summary']:\n","                print(f\"\\nüèÜ LATEST PERFORMANCE RANKING:\")\n","                for i, score in enumerate(latest['summary']['model_performance_scores'][:3], 1):\n","                    model_name = score['model'].split('/')[-1]\n","                    print(f\"  {i}. {model_name} (Score: {score['performance_score']})\")\n","\n","    return results_folder, iteration_files\n","\n","def load_specific_iteration(timestamp_or_index=None):\n","    \"\"\"Load a specific benchmark iteration for detailed analysis\"\"\"\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    iteration_files = [f for f in os.listdir(results_folder) if f.startswith('benchmark_2') and f.endswith('.json')]\n","    iteration_files.sort(reverse=True)\n","\n","    if not iteration_files:\n","        print(\"‚ùå No iterations found!\")\n","        return None\n","\n","    # Determine which file to load\n","    if timestamp_or_index is None:\n","        # Load most recent\n","        target_file = iteration_files[0]\n","        print(f\"üìÑ Loading most recent iteration: {target_file}\")\n","    elif isinstance(timestamp_or_index, int):\n","        # Load by index (1-based)\n","        if 1 <= timestamp_or_index <= len(iteration_files):\n","            target_file = iteration_files[timestamp_or_index - 1]\n","            print(f\"üìÑ Loading iteration #{timestamp_or_index}: {target_file}\")\n","        else:\n","            print(f\"‚ùå Invalid index. Available: 1-{len(iteration_files)}\")\n","            return None\n","    else:\n","        # Load by timestamp\n","        target_file = f\"benchmark_{timestamp_or_index}.json\"\n","        if target_file not in iteration_files:\n","            print(f\"‚ùå Timestamp {timestamp_or_index} not found!\")\n","            return None\n","        print(f\"üìÑ Loading iteration: {target_file}\")\n","\n","    # Load the iteration data\n","    with open(f\"{results_folder}/{target_file}\", 'r', encoding='utf-8') as f:\n","        iteration_data = json.load(f)\n","\n","\n","    # Display environment config if available\n","    if 'environment_config' in iteration_data:\n","        env = iteration_data['environment_config']\n","        if env['hardware']['gpu']['available']:\n","            gpu = env['hardware']['gpu']['devices'][0]\n","            gpu_type = env[\"colab_specific\"].get(\"colab_gpu_type\", 'Unknown') if env[\"colab_specific\"][\"is_colab\"] else 'Local'\n","            print(f\"üéÆ Environment: {gpu['name']} ({gpu['memory_total_gb']} GB) - {gpu_type}\")\n","        else:\n","            print(f\"üñ•Ô∏è Environment: CPU-only\")\n","\n","        env_type = \"Google Colab\" if env[\"colab_specific\"][\"is_colab\"] else \"Local/Other\"\n","        print(f\"üåê Platform: {env_type}\")\n","\n","    return iteration_data\n","\n","def compare_environments_performance():\n","    \"\"\"Compare performance across different environment configurations\"\"\"\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    history_file = f\"{results_folder}/benchmark_comparison_history.json\"\n","\n","    if not os.path.exists(history_file):\n","        print(\"‚ùå No comparison history found. Run benchmarks first!\")\n","        return\n","\n","    with open(history_file, 'r', encoding='utf-8') as f:\n","        history = json.load(f)\n","\n","    if len(history['iterations']) < 2:\n","        print(\"‚ùå Need at least 2 benchmark iterations to compare environments\")\n","        return\n","\n","    print(\"üîÑ ENVIRONMENT PERFORMANCE COMPARISON\")\n","    print(\"=\" * 60)\n","\n","    # Group iterations by environment\n","    env_groups = {}\n","\n","    for iteration in history['iterations']:\n","        # Load full iteration data to get environment config\n","        iteration_file = f\"{results_folder}/benchmark_{iteration['timestamp']}.json\"\n","        if os.path.exists(iteration_file):\n","            with open(iteration_file, 'r', encoding='utf-8') as f:\n","                full_data = json.load(f)\n","\n","            if 'environment_config' in full_data:\n","                env = full_data['environment_config']\n","\n","                # Create environment signature\n","                if env['hardware']['gpu']['available']:\n","                    gpu_name = env['hardware']['gpu']['devices'][0]['name']\n","                    gpu_memory = env['hardware']['gpu']['devices'][0]['memory_total_gb']\n","                    colab_type = env[\"colab_specific\"].get(\"colab_gpu_type\", 'Unknown')\n","                    env_signature = f\"{colab_type} ({gpu_name}) - {gpu_memory}GB\"\n","                else:\n","                    env_signature = \"CPU-only\"\n","\n","                if env_signature not in env_groups:\n","                    env_groups[env_signature] = []\n","\n","                env_groups[env_signature].append({\n","                    'timestamp': iteration['timestamp'],\n","                    'datetime': iteration['datetime'],\n","                    'summary': iteration['summary']\n","                })\n","\n","    # Display comparison\n","    for env_sig, iterations in env_groups.items():\n","        print(f\"\\nüéÆ {env_sig}:\")\n","        print(f\"\\tüìä Iterations: {len(iterations)}\")\n","\n","        if iterations and 'model_performance_scores' in iterations[-1]['summary']:\n","            latest_scores = iterations[-1]['summary']['model_performance_scores']\n","            print(f\"\\tüèÜ Best Model: {latest_scores[0]['model'].split('/')[-1]} (Score: {latest_scores[0]['performance_score']})\")\n","\n","            # Calculate average performance across iterations for this environment\n","            if len(iterations) > 1:\n","                avg_inference_times = []\n","                for iter_data in iterations:\n","                    if 'overall_stats' in iter_data['summary']:\n","                        avg_inference_times.append(iter_data['summary']['overall_stats']['avg_inference_time'])\n","\n","                if avg_inference_times:\n","                    avg_time = sum(avg_inference_times) / len(avg_inference_times)\n","                    print(f\"   ‚ö° Avg Inference Time: {avg_time:.2f}s (across {len(avg_inference_times)} runs)\")\n","\n","    # Show environment impact on specific models\n","    print(f\"\\nüìà MODEL PERFORMANCE BY ENVIRONMENT:\")\n","    print(\"-\" * 70)\n","\n","    model_env_performance = {}\n","    for env_sig, iterations in env_groups.items():\n","        for iteration in iterations:\n","            if \"model_performance_scores\" in iteration[\"summary\"]:\n","                for model_score in iteration[\"summary\"][\"model_performance_scores\"]:\n","                    model_name = model_score[\"model\"].split(\"/\")[-1]\n","                    if model_name not in model_env_performance:\n","                        model_env_performance[model_name] = {}\n","\n","                    if env_sig not in model_env_performance[model_name]:\n","                        model_env_performance[model_name][env_sig] = []\n","\n","                    model_env_performance[model_name][env_sig].append(model_score[\"performance_score\"])\n","\n","    # Display model performance across environments\n","    for model_name, env_scores in model_env_performance.items():\n","        print(f\"\\nü§ñ {model_name}:\")\n","        for env_sig, scores in env_scores.items():\n","            avg_score = sum(scores) / len(scores)\n","            runs = len(scores)\n","            print(f\"   {env_sig}: {avg_score:.2f} avg score ({runs} run{'s' if runs != 1 else ''})\")\n","\n","# Quick exploration\n","print(\"üîç BENCHMARK EXPLORER READY!\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1753579067504,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"mWpISuCxFwfE","outputId":"1860ad5d-93fc-41d9-d838-78786e8b8c45"},"outputs":[{"output_type":"stream","name":"stdout","text":["üìÅ BENCHMARK ITERATIONS FOUND: 5\n","------------------------------------------------------------\n","1. benchmark_comparison_history.json\n","2. benchmark_20250725_181432.json (2025-07-25 18:14:32)\n","3. benchmark_20250725_165345.json (2025-07-25 16:53:45)\n","4. benchmark_20250725_082023.json (2025-07-25 08:20:23)\n","5. benchmark_20250724_141407.json (2025-07-24 14:14:07)\n","\n","üìà COMPARISON HISTORY:\n","Total iterations tracked: 1\n","Models tracked: gemma-3-4b-it, Phi-3-mini-4k-instruct, Mistral-7B-Instruct-v0.3, Meta-Llama-3-8B-Instruct\n","Last updated: 2025-07-24 14:44:22\n","\n","üèÜ LATEST PERFORMANCE RANKING:\n","  1. Mistral-7B-Instruct-v0.3 (Score: 6.0)\n","  2. Meta-Llama-3-8B-Instruct (Score: 7.22)\n","  3. Phi-3-mini-4k-instruct (Score: 8.02)\n"]}],"source":["want_to_explore_benchmark_history = True\n","\n","if want_to_explore_benchmark_history:\n","    # Auto-explore if results exist\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    if os.path.exists(results_folder):\n","        explore_benchmark_history()\n","    else:\n","        print(\"‚ùå No benchmark iterations found. Run the benchmark first!\")\n"]},{"cell_type":"markdown","metadata":{"id":"COdYWYdMtTGp"},"source":["# **Stage 3: Adapt & Align Model**"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1753579067520,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"YMpUbXaoWf2q"},"outputs":[],"source":["# Selected LLM model\n","selected_text_generator_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","# selected_text_generator_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","# Selected RAG embedding model\n","embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n","# embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\"\n","\n","# Selected guardrail model\n","input_guardrail_model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n","\n","# Selected output guardrail model\n","# output_guardrail_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","output_guardrail_model_name = \"microsoft/Phi-3-mini-4k-instruct\""]},{"cell_type":"markdown","metadata":{"id":"FIEHKdMe-zZm"},"source":["## ***PHASE 1***- üîß RAG KNOWLEDGE BASE SETUP\n","Creating vectorized knowledge base from CV and job expectations"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1753579067589,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"YfB38JnsPBsT","outputId":"2de0c5a5-92e3-4daa-b364-bc331240feb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["üß† RAG Knowledge Base class initialized!\n","üìö Ready to process CV and job expectations\n"]}],"source":["class RAGKnowledgeBase:\n","\n","    def __init__(self, project_path: str):\n","        self.project_path = project_path\n","        self.embeddings = None\n","        self.vectorstore = None\n","        self.documents = []\n","\n","        # Text splitter configuration for optimal chunking\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=50,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \". \", \"##\"] #, \" \", \"\"]\n","            # separators=[\"##\"]\n","        )\n","\n","    def setup_embeddings(self):\n","        \"\"\"Initialize sentence transformer embeddings optimized for recruitment context\"\"\"\n","        print(\"ü§ñ Initializing embeddings model...\")\n","\n","        # Using a higher-quality model optimized for semantic similarity and retrieval\n","        self.embeddings = HuggingFaceEmbeddings(\n","            model_name=embedding_model_name,\n","            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n","            encode_kwargs={\"normalize_embeddings\": True}\n","        )\n","\n","        print(\"‚úÖ Embeddings model loaded successfully\")\n","        return self.embeddings\n","\n","    def load_and_process_documents(self, cv_content: str, expectations_content: str):\n","        \"\"\"Load and process CV and job expectations into document chunks\"\"\"\n","        print(\"üìÑ Processing documents for RAG...\")\n","\n","        # Create documents with metadata\n","        documents = [\n","            Document(\n","                page_content=cv_content,\n","                metadata={\"source\": \"cv\", \"type\": \"professional_profile\"}\n","            ),\n","            Document(\n","                page_content=expectations_content,\n","                metadata={\"source\": \"job_expectations\", \"type\": \"requirements\"}\n","            )\n","        ]\n","\n","        # Split documents into chunks\n","        self.documents = self.text_splitter.split_documents(documents)\n","\n","        print(f\"‚úÖ Created {len(self.documents)} document chunks\")\n","        for doc in self.documents:\n","            print(f\"   üìã {doc.metadata['source']}: {len(doc.page_content)} chars\")\n","\n","        return self.documents\n","\n","    def create_vectorstore(self):\n","        \"\"\"Create FAISS vectorstore from processed documents\"\"\"\n","        if not self.documents:\n","            raise ValueError(\"No documents loaded. Call load_and_process_documents first.\")\n","\n","        if not self.embeddings:\n","            self.setup_embeddings()\n","\n","        print(\"üîç Creating FAISS vectorstore...\")\n","\n","        # Create vectorstore\n","        self.vectorstore = FAISS.from_documents(\n","            documents=self.documents,\n","            embedding=self.embeddings\n","        )\n","\n","        print(\"‚úÖ Vectorstore created successfully\")\n","        return self.vectorstore\n","\n","    def search_relevant_context(self, query: str, k: int = 3):\n","        \"\"\"Search for relevant context given a query\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vectorstore not created. Call create_vectorstore first.\")\n","\n","        # Perform similarity search\n","        relevant_docs = self.vectorstore.similarity_search(query, k=k)\n","\n","        return relevant_docs\n","\n","    def get_context_string(self, query: str, k: int = 3):\n","        \"\"\"Get formatted context string for prompt injection with job_expectations always included\"\"\"\n","        relevant_docs = self.search_relevant_context(query, k)\n","\n","        context_parts = []\n","\n","        # ALWAYS include job_expectations first (critical but small)\n","        job_expectations_included = False\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source == \"job_expectations\":\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                job_expectations_included = True\n","                break\n","\n","        # If job_expectations wasn't in the retrieved docs, find and add it\n","        if not job_expectations_included:\n","            for doc in self.documents:\n","                if doc.metadata.get(\"source\") == \"job_expectations\":\n","                    content = doc.page_content.strip()\n","                    context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                    break\n","\n","        # Add other relevant documents\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source != \"job_expectations\":  # Skip if already added\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[{source.upper()}]: {content}\")\n","\n","        return \"\\n\\n\".join(context_parts)\n","\n","# Initialize RAG knowledge base\n","rag_kb = RAGKnowledgeBase(project_path)\n","\n","print(\"üß† RAG Knowledge Base class initialized!\")\n","print(\"üìö Ready to process CV and job expectations\")\n"]},{"cell_type":"markdown","metadata":{"id":"8yzAGLDP-zZm"},"source":["## ***PHASE 2*** -  üöÄ SETUP RAG KNOWLEDGE BASE\n","Load and process documents for retrieval"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["88ad41200a5b4c02a897b55af13b9078","3ec005f618ed486ba0b0d607e0ca040d","feaaf14d8bb3489ebedc897e0059e871","7a20ff6d93ff4af98306b048e7289abe","66af426a4a28454e9252140eeeeb8985","d3e65832b96b429489aceaa500e35845","35528ee2c7994847a4059abbaa763705","b01d89e676864bc89fe631b78e374b81","f2f46f7d0e3c4ba899858fa25725fff5","3e936be7d40c476396d3804a260406e7","2e6277696fba47ecbb7b534e52233bb4","6912cca2be3a4f03b99d34073a5f3c07","18b11cf5a7f34e1fae9a1e776eea8363","a3dddeb2d78c4d21bd93fd4724bffd58","e732d81842f34aca933b40f5bac40f5a","b3ae7d5875694fd0bc2ca7db43d65dc4","70d65876d7954873844fc309e909bc56","42fc51267ffd4a928f08bbbc8181af34","411327e1ec644e32bdedce0d827ce584","3a3b87fb514b43fcbdc2cbba598a0e6a","73ccdb9caa274bc0a9d9a58c687d50e8","31e9b252f95b457ab43161b6d4af53ef","f1fe8320637c481f873f0b4505b3a5ef","51412af0c299412a87b917f59ea014f7","363f05f5ec2d43058dea34279d0dc874","4887dd4c75564f7a8d70ef3a890a0a87","7000bd193c8b425aa277443f8f5fd706","54a68d5e970d43a19ca8be700af15289","ba151cadfd134288b8814080e2e181cb","c46d15055068469981a1c5777c7da791","b424fd760e6a4ac6aa3202bd6e8581b5","5d9834bf1b9e4e1b99cb5239c2f71ae4","f7338316b6f4441db1506ad93eb64f35","f4fe373642244480bfbea95082970ece","a28cebb5daa34bb687a99f27824f647d","4d18362b11d94368a79bc74da294cead","1d4a6e524e4e47a783a78721175448e1","e264a7653dfd4214bf921da761fc2999","af45793708fc4473ae46bc47ef80bfc0","5db7362ee91b43d6a9d7f4c1cd6a4f37","1cd1ce90c7d142f6bad3fa45ffdc392e","fb813db52057469c9ec4bd12ab6c059a","095b4055c9834374aa200f4f9e5f0957","71daa99d833c44dfbcb28378f19296db","9bbf9e1bd5b24737bf6680cb4d5151e2","547ffd650a8a44ecb516f117c541ba90","b121c94fc0a94c55b664bf3b262550fd","483283404aa546b19b8906eb12f0f448","de713b474d7c447da363fea6d6142a45","3addd8ce190740c2b5a78407d0e9c39d","0280cff583a647f482fe7b6ca70d5e89","9b0ab7ae7bd74efe8613e9c20e70a1c5","5e87457a60974a86a2a44e110d25aa8a","666f5274ca8e4db1b15edf4f700de8f5","c9c3149e276a466380d3de30581b45fc","0127423fcaec406581629df48ab81e01","7c821a32319048a595f74032cb693f93","1a1e1d3cc2684110b8bb34ca91353963","852f4e41f68a428ea1cc4c26accc1707","e530e9f1dafa443eaac43ec821811265","abbe450404d24715b2f5da0371ede16c","4002cf9bf1904aa2984e5fc276ccc849","4c6705da58d946e5a08d6633cc3b2625","d0522083c62340bc9b5500eb57b19004","251996909e61455980c4296fcaa8f180","bdc80d4883a1434fafcfc6d8ab4acbb3","a51617abe5f4406796270d5469608ef2","570e4d3d11b440b4845cb3c7d63d44ca","197ab49a288e480abbe28b9a9b55047e","019f8f93a3624c23a780d42dbefaba8c","5785bc0676a34cb1acb0bf2d1cf52f65","9a1c60a2e9a14b228d3c5ce82312f7b9","9276e237f66e4b6884e3dc892dd67a09","1b6cc761986940e9a7a1b5bda1ff3341","899d595aa7e6441790a7d8e613bf007c","fc31cf19e2a345daa79ad45a332c4408","a0e642c398ea4f39a55f3bd92a5402e5","6c84a84fdba54e23abe612a8607dddb1","738e9212723a487daef475abaf14fc41","0b3bf52614154d46b96fbdde28315f12","d32fd62bd15d4d03ac8e5623760eae85","cdd8d6eeeefd45aca0b4f25b79a596ee","066aaa418bc64998980af903ab2be2df","195227ab123f4f5b9cde73cff9b5985b","6f3dc38a073a46168ff81fdf6c338919","a215cb8a1ed0441489d165446ad22c17","4b3f0d727d324f639686c72f83bf8a93","1c913c750bbc49d5a5ac09d4353bedfd","ac840310aaa04785934534dc2371cd4c","a354b1d0c82c45878008cf118b043851","109f5257cd74447facfa59eab4a653b7","e0388114435745d6a013430deaaac0fa","c6ef72c9cc3645548ded6bd3cf17d851","7aeaa6e122c94488885e8b3c9aaf7e1c","ee5762e0cfb34efea8e08f1cd1c27d7b","48e24943db7e4823bf6a34b67ba75d14","f873f7fc149e4032b1db2c06a08597e6","0366b69500ec416582b6b69ff4294ad3","d545ddf812f442d69c2aafd7f20107a6","d924c60193e84619aa8f0a4039626226","71198e1856c64e668f61331b6b857ccf","4d3f3801a056454e820e5be4073d0d7e","07ec1400d71e40ddafd6908e5566e020","e85ac3efc7a144f1a1b0b86f46bee851","90c5e6a23c45404a821bf687380fbd2f","aef9a274f745449d934510b95d27f6ab","0fe95f389b474e649c274ed16b4efdf0","a6f0c0dc2ede411a9eb84c593836e35c","dc1e2236d65844c0a01cdff8491ce438","c6ee30804e6f414d911e00a69b23fb79","ae85ffa860e74c9db1e7888a27a33c43","b27e0b9992db494d8932954b671f3ed2","cc82c2bdfe98471f9bcb0ab1411e3209","8db66b18b37044008ee54262de7da4c0","a6b63db1325f44c2abd1577c8551fbc4","385cc9eadb3a4cc1bc75bbfd07701379","8f15b256783a428e804a1025c2c142e5","3a61a0c009b24ddd8d19c95c62ea58a1","ab58961c70c4481cb3909a3a0b11cde4","2cb4e4202273415395fcb55cf4b9aa59","255f55a4bfa044fb8f8ca1d69fb46e52"]},"executionInfo":{"elapsed":11121,"status":"ok","timestamp":1753579078713,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"xzTD-RelPBsT","outputId":"f52d6a11-2933-44dc-baef-fa2429a4cacb"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîß Setting up RAG Knowledge Base...\n","ü§ñ Initializing embeddings model...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-11-2240822892.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","  self.embeddings = HuggingFaceEmbeddings(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ad41200a5b4c02a897b55af13b9078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6912cca2be3a4f03b99d34073a5f3c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1fe8320637c481f873f0b4505b3a5ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4fe373642244480bfbea95082970ece"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bbf9e1bd5b24737bf6680cb4d5151e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0127423fcaec406581629df48ab81e01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a51617abe5f4406796270d5469608ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c84a84fdba54e23abe612a8607dddb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac840310aaa04785934534dc2371cd4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d924c60193e84619aa8f0a4039626226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae85ffa860e74c9db1e7888a27a33c43"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Embeddings model loaded successfully\n","üìÑ Processing documents for RAG...\n","‚úÖ Created 24 document chunks\n","   üìã cv: 193 chars\n","   üìã cv: 18 chars\n","   üìã cv: 479 chars\n","   üìã cv: 239 chars\n","   üìã cv: 231 chars\n","   üìã cv: 412 chars\n","   üìã cv: 217 chars\n","   üìã cv: 371 chars\n","   üìã cv: 190 chars\n","   üìã cv: 451 chars\n","   üìã cv: 433 chars\n","   üìã cv: 340 chars\n","   üìã cv: 216 chars\n","   üìã cv: 401 chars\n","   üìã cv: 391 chars\n","   üìã cv: 348 chars\n","   üìã cv: 287 chars\n","   üìã cv: 487 chars\n","   üìã cv: 40 chars\n","   üìã cv: 378 chars\n","   üìã cv: 457 chars\n","   üìã cv: 156 chars\n","   üìã cv: 255 chars\n","   üìã job_expectations: 326 chars\n","üîç Creating FAISS vectorstore...\n","‚úÖ Vectorstore created successfully\n","\n","üß™ Testing retrieval system...\n","\n","‚ùì Query: What are my technical skills in data engineering?\n","üìÑ Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between ‚Ç¨60,000 and ‚Ç¨65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: ## I.T. Skills\n","Generative AI: RAG (Retrieval-Augmented Generation), AI Agents, LLMs, Prompt Engineering, Semantic Kernel, LangChain, Fine Tuning, OpenAI APIs..\n","Data Engineering: ETL/ELT Processes, Data Lake, Data Warehouse, Apache Airflow, Compositor (GCP), Spark, Hadoop, Hive, PySpark.\n","\n","[CV]: . An AI & Data Engineer with experience in Generative AI, cloud platforms (Azure, GCP), and ELT processes. Skilled in project management, APIs, and agile methodologies with a strong analytical mindset, a fast learner, and results-oriented....\n","\n","‚ùì Query: What technologies do I work with?\n","üìÑ Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between ‚Ç¨60,000 and ‚Ç¨65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: ## Languages\n","Spanish: Native\n","English: Professional Working Proficiency\n","\n","\n","## Main Projects\n","Generative AI apps with Semantic Kernel / Beam Suntory\n","Development of Digital Humans using LLMs / BeHumans\n","Cloud-based ETL pipeline with Databricks & Azure / Minsait\n","\n","[CV]: Project Manager\n","Digittude Technology Solutions SL\n","January 2024 - August 2024 (Madrid, Spain)\n","CLIENT: BE HUMANS\n","Led Generative AI initiatives for the development and deployment of Digital Human (DH) solutions for various clients.\n","Utilized the platform's API to technically configure and customize Digital Humans, adapting their functionalities to diverse client use cases....\n","\n","‚ùì Query: What type of job am I looking for?\n","üìÑ Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between ‚Ç¨60,000 and ‚Ç¨65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: . An AI & Data Engineer with experience in Generative AI, cloud platforms (Azure, GCP), and ELT processes. Skilled in project management, APIs, and agile methodologies with a strong analytical mindset, a fast learner, and results-oriented.\n","\n","[CV]: Passionate about Artificial Intelligence and Data Engineering, with experience in developing and deploying robust backend systems and cutting-edge solutions in Generative AI, Cloud Computing, and Big Data. Throughout my career at companies like Digittude Technology and Minsait, I have contributed to projects with advanced AI architectures like Retrieval-Augmented Generation (RAG) and AI agent-based systems, using orchestration frameworks such as Semantic Kernel and LangChain...\n","\n","‚úÖ RAG Knowledge Base ready for AI Assistant!\n"]}],"source":["# Setup embeddings and process documents\n","print(\"üîß Setting up RAG Knowledge Base...\")\n","rag_kb.setup_embeddings()\n","\n","# Load and process documents\n","if cv_content and expectations_content:\n","    rag_kb.load_and_process_documents(cv_content, expectations_content)\n","    rag_kb.create_vectorstore()\n","\n","    print(\"\\nüß™ Testing retrieval system...\")\n","\n","    # Test retrieval with sample queries\n","    test_queries = [\n","        \"What are my technical skills in data engineering?\",\n","        \"What technologies do I work with?\",\n","        \"What type of job am I looking for?\"\n","    ]\n","\n","    for query in test_queries:\n","        print(f\"\\n‚ùì Query: {query}\")\n","        context = rag_kb.get_context_string(query, k=2)\n","        # print(f\"üìÑ Retrieved context (first 200 chars): {context[:200]}...\")\n","        print(f\"üìÑ Retrieved context: \\n{context}...\")\n","\n","    print(\"\\n‚úÖ RAG Knowledge Base ready for AI Assistant!\")\n","else:\n","    print(\"‚ùå CV or expectations content missing. Check data loading.\")\n"]},{"cell_type":"markdown","metadata":{"id":"p_er1dHPjShl"},"source":["## ***PHASE 3:*** Input Guardrail System\n","\n","Implementing an intelligent guardrail that performs **Intent Detection** to classify recruiter messages as either:\n","- **Generic messages** ‚Üí State: \"pending_details\" ‚Üí Request more information\n","- **Concrete job offers** ‚Üí Pass to RAG system for analysis\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1753579078771,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"Z2vZjGwujShl","outputId":"f5a9ea8d-36fb-459c-938c-f76550aa4421"},"outputs":[{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Input Guardrail system initialized!\n","üìä Message classification: GENERIC vs CONCRETE_OFFER\n","üéØ State management: pending_details, analyzing, passed, stand_by, finished\n","‚ö° Using Phi-3-mini for fast intent detection\n"]}],"source":["from enum import Enum\n","from dataclasses import dataclass\n","from typing import Dict, Any, Tuple\n","\n","class MessageType(Enum):\n","    GENERIC = \"generic\"\n","    CONCRETE_OFFER = \"concrete_offer\"\n","\n","class ConversationState(Enum):\n","    PENDING_DETAILS = \"pending_details\"\n","    ANALYZING = \"analyzing\"\n","    PASSED = \"passed\"\n","    STAND_BY = \"stand_by\"\n","    FINISHED = \"finished\"\n","\n","@dataclass\n","class GuardrailResult:\n","    message_type: MessageType\n","    confidence: float\n","    state: ConversationState\n","    response: str = None\n","    should_continue_to_rag: bool = False\n","\n","class InputGuardrail:\n","    \"\"\"\n","    Intelligent guardrail using Phi-3-mini for fast intent detection\n","    and initial response generation for generic messages\n","    \"\"\"\n","\n","    def __init__(self, cache_path: str):\n","        self.cache_path = cache_path\n","        self.model_name = input_guardrail_model_name # \"microsoft/Phi-3-mini-4k-instruct\"\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Classification prompt for intent detection\n","        self.classification_prompt = \"\"\"\n","You are an expert at analyzing recruiter messages. Your task is to classify if a message contains a CONCRETE JOB OFFER or is a GENERIC message.\n","\n","Classification criteria:\n","- CONCRETE OFFER: Contains specific job details like role title, responsibilities, technologies, salary range, company name, or detailed requirements\n","- GENERIC: Vague messages like \"Are you open to opportunities?\", \"I'd like to connect\", or general networking without specific job details\n","\n","Examples:\n","- \"Hi, are you open to new opportunities?\" ‚Üí GENERIC\n","- \"We have a Senior Data Engineer role using Python and AWS, ‚Ç¨70k-80k remote\" ‚Üí CONCRETE OFFER\n","- \"I'd love to connect and discuss potential opportunities\" ‚Üí GENERIC\n","- \"Java Developer position, 5+ years experience, London office, competitive salary\" ‚Üí CONCRETE OFFER\n","\n","Analyze this recruiter message and respond with ONLY:\n","CLASSIFICATION: [GENERIC or CONCRETE_OFFER]\n","CONFIDENCE: [0.0-1.0]\n","\n","Message to analyze:\n","{message}\n","\n","Response:\"\"\"\n","\n","        # Response template for generic messages\n","        self.generic_response_template = \"\"\"Hello,\n","\n","Thank you for reaching out! I appreciate your interest in connecting.\n","\n","I'm currently open to exploring new opportunities that align with my background in Data Engineering and AI. However, I'd need more specific details about the role to properly evaluate if it's a good match.\n","\n","Could you please share more information about:\n","- The specific role title and main responsibilities\n","- Required technical skills and technologies\n","- Company type and industry\n","- Salary range and working arrangements (remote/hybrid/on-site)\n","- Location requirements\n","\n","I look forward to learning more about the opportunity!\n","\n","Best regards,\n","Cristopher\"\"\"\n","\n","    def load_model(self):\n","        \"\"\"Load Phi-3-mini model for fast classification\"\"\"\n","        print(f\"\\nüõ°Ô∏è Loading guardrail model: {self.model_name}\")\n","\n","        # Check cache\n","        cached_models = check_cached_models(self.cache_path)\n","        model_folder_space = self.model_name.replace('/', ' ')\n","\n","        if model_folder_space in cached_models:\n","            print(\"‚ö° Loading from cache...\")\n","            model_cache_dir = os.path.join(self.cache_path, model_folder_space)\n","        else:\n","            print(\"üì• Downloading model...\")\n","            model_cache_dir = create_model_cache_dir(self.model_name, self.cache_path)\n","\n","        # Quantization for efficiency\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(\n","                self.model_name,\n","                cache_dir=model_cache_dir\n","            )\n","            if self.tokenizer.pad_token is None:\n","                self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                self.model_name,\n","                quantization_config=bnb_config,\n","                device_map=\"auto\",\n","                trust_remote_code=True,\n","                cache_dir=model_cache_dir,\n","                attn_implementation=\"eager\"  # Phi-3 optimization\n","            )\n","\n","            print(\"‚úÖ Guardrail model loaded successfully!\")\n","            return self.model, self.tokenizer\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error loading guardrail model: {str(e)}\")\n","            raise\n","\n","    def classify_message(self, recruiter_message: str) -> Tuple[MessageType, float]:\n","        \"\"\"Classify recruiter message as generic or concrete offer\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Guardrail model not loaded. Call load_model() first.\")\n","\n","        # Format classification prompt\n","        prompt = self.classification_prompt.format(message=recruiter_message.strip())\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\", # return tensors in pytorch format\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=50, # max number of tokens to generate\n","                    do_sample=False,  # Deterministic for classification\n","                    temperature=0.1, # temperature for sampling\n","                    pad_token_id=self.tokenizer.eos_token_id, # pad token id\n","                    use_cache=False  # Phi-3 optimization\n","                )\n","\n","            # Decode response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            classification_response = full_response.replace(prompt, \"\").strip()\n","\n","            # Parse classification result\n","            message_type, confidence = self._parse_classification(classification_response)\n","\n","            return message_type, confidence\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Classification error: {str(e)}\")\n","            # Fallback: basic keyword detection\n","            return self._fallback_classification(recruiter_message)\n","\n","    def _parse_classification(self, response: str) -> Tuple[MessageType, float]:\n","        \"\"\"Parse model classification response\"\"\"\n","        response_lower = response.lower()\n","\n","        # Extract classification\n","        if \"concrete_offer\" in response_lower or \"concrete offer\" in response_lower:\n","            message_type = MessageType.CONCRETE_OFFER\n","        elif \"generic\" in response_lower:\n","            message_type = MessageType.GENERIC\n","        else:\n","            # Fallback based on response content\n","            if any(keyword in response_lower for keyword in [\"specific\", \"detailed\", \"role\", \"position\", \"salary\"]):\n","                message_type = MessageType.CONCRETE_OFFER\n","            else:\n","                message_type = MessageType.GENERIC\n","\n","        # Extract confidence\n","        confidence = 0.8  # Default confidence\n","        try:\n","            # Look for confidence pattern\n","            import re\n","            confidence_match = re.search(r'confidence:\\s*([0-9.]+)', response_lower)\n","            if confidence_match:\n","                confidence = float(confidence_match.group(1))\n","                confidence = max(0.0, min(1.0, confidence))  # Clamp to [0,1]\n","        except:\n","            pass\n","\n","        return message_type, confidence\n","\n","    def _fallback_classification(self, message: str) -> Tuple[MessageType, float]:\n","        \"\"\"Fallback classification using keyword detection\"\"\"\n","        message_lower = message.lower()\n","\n","        # Keywords indicating concrete offers\n","        concrete_keywords = [\n","            \"position\", \"role\", \"job\", \"salary\", \"‚Ç¨\", \"$\", \"¬£\", \"experience\",\n","            \"requirements\", \"responsibilities\", \"company\", \"team\", \"technologies\",\n","            \"remote\", \"on-site\", \"hybrid\", \"years\", \"senior\", \"junior\", \"developer\",\n","            \"engineer\", \"analyst\", \"manager\", \"python\", \"java\", \"javascript\",\n","            \"cloud\",\"aws\", \"azure\", \"gcp\", \"react\", \"angular\", \"node\", \"sql\", \"nosql\",\n","            \"data\", \"ai\"\n","        ]\n","\n","        # Keywords indicating generic messages\n","        generic_keywords = [\n","            \"open to\", \"opportunities\", \"connect\", \"network\", \"interested in\",\n","            \"catch up\", \"chat\", \"discuss\", \"explore\", \"available\", \"looking for\"\n","        ]\n","\n","        concrete_score = sum(1 for keyword in concrete_keywords if keyword in message_lower)\n","        generic_score = sum(1 for keyword in generic_keywords if keyword in message_lower)\n","\n","        if concrete_score > generic_score and concrete_score >= 2:\n","            return MessageType.CONCRETE_OFFER, 0.7\n","        else:\n","            return MessageType.GENERIC, 0.6\n","\n","    def process_message(self, recruiter_message: str) -> GuardrailResult:\n","        \"\"\"Process recruiter message through the guardrail system\"\"\"\n","\n","        print(\"üõ°Ô∏è Processing message through guardrail...\")\n","\n","        # Step 1: Classify message type\n","        message_type, confidence = self.classify_message(recruiter_message)\n","\n","        print(f\"   üìä Classification: {message_type.value} (confidence: {confidence:.2f})\")\n","\n","        # Step 2: Handle based on classification\n","        if message_type == MessageType.GENERIC:\n","            # Generic message: return response and set pending_details state\n","            return GuardrailResult(\n","                message_type=message_type,\n","                confidence=confidence,\n","                state=ConversationState.PENDING_DETAILS,\n","                response=self.generic_response_template,\n","                should_continue_to_rag=False\n","            )\n","\n","        else:  # CONCRETE_OFFER\n","            # Concrete offer: pass to RAG system\n","            return GuardrailResult(\n","                message_type=message_type,\n","                confidence=confidence,\n","                state=ConversationState.ANALYZING,\n","                response=None,\n","                should_continue_to_rag=True\n","            )\n","\n","# Initialize Input Guardrail\n","input_guardrail = InputGuardrail(cache_path=cache_path)\n","\n","print(\"üõ°Ô∏è Input Guardrail system initialized!\")\n","print(\"üìä Message classification: GENERIC vs CONCRETE_OFFER\")\n","print(\"üéØ State management: pending_details, analyzing, passed, stand_by, finished\")\n","print(\"‚ö° Using Phi-3-mini for fast intent detection\")\n"]},{"cell_type":"markdown","metadata":{"id":"VHTP08eG-zZm"},"source":["## ***PHASE 4:*** üõ°Ô∏è OUTPUT GUARDRAIL SYSTEM\n","Validates and improves response naturalness"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1753579078806,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"xAvoOvVW-zZm","outputId":"78c54ca6-376f-402e-daae-8e8fd9d7d9e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Output Guardrail system initialized!\n","üìù Response validation: First person usage, no placeholders, natural tone\n","üîÑ Iterative improvement: Up to k=5 iterations\n","üéØ Fallback generation: Guardrail creates corrected version if needed\n","‚ö° Using Meta-Llama-3-8B-Instruct for validation and correction\n","üí° Alternative models available: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\n"]}],"source":["from typing import List, Dict, Tuple\n","import re\n","\n","class OutputGuardrail:\n","    \"\"\"\n","    Output guardrail that validates response naturalness and ensures first-person perspective.\n","    Uses Meta-Llama-3-8B-Instruct for validation and correction.\n","    \"\"\"\n","\n","    def __init__(self, cache_path: str):\n","        self.cache_path = cache_path\n","        self.model_name = output_guardrail_model_name #\"meta-llama/Meta-Llama-3-8B-Instruct\"\n","        # Alternative models for consideration:\n","        # self.model_name = \"google/gemma-3-4b-it\"  # Alternative option 1\n","        # self.model_name = \"microsoft/Phi-3-mini-4k-instruct\"  # Alternative option 2 (but already used in input guardrail)\n","\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Validation prompt for checking naturalness\n","        self.validation_prompt = \"\"\"\n","You are an expert at validating professional email responses for naturalness and authenticity.\n","\n","Your task is to check if a response message follows these CRITICAL rules:\n","1. FIRST PERSON: Must use \"I\", \"my\", \"me\" - NEVER \"the candidate\", \"his/her\", \"candidate's\"\n","2. NO PLACEHOLDERS: Must NOT contain \"[recruiter name]\", \"[name]\", or any text in brackets\n","3. NATURAL TONE: Must sound like a real person wrote it, not an AI\n","\n","Analyze this response and identify ANY violations:\n","\n","RESPONSE TO CHECK:\n","{response}\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List specific problems found, or \"None\"]\n","\n","Examples of ISSUES:\n","- \"Uses 'the candidate' instead of 'I'\"\n","- \"Contains placeholder '[recruiter name]'\"\n","- \"Uses 'candidate's experience' instead of 'my experience'\"\n","\"\"\"\n","\n","        # Correction prompt for fixing issues\n","        self.correction_prompt = \"\"\"\n","You are an expert at writing natural, professional recruitment responses.\n","\n","Fix this response to be completely natural and authentic. CRITICAL REQUIREMENTS:\n","1. Use ONLY first person: \"I\", \"my\", \"me\" (NEVER \"the candidate\", \"candidate's\", etc.)\n","2. Remove ALL placeholders like \"[recruiter name]\", \"[name]\", etc.\n","3. Keep the same professional tone and content intent\n","4. Make it sound like a real person wrote it\n","\n","ORIGINAL RESPONSE:\n","{original_response}\n","\n","IDENTIFIED ISSUES:\n","{issues}\n","\n","RECRUITER MESSAGE CONTEXT:\n","{recruiter_message}\n","\n","Write a corrected version that fixes ALL issues while maintaining the same meaning and professional tone:\n","\"\"\"\n","\n","    def load_model(self):\n","        \"\"\"Load Meta-Llama-3-8B-Instruct model for validation and correction\"\"\"\n","        print(f\"\\nüõ°Ô∏è Loading output guardrail model: {self.model_name}\")\n","\n","        # Check cache\n","        cached_models = check_cached_models(self.cache_path)\n","        model_folder_space = self.model_name.replace('/', ' ')\n","\n","        if model_folder_space in cached_models:\n","            print(\"‚ö° Loading from cache...\")\n","            model_cache_dir = os.path.join(self.cache_path, model_folder_space)\n","        else:\n","            print(\"üì• Downloading model...\")\n","            model_cache_dir = create_model_cache_dir(self.model_name, self.cache_path)\n","\n","        # Quantization for efficiency\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(\n","                self.model_name,\n","                cache_dir=model_cache_dir\n","            )\n","            if self.tokenizer.pad_token is None:\n","                self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                self.model_name,\n","                quantization_config=bnb_config,\n","                device_map=\"auto\",\n","                trust_remote_code=True,\n","                cache_dir=model_cache_dir\n","            )\n","\n","            print(\"‚úÖ Output guardrail model loaded successfully!\")\n","            return self.model, self.tokenizer\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error loading output guardrail model: {str(e)}\")\n","            raise\n","\n","    def validate_response(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Validate response for naturalness and first-person perspective\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            # Fallback to rule-based validation if model not loaded\n","            return self._rule_based_validation(response)\n","\n","        prompt = self.validation_prompt.format(response=response.strip())\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=100,\n","                    do_sample=False,\n","                    temperature=0.1,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and parse validation result\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            validation_response = full_response.replace(prompt, \"\").strip()\n","\n","            return self._parse_validation_result(validation_response)\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Validation error: {str(e)}. Using fallback validation.\")\n","            return self._rule_based_validation(response)\n","\n","    def _rule_based_validation(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Fallback rule-based validation\"\"\"\n","        issues = []\n","\n","        # Check for third person references\n","        third_person_patterns = [\n","            r'\\bthe candidate\\b',\n","            r'\\bcandidate\\'s\\b',\n","            r'\\bhis/her\\b',\n","            r'\\bhis or her\\b',\n","            r'\\bhis\\b(?!\\s+name)',  # \"his\" but not \"his name\"\n","            r'\\bher\\b(?!\\s+email)', # \"her\" but not \"her email\"\n","        ]\n","\n","        for pattern in third_person_patterns:\n","            if re.search(pattern, response, re.IGNORECASE):\n","                issues.append(f\"Uses third person reference: '{pattern}'\")\n","\n","        # Check for placeholders\n","        placeholder_patterns = [\n","            r'\\[recruiter name\\]',\n","            r'\\[name\\]',\n","            r'\\[.*?\\]',  # Any text in square brackets\n","        ]\n","\n","        for pattern in placeholder_patterns:\n","            if re.search(pattern, response, re.IGNORECASE):\n","                issues.append(f\"Contains placeholder: '{pattern}'\")\n","\n","        return len(issues) == 0, issues\n","\n","    def _parse_validation_result(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Parse model validation response\"\"\"\n","        response_lower = response.lower()\n","\n","        # Extract validation status\n","        is_valid = \"validation: pass\" in response_lower or \"pass\" in response_lower\n","\n","        # Extract issues\n","        issues = []\n","        if not is_valid:\n","            # Try to extract issues section\n","            issues_match = re.search(r'issues:\\s*(.+?)(?=\\n|$)', response, re.IGNORECASE | re.DOTALL)\n","            if issues_match:\n","                issues_text = issues_match.group(1).strip()\n","                if issues_text.lower() != \"none\":\n","                    # Split by common delimiters\n","                    issues = [issue.strip() for issue in re.split(r'[,;-]|\\n', issues_text) if issue.strip()]\n","\n","            # Fallback: use rule-based validation\n","            if not issues:\n","                _, issues = self._rule_based_validation(response)\n","\n","        return is_valid, issues\n","\n","    def correct_response(self, original_response: str, issues: List[str], recruiter_message: str) -> str:\n","        \"\"\"Generate corrected response using the model\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            return self._rule_based_correction(original_response)\n","\n","        issues_text = \"; \".join(issues) if issues else \"General naturalness improvements needed\"\n","\n","        prompt = self.correction_prompt.format(\n","            original_response=original_response,\n","            issues=issues_text,\n","            recruiter_message=recruiter_message\n","        )\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=400,\n","                    do_sample=True,\n","                    temperature=0.5,\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode corrected response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            corrected_response = full_response.replace(prompt, \"\").strip()\n","\n","            return corrected_response\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Correction error: {str(e)}. Using fallback correction.\")\n","            return self._rule_based_correction(original_response)\n","\n","    def _rule_based_correction(self, response: str) -> str:\n","        \"\"\"Fallback rule-based correction\"\"\"\n","        corrected = response\n","\n","        # Fix third person references\n","        corrections = [\n","            (r'\\bthe candidate\\'s\\b', 'my'),\n","            (r'\\bthe candidate\\b', 'I'),\n","            (r'\\bcandidate\\'s\\b', 'my'),\n","            (r'\\bhis/her\\b', 'my'),\n","            (r'\\bhis or her\\b', 'my'),\n","        ]\n","\n","        for pattern, replacement in corrections:\n","            corrected = re.sub(pattern, replacement, corrected, flags=re.IGNORECASE)\n","\n","        # Remove common placeholders\n","        corrected = re.sub(r'\\[recruiter name\\]', '', corrected, flags=re.IGNORECASE)\n","        corrected = re.sub(r'\\[name\\]', '', corrected, flags=re.IGNORECASE)\n","\n","        # Clean up any double spaces\n","        corrected = re.sub(r'\\s+', ' ', corrected).strip()\n","\n","        return corrected\n","\n","    def validate_and_improve_response(self, original_response: str, recruiter_message: str, max_iterations: int = 5) -> str:\n","        \"\"\"Main method: validate and iteratively improve response naturalness\"\"\"\n","\n","        print(f\"   üîç Validating response naturalness...\")\n","\n","        current_response = original_response\n","        iteration = 0\n","\n","        while iteration < max_iterations:\n","            iteration += 1\n","            print(f\"      üîÑ Iteration {iteration}/{max_iterations}\")\n","\n","            # Validate current response\n","            is_valid, issues = self.validate_response(current_response)\n","\n","            if is_valid:\n","                print(f\"      ‚úÖ Response passed validation on iteration {iteration}\")\n","                return current_response\n","\n","            print(f\"      ‚ö†Ô∏è Issues found: {'; '.join(issues)}\")\n","\n","            # Correct the response\n","            current_response = self.correct_response(current_response, issues, recruiter_message)\n","\n","        # If we reach here, we've exhausted max_iterations\n","        print(f\"      üö® Max iterations ({max_iterations}) reached. Generating final corrected version...\")\n","\n","        # Generate final corrected version using guardrail\n","        final_response = self.correct_response(original_response, [\"Comprehensive naturalness fix\"], recruiter_message)\n","\n","        return final_response\n","\n","# Initialize Output Guardrail\n","output_guardrail = OutputGuardrail(cache_path=cache_path)\n","\n","print(\"üõ°Ô∏è Output Guardrail system initialized!\")\n","print(\"üìù Response validation: First person usage, no placeholders, natural tone\")\n","print(\"üîÑ Iterative improvement: Up to k=5 iterations\")\n","print(\"üéØ Fallback generation: Guardrail creates corrected version if needed\")\n","print(\"‚ö° Using Meta-Llama-3-8B-Instruct for validation and correction\")\n","print(\"üí° Alternative models available: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\")\n"]},{"cell_type":"markdown","metadata":{"id":"1PXmGeOhjShl"},"source":["## ***PHASE 5:*** AI Assistant with Guardrail Integration\n","\n","Integrating the input guardrail with the RAG system and implementing the complete business logic including match scoring and state management.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1753579078838,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"Vsg3banCjShl"},"outputs":[],"source":["class AIRecruiterAssistant:\n","    \"\"\"\n","    Complete AI Assistant that implements the full business logic:\n","    1. Input Guardrail (Generic vs Concrete detection)\n","    2. RAG-powered analysis for concrete offers\n","    3. Match scoring calculation\n","    4. State-based decision making\n","    \"\"\"\n","\n","    def __init__(self, model_name: str, rag_knowledge_base: RAGKnowledgeBase,\n","                 input_guardrail: InputGuardrail, output_guardrail: OutputGuardrail, cache_path: str):\n","        self.model_name = model_name\n","        self.rag_kb = rag_knowledge_base\n","        self.guardrail = input_guardrail\n","        self.output_guardrail = output_guardrail\n","        self.cache_path = cache_path\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Enhanced system prompts for different scenarios\n","        self.match_scoring_prompt = \"\"\"\n","You are an expert career advisor analyzing job fit. Your task is to calculate a precise match score between a job offer and a candidate's profile.\n","\n","SCORING CRITERIA (0-100%):\n","- Technical Skills Match (30%): How well do required technologies align with candidate's expertise?\n","- Role Type Match (25%): Does the role type (Data Engineer, AI Engineer, etc.) match the candidate's focus?\n","- Salary Expectations (20%): How well does the offered range align with expectations?\n","- Work Arrangement (10%): Remote/hybrid preferences alignment\n","- Experience Level (15%): Seniority level match\n","\n","CANDIDATE CONTEXT:\n","{context}\n","\n","JOB OFFER TO ANALYZE:\n","{job_offer}\n","\n","Calculate a precise match score (0-100) and provide brief reasoning for each criteria.\n","\n","Response format:\n","MATCH_SCORE: [0-100]\n","TECHNICAL_SKILLS: [score] - [brief reason]\n","ROLE_TYPE: [score] - [brief reason]\n","SALARY: [score] - [brief reason]\n","WORK_ARRANGEMENT: [score] - [brief reason]\n","EXPERIENCE: [score] - [brief reason]\n","OVERALL_REASONING: [2-3 sentences explaining the fit]\"\"\"\n","\n","        # Response templates for different match score ranges\n","        self.response_templates = {\n","            \"passed\": \"\"\"\n","Hello,\n","\n","Thank you for this excellent opportunity! This position looks like a fantastic fit for my background and experience.\n","\n","{specific_fit_reasons}\n","\n","I'm very interested in moving forward. Could we schedule a conversation to discuss this further?\n","\n","I'm generally available for:\n","- Quick introductory call (15-30 minutes)\n","- Technical discussion session\n","- Meet-and-greet with the team\n","\n","Please let me know your availability and preferred format for our initial conversation.\n","\n","Looking forward to connecting!\n","\n","Best regards,\n","Cristopher\"\"\",\n","\n","            \"stand_by\": \"\"\"\n","Hello,\n","\n","Thank you for sharing this opportunity. The role seems interesting and I appreciate you thinking of me.\n","\n","{specific_considerations}\n","\n","I'd like to take some time to review this opportunity more thoroughly and would prefer to discuss the details in a brief call before making any commitments.\n","\n","Could you provide a bit more information about {additional_info_needed}? This would help me better understand if it's the right fit.\n","\n","I'll get back to you within the next few days with my thoughts.\n","\n","Best regards,\n","Cristopher\"\"\",\n","\n","            \"finished\": \"\"\"\n","Hello,\n","\n","Thank you for considering me for this opportunity. I appreciate you taking the time to reach out.\n","\n","After reviewing the position details, I don't think this role is the best fit for my current career direction and expertise.\n","\n","{mismatch_explanation}\n","\n","I'm primarily focused on opportunities involving {ideal_profile_summary}, ideally in a {preferred_work_style} setting.\n","\n","I'd be happy to stay connected for future opportunities that might be a better alignment. Thank you again for thinking of me!\n","\n","Best regards,\n","Cristopher\"\"\"\n","        }\n","\n","    def load_models(self):\n","        \"\"\"Load input guardrail, output guardrail, and main models\"\"\"\n","        print(\"üöÄ Loading Enhanced AI Assistant models...\")\n","\n","        # Load input guardrail model first\n","        self.guardrail.load_model()\n","\n","        # Load output guardrail model\n","        self.output_guardrail.load_model()\n","\n","        # Load main model using benchmark's logic\n","        print(f\"\\nüì• Loading main model: {self.model_name}\")\n","        benchmark = CacheAwareModelBenchmark()\n","        self.model, self.tokenizer = benchmark.load_model_with_quantization(self.model_name)\n","\n","        print(\"‚úÖ All models loaded successfully!\")\n","\n","    def calculate_match_score(self, recruiter_message: str) -> Dict[str, Any]:\n","        \"\"\"Calculate detailed match score for a concrete job offer\"\"\"\n","\n","        print(\"üìä Calculating match score...\")\n","\n","        # Get relevant context\n","        context = self.rag_kb.get_context_string(recruiter_message, k=3)\n","\n","        # Format scoring prompt\n","        scoring_prompt = self.match_scoring_prompt.format(\n","            context=context,\n","            job_offer=recruiter_message\n","        )\n","\n","        try:\n","            inputs = self.tokenizer(\n","                scoring_prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=300,\n","                    do_sample=True,\n","                    temperature=0.3,  # Lower temperature for more consistent scoring\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and parse scoring response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            scoring_response = full_response.replace(scoring_prompt, \"\").strip()\n","\n","            # Parse the scoring response\n","            match_data = self._parse_match_score(scoring_response)\n","            match_data[\"context_used\"] = context\n","\n","            return match_data\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Error calculating match score: {str(e)}\")\n","            # Fallback scoring based on keywords\n","            return self._fallback_match_scoring(recruiter_message)\n","\n","    def _parse_match_score(self, response: str) -> Dict[str, Any]:\n","        \"\"\"Parse match scoring response from the model\"\"\"\n","        import re\n","\n","        # Extract overall match score\n","        match_score = 50  # Default fallback\n","        match_pattern = re.search(r'MATCH_SCORE:\\s*(\\d+)', response)\n","        if match_pattern:\n","            match_score = int(match_pattern.group(1))\n","            match_score = max(0, min(100, match_score))  # Clamp to [0,100]\n","\n","        # Extract component scores\n","        components = {}\n","        component_patterns = {\n","            \"technical_skills\": r'TECHNICAL_SKILLS:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"role_type\": r'ROLE_TYPE:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"salary\": r'SALARY:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"work_arrangement\": r'WORK_ARRANGEMENT:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"experience\": r'EXPERIENCE:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)'\n","        }\n","\n","        for component, pattern in component_patterns.items():\n","            match = re.search(pattern, response, re.IGNORECASE)\n","            if match:\n","                score = int(match.group(1))\n","                reason = match.group(2).strip()\n","                components[component] = {\"score\": score, \"reason\": reason}\n","\n","        # Extract overall reasoning\n","        reasoning_pattern = re.search(r'OVERALL_REASONING:\\s*(.+?)(?=\\n\\n|$)', response, re.DOTALL)\n","        overall_reasoning = reasoning_pattern.group(1).strip() if reasoning_pattern else \"Good potential fit based on available information.\"\n","\n","        return {\n","            \"match_score\": match_score,\n","            \"components\": components,\n","            \"overall_reasoning\": overall_reasoning,\n","            \"raw_response\": response\n","        }\n","\n","    def _fallback_match_scoring(self, message: str) -> Dict[str, Any]:\n","        \"\"\"Fallback match scoring using keyword analysis\"\"\"\n","        message_lower = message.lower()\n","\n","        # Simple keyword-based scoring\n","        score = 50  # Base score\n","\n","        # Technical skills boost\n","        tech_keywords = [\"python\", \"ai\", \"data\", \"engineering\", \"cloud\", \"aws\", \"azure\", \"gcp\", \"langchain\", \"rag\"]\n","        tech_matches = sum(1 for keyword in tech_keywords if keyword in message_lower)\n","        score += min(tech_matches * 5, 25)\n","\n","        # Salary analysis\n","        if any(sal in message_lower for sal in [\"60k\", \"65k\", \"70k\", \"‚Ç¨60\", \"‚Ç¨65\", \"‚Ç¨70\"]):\n","            score += 15\n","        elif any(sal in message_lower for sal in [\"80k\", \"90k\", \"‚Ç¨80\", \"‚Ç¨90\"]):\n","            score += 10\n","\n","        # Remote work boost\n","        if \"remote\" in message_lower:\n","            score += 10\n","\n","        return {\n","            \"match_score\": min(score, 100),\n","            \"components\": {},\n","            \"overall_reasoning\": \"Fallback scoring based on keyword analysis\",\n","            \"raw_response\": \"Fallback analysis used\"\n","        }\n","\n","    def generate_response_for_match(self, match_data: Dict[str, Any], recruiter_message: str) -> Tuple[str, ConversationState]:\n","        \"\"\"Generate appropriate response based on match score\"\"\"\n","\n","        match_score = match_data[\"match_score\"]\n","        print(f\"   üéØ Match Score: {match_score}%\")\n","\n","        if match_score > 80:\n","            # PASSED: Positive response + schedule call\n","            state = ConversationState.PASSED\n","\n","            # Extract specific fit reasons\n","            fit_reasons = self._extract_fit_reasons(match_data, recruiter_message)\n","            response = self.response_templates[\"passed\"].format(\n","                specific_fit_reasons=fit_reasons\n","            )\n","\n","            print(f\"   ‚úÖ State: PASSED (Schedule call)\")\n","\n","        elif 60 <= match_score <= 80:\n","            # STAND-BY: Cordial response + manual review needed\n","            state = ConversationState.STAND_BY\n","\n","            considerations, additional_info = self._extract_standby_details(match_data, recruiter_message)\n","            response = self.response_templates[\"stand_by\"].format(\n","                specific_considerations=considerations,\n","                additional_info_needed=additional_info\n","            )\n","\n","            print(f\"   ‚è∏Ô∏è State: STAND-BY (Manual review needed)\")\n","\n","        else:  # match_score < 60\n","            # FINISHED: Polite decline + explain ideal profile\n","            state = ConversationState.FINISHED\n","\n","            mismatch_explanation = self._extract_mismatch_reasons(match_data, recruiter_message)\n","            response = self.response_templates[\"finished\"].format(\n","                mismatch_explanation=mismatch_explanation,\n","                ideal_profile_summary=\"Data Engineering and AI projects using Python, cloud technologies, and modern frameworks\",\n","                preferred_work_style=\"remote or hybrid\"\n","            )\n","\n","            print(f\"   ‚ùå State: FINISHED (Declined)\")\n","\n","        return response, state\n","\n","    def _extract_fit_reasons(self, match_data: Dict[str, Any], message: str) -> str:\n","        \"\"\"Extract specific reasons why the job is a good fit\"\"\"\n","        reasons = []\n","\n","        if \"components\" in match_data:\n","            components = match_data[\"components\"]\n","\n","            # Check high-scoring components\n","            for component, data in components.items():\n","                if data.get(\"score\", 0) >= 80:\n","                    reason = data.get(\"reason\", \"\").strip()\n","                    if reason:\n","                        reasons.append(f\"‚Ä¢ {reason}\")\n","\n","        if not reasons:\n","            # Fallback based on overall reasoning\n","            reasons.append(f\"‚Ä¢ {match_data.get('overall_reasoning', 'The role aligns well with my technical background')}\")\n","\n","        return \"\\n\".join(reasons[:3])  # Limit to top 3 reasons\n","\n","    def _extract_standby_details(self, match_data: Dict[str, Any], message: str) -> Tuple[str, str]:\n","        \"\"\"Extract details for stand-by responses\"\"\"\n","        considerations = \"The opportunity has several interesting aspects, but I'd like to better understand the fit.\"\n","        additional_info = \"the team structure, specific project details, and growth opportunities\"\n","\n","        # Try to extract from components with moderate scores\n","        if \"components\" in match_data:\n","            components = match_data[\"components\"]\n","            moderate_scores = [comp for comp, data in components.items() if 60 <= data.get(\"score\", 0) <= 80]\n","\n","            if moderate_scores:\n","                considerations = f\"While the {', '.join(moderate_scores)} aspects look promising, some details need clarification.\"\n","\n","        return considerations, additional_info\n","\n","    def _extract_mismatch_reasons(self, match_data: Dict[str, Any], message: str) -> str:\n","        \"\"\"Extract specific reasons for job mismatch\"\"\"\n","        reasons = []\n","\n","        if \"components\" in match_data:\n","            components = match_data[\"components\"]\n","\n","            # Check low-scoring components\n","            for component, data in components.items():\n","                if data.get(\"score\", 0) < 60:\n","                    reason = data.get(\"reason\", \"\").strip()\n","                    if reason:\n","                        component_name = component.replace(\"_\", \" \").title()\n","                        reasons.append(f\"‚Ä¢ {component_name}: {reason}\")\n","\n","        if not reasons:\n","            reasons.append(\"‚Ä¢ The role requirements don't align closely with my current expertise and career focus.\")\n","\n","        return \"\\n\".join(reasons[:2])  # Limit to top 2 mismatch reasons\n","\n","    def process_recruiter_message(self, recruiter_message: str) -> Dict[str, Any]:\n","        \"\"\"Complete end-to-end processing of recruiter message\"\"\"\n","\n","        print(\"üöÄ Processing recruiter message through complete system...\")\n","        print(\"=\"*60)\n","\n","        start_time = time.time()\n","\n","        # Step 1: Input Guardrail Processing\n","        guardrail_result = self.guardrail.process_message(recruiter_message)\n","\n","        if not guardrail_result.should_continue_to_rag:\n","            # Generic message: return guardrail response\n","            total_time = time.time() - start_time\n","            return {\n","                \"final_response\": guardrail_result.response,\n","                \"state\": guardrail_result.state,\n","                \"message_type\": guardrail_result.message_type,\n","                \"confidence\": guardrail_result.confidence,\n","                \"processing_time\": total_time,\n","                \"pipeline_stage\": \"guardrail_only\"\n","            }\n","\n","        # Step 2: RAG + Match Scoring for concrete offers\n","        print(\"üîç Proceeding to RAG analysis...\")\n","        match_data = self.calculate_match_score(recruiter_message)\n","\n","        # Step 3: Generate appropriate response based on score\n","        final_response, final_state = self.generate_response_for_match(match_data, recruiter_message)\n","\n","        total_time = time.time() - start_time\n","\n","        # Step 4: Output Guardrail Processing (for concrete offers that got responses)\n","        print(\"üõ°Ô∏è Processing through output guardrail...\")\n","        final_response = self.output_guardrail.validate_and_improve_response(\n","            final_response, recruiter_message, max_iterations=5\n","        )\n","\n","        total_time = time.time() - start_time\n","\n","        return {\n","            \"final_response\": final_response,\n","            \"state\": final_state,\n","            \"message_type\": guardrail_result.message_type,\n","            \"confidence\": guardrail_result.confidence,\n","            \"match_score\": match_data[\"match_score\"],\n","            \"match_details\": match_data,\n","            \"processing_time\": total_time,\n","            \"pipeline_stage\": \"complete_rag_analysis\"\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"xEgAE03b-zZn"},"source":["### üîß CREATE AI ASSISTANT OBJECT WITH ALL COMPONENTS\n","Now that all components are initialized, create the complete AI assistant"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1753579078848,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"zYx2otIv-zZn","outputId":"63cea7b5-9059-4d8d-d41f-bd00798bf940"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ CREATING AI ASSISTANT WITH COMPLETE PIPELINE...\n","\n","ü§ñ AI ASSISTANT CREATED SUCCESSFULLY!\n","================================================================================\n","‚úÖ Input Guardrail: Ready\n","‚úÖ Output Guardrail: Ready\n","‚úÖ RAG Knowledge Base: Ready\n","‚úÖ Main Model Configuration: Ready\n","‚úÖ Complete Pipeline: Ready for testing\n","================================================================================\n","\n","üí° NEXT STEPS:\n","1. Load models: ai_assistant.load_models()\n","2. Test system: test_custom_message('Your message', 'Test Name')\n","3. Run full demo: run_complete_demo()\n"]}],"source":["print(\"üöÄ CREATING AI ASSISTANT WITH COMPLETE PIPELINE...\")\n","\n","# Create AI Assistant with all components\n","ai_assistant = AIRecruiterAssistant(\n","    model_name=selected_text_generator_model,\n","    rag_knowledge_base=rag_kb,\n","    input_guardrail=input_guardrail,\n","    output_guardrail=output_guardrail,\n","    cache_path=cache_path\n",")\n","\n","print(\"\\nü§ñ AI ASSISTANT CREATED SUCCESSFULLY!\")\n","print(\"=\"*80)\n","print(\"‚úÖ Input Guardrail: Ready\")\n","print(\"‚úÖ Output Guardrail: Ready\")\n","print(\"‚úÖ RAG Knowledge Base: Ready\")\n","print(\"‚úÖ Main Model Configuration: Ready\")\n","print(\"‚úÖ Complete Pipeline: Ready for testing\")\n","print(\"=\"*80)\n","\n","print(\"\\nüí° NEXT STEPS:\")\n","print(\"1. Load models: ai_assistant.load_models()\")\n","print(\"2. Test system: test_custom_message('Your message', 'Test Name')\")\n","print(\"3. Run full demo: run_complete_demo()\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753579078856,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"_XWLkB0N-zZn","outputId":"a97ba957-9145-4584-9372-081fd8d0fb6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîç VERIFYING AI ASSISTANT OBJECT...\n","‚úÖ AI Assistant object verified successfully!\n","‚úÖ Model: mistralai/Mistral-7B-Instruct-v0.3\n","‚úÖ Input Guardrail: microsoft/Phi-3-mini-4k-instruct\n","‚úÖ Output Guardrail: microsoft/Phi-3-mini-4k-instruct\n","‚úÖ RAG Embeddings: sentence-transformers/all-MiniLM-L6-v2\n","\n","üöÄ AI ASSISTANT READY FOR USE!\n","‚Ä¢ Call ai_assistant.load_models() to load all models\n","‚Ä¢ Call test_custom_message() to test individual messages\n","‚Ä¢ Call run_complete_demo() to test all scenarios\n"]}],"source":["# üîß VERIFY AI ASSISTANT OBJECT\n","# Confirm that ai_assistant object was created correctly\n","\n","print(\"üîç VERIFYING AI ASSISTANT OBJECT...\")\n","\n","try:\n","    # Check if ai_assistant exists and has all required components\n","    assert hasattr(ai_assistant, 'guardrail'), \"Input guardrail missing\"\n","    assert hasattr(ai_assistant, 'output_guardrail'), \"Output guardrail missing\"\n","    assert hasattr(ai_assistant, 'rag_kb'), \"RAG knowledge base missing\"\n","    assert hasattr(ai_assistant, 'model_name'), \"Model name missing\"\n","\n","    print(\"‚úÖ AI Assistant object verified successfully!\")\n","    print(f\"‚úÖ Model: {ai_assistant.model_name}\")\n","    print(f\"‚úÖ Input Guardrail: {ai_assistant.guardrail.model_name}\")\n","    print(f\"‚úÖ Output Guardrail: {ai_assistant.output_guardrail.model_name}\")\n","    print(f\"‚úÖ RAG Embeddings: {embedding_model_name}\")\n","\n","    print(f\"\\nüöÄ AI ASSISTANT READY FOR USE!\")\n","    print(f\"‚Ä¢ Call ai_assistant.load_models() to load all models\")\n","    print(f\"‚Ä¢ Call test_custom_message() to test individual messages\")\n","    print(f\"‚Ä¢ Call run_complete_demo() to test all scenarios\")\n","\n","except NameError:\n","    print(\"‚ùå ai_assistant object not found - check previous cells\")\n","except AssertionError as e:\n","    print(f\"‚ùå ai_assistant object incomplete: {str(e)}\")\n","except Exception as e:\n","    print(f\"‚ùå Error verifying ai_assistant: {str(e)}\")\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1753579078872,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"tEIe8QRz-zZl","outputId":"66d54edf-94c2-47db-acd8-4bb8322d6784"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","‚úÖ ENHANCED AI RECRUITER ASSISTANT - COMPLETE PIPELINE READY!\n","================================================================================\n","\n","üõ°Ô∏è INPUT GUARDRAIL (Phi-3-mini-4k-instruct):\n","   ‚Ä¢ Generic vs Concrete message classification\n","   ‚Ä¢ Automatic response for generic messages\n","   ‚Ä¢ Pass concrete offers to RAG analysis\n","\n","üß† MAIN GENERATOR (Mistral-7B-Instruct-v0.3):\n","   ‚Ä¢ RAG-powered match scoring analysis\n","   ‚Ä¢ State-based decision making (passed/stand_by/finished)\n","   ‚Ä¢ Context-aware professional response generation\n","\n","üõ°Ô∏è OUTPUT GUARDRAIL (Meta-Llama-3-8B-Instruct):\n","   ‚Ä¢ Validates response naturalness (first person usage)\n","   ‚Ä¢ Removes placeholders like '[recruiter name]'\n","   ‚Ä¢ Iterative improvement (up to k=5 iterations)\n","   ‚Ä¢ Fallback correction if max iterations reached\n","\n","üìä BUSINESS LOGIC IMPLEMENTED:\n","   ‚Ä¢ Generic messages ‚Üí 'pending_details' ‚Üí Request more information\n","   ‚Ä¢ High match (>80%) ‚Üí 'passed' ‚Üí Schedule call\n","   ‚Ä¢ Medium match (60-80%) ‚Üí 'stand_by' ‚Üí Manual review\n","   ‚Ä¢ Low match (<60%) ‚Üí 'finished' ‚Üí Polite decline\n","\n","üîß ALTERNATIVE MODELS AVAILABLE:\n","   ‚Ä¢ Output Guardrail alternatives: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\n","   ‚Ä¢ All models cached for faster loading\n","\n","üöÄ SYSTEM READY FOR TESTING!\n","================================================================================\n"]}],"source":["# üìã COMPLETE SYSTEM SUMMARY\n","print(\"=\"*80)\n","print(\"‚úÖ ENHANCED AI RECRUITER ASSISTANT - COMPLETE PIPELINE READY!\")\n","print(\"=\"*80)\n","\n","print(\"\\nüõ°Ô∏è INPUT GUARDRAIL (Phi-3-mini-4k-instruct):\")\n","print(\"   ‚Ä¢ Generic vs Concrete message classification\")\n","print(\"   ‚Ä¢ Automatic response for generic messages\")\n","print(\"   ‚Ä¢ Pass concrete offers to RAG analysis\")\n","\n","print(\"\\nüß† MAIN GENERATOR (Mistral-7B-Instruct-v0.3):\")\n","print(\"   ‚Ä¢ RAG-powered match scoring analysis\")\n","print(\"   ‚Ä¢ State-based decision making (passed/stand_by/finished)\")\n","print(\"   ‚Ä¢ Context-aware professional response generation\")\n","\n","print(\"\\nüõ°Ô∏è OUTPUT GUARDRAIL (Meta-Llama-3-8B-Instruct):\")\n","print(\"   ‚Ä¢ Validates response naturalness (first person usage)\")\n","print(\"   ‚Ä¢ Removes placeholders like '[recruiter name]'\")\n","print(\"   ‚Ä¢ Iterative improvement (up to k=5 iterations)\")\n","print(\"   ‚Ä¢ Fallback correction if max iterations reached\")\n","\n","print(\"\\nüìä BUSINESS LOGIC IMPLEMENTED:\")\n","print(\"   ‚Ä¢ Generic messages ‚Üí 'pending_details' ‚Üí Request more information\")\n","print(\"   ‚Ä¢ High match (>80%) ‚Üí 'passed' ‚Üí Schedule call\")\n","print(\"   ‚Ä¢ Medium match (60-80%) ‚Üí 'stand_by' ‚Üí Manual review\")\n","print(\"   ‚Ä¢ Low match (<60%) ‚Üí 'finished' ‚Üí Polite decline\")\n","\n","print(\"\\nüîß ALTERNATIVE MODELS AVAILABLE:\")\n","print(\"   ‚Ä¢ Output Guardrail alternatives: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\")\n","print(\"   ‚Ä¢ All models cached for faster loading\")\n","\n","print(\"\\nüöÄ SYSTEM READY FOR TESTING!\")\n","print(\"=\"*80)\n"]},{"cell_type":"markdown","metadata":{"id":"pMsQrWNljShm"},"source":["## ***PHASE 6:*** Complete System Testing\n","\n","Testing the full pipeline with different recruiter message scenarios to validate the guardrail and RAG integration.\n"]},{"cell_type":"markdown","metadata":{"id":"iPkWvNZa-zZn"},"source":["### üß™ COMPREHENSIVE TESTING SUITE\n","Testing all scenarios: Generic messages, High/Medium/Low match concrete offers"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1753579078898,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"C2kasTW4jShm","outputId":"34f6af28-8d8d-488e-ce96-21e33ed8ead9"},"outputs":[{"output_type":"stream","name":"stdout","text":["üß™ Complete Testing Suite Ready!\n","\n","üîß Available functions:\n","   ‚Ä¢ run_complete_demo() - Complete demo with all 4 scenarios\n","   ‚Ä¢ test_custom_message(message, name) - Test specific message\n","\n","üí° Example usage:\n","   run_complete_demo()\n","   test_custom_message('Your message here', 'My Test')\n"]}],"source":["def run_complete_demo():\n","    \"\"\"Complete demonstration of the AI assistant with all business logic scenarios\"\"\"\n","\n","    print(\"=\"*80)\n","    print(\"üöÄ AI RECRUITER ASSISTANT - COMPLETE BUSINESS LOGIC DEMO\")\n","    print(\"=\"*80)\n","\n","    # Load all models\n","    print(\"\\nüì• STEP 1: Loading models...\")\n","    print(\"=\"*80)\n","    ai_assistant.load_models()\n","\n","    # Test scenarios covering all business logic paths\n","    test_scenarios = [\n","        {\n","            \"name\": \"Generic Networking Message\",\n","            \"message\": \"\"\"\n","            Hi Cristopher,\n","\n","            I hope you're doing well! I came across your profile and was impressed by your background.\n","\n","            Are you currently open to new opportunities? I'd love to connect and discuss some exciting possibilities.\n","\n","            Best regards,\n","            Sarah\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as GENERIC ‚Üí State: pending_details\"\n","        },\n","\n","        {\n","            \"name\": \"High Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hi Cristopher,\n","\n","            We have an exciting Senior Data Engineer position at our AI-focused fintech startup.\n","\n","            Role details:\n","            - Building ETL/ELT pipelines with Python and Apache Airflow\n","            - Developing RAG systems using LangChain and vector databases\n","            - Working with cloud platforms (Azure/GCP)\n","            - 100% remote work\n","            - Salary: ‚Ç¨60,000-65,000 gross\n","            - Team: 15 engineers, very collaborative culture\n","\n","            Would you be interested in discussing this opportunity?\n","\n","            Best regards,\n","            Maria Rodriguez\n","            Technical Recruiter\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER ‚Üí High match score (>80%) ‚Üí State: passed\"\n","        },\n","\n","        {\n","            \"name\": \"Medium Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hello,\n","\n","            We're looking for a Data Scientist for our e-commerce platform.\n","\n","            Requirements:\n","            - 3+ years experience with Python and machine learning\n","            - Experience with recommendation systems\n","            - SQL and data analysis skills\n","            - Hybrid work (2 days office in Madrid)\n","            - Salary: ‚Ç¨75,000-85,000\n","\n","            The role involves building ML models for customer behavior prediction.\n","\n","            Interested?\n","\n","            Thanks,\n","            Roberto\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER ‚Üí Medium match score (60-80%) ‚Üí State: stand_by\"\n","        },\n","\n","        {\n","            \"name\": \"Low Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hi,\n","\n","            We have a Java Backend Developer position available.\n","\n","            Requirements:\n","            - 5+ years Java/Spring Boot experience\n","            - Microservices architecture\n","            - On-site work in London\n","            - Banking domain experience preferred\n","            - Competitive salary\n","\n","            Let me know if you're interested.\n","\n","            Best,\n","            John Smith\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER ‚Üí Low match score (<60%) ‚Üí State: finished\"\n","        }\n","    ]\n","\n","    print(f\"\\nüß™ STEP 2: Testing {len(test_scenarios)} scenarios...\")\n","    print(\"=\"*80)\n","\n","    results = []\n","\n","    for i, scenario in enumerate(test_scenarios, 1):\n","        print(f\"\\nüì® SCENARIO #{i}: {scenario['name']}\")\n","        print(\"=\"*60)\n","        print(f\"Expected: {scenario['expected_outcome']}\")\n","        print(\"\\nMessage:\")\n","        print(scenario['message'].strip())\n","\n","        print(f\"\\nü§ñ PROCESSING...\")\n","        print(\"-\"*40)\n","\n","        # Process through complete system\n","        try:\n","            result = ai_assistant.process_recruiter_message(scenario['message'])\n","            results.append({**result, \"scenario_name\": scenario['name']})\n","\n","            # Display results\n","            print(f\"\\nüí¨ FINAL RESPONSE:\")\n","            print(\"<<START>>\")\n","            print(result['final_response'])\n","            print(\"<<END>>\")\n","\n","            print(f\"\\nüìä ANALYSIS SUMMARY:\")\n","            print(f\"   üõ°Ô∏è Message Type: {result['message_type'].value}\")\n","            print(f\"   üéØ Final State: {result['state'].value}\")\n","            print(f\"   üìà Confidence: {result['confidence']:.2f}\")\n","            if 'match_score' in result:\n","                print(f\"   üèÜ Match Score: {result['match_score']}%\")\n","            print(f\"   ‚ö° Processing Time: {result['processing_time']:.2f}s\")\n","            print(f\"   üîß Pipeline Stage: {result['pipeline_stage']}\")\n","\n","            # Show detailed match analysis for concrete offers\n","            if result['pipeline_stage'] == 'complete_rag_analysis' and 'match_details' in result:\n","                match_details = result['match_details']\n","                if 'components' in match_details and match_details['components']:\n","                    print(f\"\\nüìã DETAILED MATCH BREAKDOWN:\")\n","                    for component, data in match_details['components'].items():\n","                        component_name = component.replace('_', ' ').title()\n","                        score = data.get('score', 'N/A')\n","                        reason = data.get('reason', 'No details')\n","                        print(f\"   ‚Ä¢ {component_name}: {score}% - {reason}\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error processing scenario: {str(e)}\")\n","            results.append({\n","                \"scenario_name\": scenario['name'],\n","                \"error\": str(e),\n","                \"final_response\": f\"Error: {str(e)}\",\n","                \"state\": \"error\"\n","            })\n","\n","        print(\"\\n\" + \"=\"*60)\n","\n","    # Summary\n","    print(f\"\\n‚úÖ TESTING COMPLETE!\")\n","    print(\"=\"*80)\n","\n","    successful_tests = len([r for r in results if 'error' not in r])\n","    print(f\"üìä Results: {successful_tests}/{len(test_scenarios)} scenarios processed successfully\")\n","\n","    # State distribution\n","    states = {}\n","    for result in results:\n","        if 'error' not in result:\n","            state = result['state'].value if hasattr(result['state'], 'value') else str(result['state'])\n","            states[state] = states.get(state, 0) + 1\n","\n","    print(f\"üéØ State Distribution:\")\n","    for state, count in states.items():\n","        print(f\"   ‚Ä¢ {state}: {count} scenario(s)\")\n","\n","    print(f\"\\nüßπ STEP 3: Cleaning up GPU memory...\")\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        print(\"‚úÖ GPU memory cleared\")\n","\n","    return results\n","\n","def test_custom_message(message: str, scenario_name: str = \"Custom Test\"):\n","    \"\"\"Test a single message through the complete system with detailed analysis\"\"\"\n","\n","    print(f\"üß™ TESTING: {scenario_name}\")\n","    print(\"=\"*60)\n","    print(\"Message:\")\n","    print(message.strip())\n","\n","    print(\"\\nüîß Processing through complete system...\")\n","\n","    try:\n","        result = ai_assistant.process_recruiter_message(message)\n","\n","        print(f\"\\nüí¨ RESPONSE:\")\n","        print(\"<<START>>\")\n","        print(result['final_response'])\n","        print(\"<<END>>\")\n","\n","        print(f\"\\nüìä DETAILED ANALYSIS:\")\n","        print(f\"   üõ°Ô∏è Message Classification: {result['message_type'].value}\")\n","        print(f\"   üéØ Final State: {result['state'].value}\")\n","        print(f\"   üìà Classification Confidence: {result['confidence']:.2f}\")\n","\n","        if 'match_score' in result:\n","            print(f\"   üèÜ Match Score: {result['match_score']}%\")\n","            print(f\"   üìã Match Reasoning: {result['match_details'].get('overall_reasoning', 'N/A')}\")\n","\n","        print(f\"   ‚ö° Total Processing Time: {result['processing_time']:.2f}s\")\n","        print(f\"   üîß Pipeline Stage: {result['pipeline_stage']}\")\n","\n","        return result\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error: {str(e)}\")\n","        return {\"error\": str(e)}\n","\n","print(\"üß™ Complete Testing Suite Ready!\")\n","print(\"\\nüîß Available functions:\")\n","print(\"   ‚Ä¢ run_complete_demo() - Complete demo with all 4 scenarios\")\n","print(\"   ‚Ä¢ test_custom_message(message, name) - Test specific message\")\n","print(\"\\nüí° Example usage:\")\n","print(\"   run_complete_demo()\")\n","print(\"   test_custom_message('Your message here', 'My Test')\")\n"]},{"cell_type":"markdown","metadata":{"id":"1PHTT9YC-zZr"},"source":["### üöÄ EXECUTE COMPLETE ENHANCED SYSTEM DEMO\n","Run the full demonstration of the guardrail + RAG system"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5eea575db1054af5923b2e51dc0ee722","e14a3b54661048b5a1ee65aef92a711e","b79defc13a3146bbaf781ebd6a772463","2a1c7136a9404766ba1065b09c114512","b49c09519ec44bd7b6f1a587e96e387b","22419fa9b7eb49f5ae5ea436ad861359","e79a68fcd08c4de59efa95c339f7066c","b085a7b1af5b4f24b3877b1d9d884f0f","f7c0a3e696b44c7f83adac9c495652cd","e18e80f1abab4681ac154054b4dd45ca","be9a20040432475e95dc25b349a62b28","9baa2e3e9ed142d490ecaeb5013d61f7","d2689bca02e24219878ead9adf15816a","78d990e387db42dcabda11c6e19c47a2","46f702c7e5724469bfdd6cbb6c2de20e","66ce915bc922430aac0c8a2fcda5a06a","090c78301ea3423d8ad53c62842a2f30","1c576d2089dd4194abde2e3ba3c2e56c","574671cf3e394d83a055d36602fc49d0","4399466f51bd4668b6f1b04ece427b44","b90c7ff3547e4c51a8c06178be68dcc0","21466de595e94b8bab58229a56330a44","7f3b6e68c50d46659319ce60327265c8","ba6ab4df840f4966bc7b6cd731762aad","b13155fbf1a84917b87d56cd8de804da","485743dfc965452194c2ba5a8aff40ec","7aa7629544104148a8b15338e6f1b35e","4d17d6745eee4398a1e1ccf30beed82a","a727e29492c74e70a7f50912064fa6d1","8edebef7c61146e482949022c0fc86b0","01d6c8f752ea492095546f924c8103f7","f2bfd659b86d483a9342c1f66ea14840","15472f2d466c46f890c48ac81050a978","780800b6e4a54117922de501a45162c6","17e4a5d69a7e4963bcc81f4b52b58311","0714c86b9ab24c2b8ee82db4fa9cbbeb","110355386e5344978a213fe05a28112c","16e086a6b36345b4b48aa271083eb3c6","23959bf794d4446f8f0798610b8e41eb","a9e26f11efac4cab97e8c36887383324","47b203e9b6ae4c3e99aee3814462cc38","6d40a31f850e4594a60c2ed8fd6c8b1d","1e5aaed31e8f45e98494be1e2821d629","b6082e7f5ed94e2b99f06ebf3a534bab","85185aafaba6471498910a12a054a68b","ac40a13964454ee8aa507b47b8e7c293","1e94504dffba4ecd829adc824b31d8bd","99171dbfeae54b9d9ee89b800e6fb6e5","ff8634fc12f74240a595122af299d539","c5a52ca842c44f6e9003c2941849bd62","746c3d779a804eed88e2f9d6a1bea06f","4c60444a66f041568e5f3535a2dba0bd","6b2ae5f0f92e4dd293062c5f08a8d003","b04e5220abd24f7db25af45ed5adb512","a28f223811fa4976b172d8f78994837b"]},"executionInfo":{"elapsed":422580,"status":"ok","timestamp":1753579501480,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"unr9R1sRjShn","outputId":"9498f372-245b-4316-a459-133e393f7049"},"outputs":[{"output_type":"stream","name":"stdout","text":["üéØ AI RECRUITER ASSISTANT WITH GUARDRAIL\n","================================================================================\n","\n","üõ°Ô∏è SYSTEM COMPONENTS:\n","‚úÖ Input Guardrail: Generic vs Concrete classification\n","‚úÖ RAG Knowledge Base: CV + Job expectations vector store\n","‚úÖ Match Scoring System: Detailed 5-criteria evaluation\n","‚úÖ State Management: pending_details, analyzing, passed, stand_by, finished\n","‚úÖ Response Generation: Context-aware, state-specific templates\n","\n","üìä BUSINESS LOGIC IMPLEMENTED:\n","‚Ä¢ Generic messages ‚Üí State: 'pending_details' ‚Üí Request details\n","‚Ä¢ Concrete offers ‚Üí RAG analysis ‚Üí Match scoring ‚Üí State-based response\n","‚Ä¢ High match (>80%) ‚Üí State: 'passed' ‚Üí Schedule call\n","‚Ä¢ Medium match (60-80%) ‚Üí State: 'stand_by' ‚Üí Manual review\n","‚Ä¢ Low match (<60%) ‚Üí State: 'finished' ‚Üí Polite decline\n","\n","üé≠ TEST SCENARIOS:\n","1. Generic networking message\n","2. High match concrete offer (Data Engineer, Python, Remote, ‚Ç¨60-65k)\n","3. Medium match concrete offer (Data Scientist, different domain)\n","4. Low match concrete offer (Java Developer, on-site)\n","\n","üöÄ STARTING COMPLETE SYSTEM DEMO...\n","\n","================================================================================\n","üöÄ AI RECRUITER ASSISTANT - COMPLETE BUSINESS LOGIC DEMO\n","================================================================================\n","\n","üì• STEP 1: Loading models...\n","================================================================================\n","üöÄ Loading Enhanced AI Assistant models...\n","\n","üõ°Ô∏è Loading guardrail model: microsoft/Phi-3-mini-4k-instruct\n","‚ö° Loading from cache...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n","WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eea575db1054af5923b2e51dc0ee722"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Guardrail model loaded successfully!\n","\n","üõ°Ô∏è Loading output guardrail model: microsoft/Phi-3-mini-4k-instruct\n","‚ö° Loading from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9baa2e3e9ed142d490ecaeb5013d61f7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Output guardrail model loaded successfully!\n","\n","üì• Loading main model: mistralai/Mistral-7B-Instruct-v0.3\n","‚ö° Loading mistralai/Mistral-7B-Instruct-v0.3 from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3b6e68c50d46659319ce60327265c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"780800b6e4a54117922de501a45162c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85185aafaba6471498910a12a054a68b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"]},{"output_type":"stream","name":"stdout","text":["\t‚úÖ mistralai/Mistral-7B-Instruct-v0.3 loaded successfully!\n","‚úÖ All models loaded successfully!\n","\n","üß™ STEP 2: Testing 4 scenarios...\n","================================================================================\n","\n","üì® SCENARIO #1: Generic Networking Message\n","============================================================\n","Expected: Should be classified as GENERIC ‚Üí State: pending_details\n","\n","Message:\n","Hi Cristopher,\n","\n","            I hope you're doing well! I came across your profile and was impressed by your background.\n","\n","            Are you currently open to new opportunities? I'd love to connect and discuss some exciting possibilities.\n","\n","            Best regards,\n","            Sarah\n","\n","ü§ñ PROCESSING...\n","----------------------------------------\n","üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["   üìä Classification: generic (confidence: 0.90)\n","\n","üí¨ FINAL RESPONSE:\n","<<START>>\n","Hello,\n","\n","Thank you for reaching out! I appreciate your interest in connecting.\n","\n","I'm currently open to exploring new opportunities that align with my background in Data Engineering and AI. However, I'd need more specific details about the role to properly evaluate if it's a good match.\n","\n","Could you please share more information about:\n","- The specific role title and main responsibilities\n","- Required technical skills and technologies\n","- Company type and industry\n","- Salary range and working arrangements (remote/hybrid/on-site)\n","- Location requirements\n","\n","I look forward to learning more about the opportunity!\n","\n","Best regards,\n","Cristopher\n","<<END>>\n","\n","üìä ANALYSIS SUMMARY:\n","   üõ°Ô∏è Message Type: generic\n","   üéØ Final State: pending_details\n","   üìà Confidence: 0.90\n","   ‚ö° Processing Time: 4.57s\n","   üîß Pipeline Stage: guardrail_only\n","\n","============================================================\n","\n","üì® SCENARIO #2: High Match Concrete Offer\n","============================================================\n","Expected: Should be classified as CONCRETE_OFFER ‚Üí High match score (>80%) ‚Üí State: passed\n","\n","Message:\n","Hi Cristopher,\n","\n","            We have an exciting Senior Data Engineer position at our AI-focused fintech startup.\n","\n","            Role details:\n","            - Building ETL/ELT pipelines with Python and Apache Airflow\n","            - Developing RAG systems using LangChain and vector databases\n","            - Working with cloud platforms (Azure/GCP)\n","            - 100% remote work\n","            - Salary: ‚Ç¨60,000-65,000 gross\n","            - Team: 15 engineers, very collaborative culture\n","\n","            Would you be interested in discussing this opportunity?\n","\n","            Best regards,\n","            Maria Rodriguez\n","            Technical Recruiter\n","\n","ü§ñ PROCESSING...\n","----------------------------------------\n","üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.95)\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["   üéØ Match Score: 90%\n","   ‚úÖ State: PASSED (Schedule call)\n","üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚ö†Ô∏è Issues found: Uses third person reference: '\\bthe candidate\\b'; Uses third person reference: '\\bcandidate\\'s\\b'\n","‚ö†Ô∏è Correction error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback correction.\n","      üîÑ Iteration 2/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 2\n","\n","üí¨ FINAL RESPONSE:\n","<<START>>\n","Hello, Thank you for this excellent opportunity! This position looks like a fantastic fit for my background and experience. ‚Ä¢ I has experience with the required technologies (Python, Apache Airflow, Azure, GCP) and also has relevant skills like PySpark, APIs, and agile methodologies. ‚Ä¢ my current role as an AI & Data Engineer aligns perfectly with the Senior Data Engineer position. ‚Ä¢ The offered salary range is within my expectations, but it's slightly lower than the stated range. I'm very interested in moving forward. Could we schedule a conversation to discuss this further? I'm generally available for: - Quick introductory call (15-30 minutes) - Technical discussion session - Meet-and-greet with the team Please let me know your availability and preferred format for our initial conversation. Looking forward to connecting! Best regards, Cristopher\n","<<END>>\n","\n","üìä ANALYSIS SUMMARY:\n","   üõ°Ô∏è Message Type: concrete_offer\n","   üéØ Final State: passed\n","   üìà Confidence: 0.95\n","   üèÜ Match Score: 90%\n","   ‚ö° Processing Time: 18.84s\n","   üîß Pipeline Stage: complete_rag_analysis\n","\n","üìã DETAILED MATCH BREAKDOWN:\n","   ‚Ä¢ Technical Skills: 90% - The candidate has experience with the required technologies (Python, Apache Airflow, Azure, GCP) and also has relevant skills like PySpark, APIs, and agile methodologies.\n","   ‚Ä¢ Role Type: 100% - The candidate's current role as an AI & Data Engineer aligns perfectly with the Senior Data Engineer position.\n","   ‚Ä¢ Salary: 80% - The offered salary range is within the candidate's expectations, but it's slightly lower than the stated range.\n","   ‚Ä¢ Work Arrangement: 100% - The job offer is 100% remote, which matches the candidate's preference.\n","   ‚Ä¢ Experience: 85% - The candidate has experience as a Data Engineer, and the seniority level of the position aligns well with their profile.\n","\n","============================================================\n","\n","üì® SCENARIO #3: Medium Match Concrete Offer\n","============================================================\n","Expected: Should be classified as CONCRETE_OFFER ‚Üí Medium match score (60-80%) ‚Üí State: stand_by\n","\n","Message:\n","Hello,\n","\n","            We're looking for a Data Scientist for our e-commerce platform.\n","\n","            Requirements:\n","            - 3+ years experience with Python and machine learning\n","            - Experience with recommendation systems\n","            - SQL and data analysis skills\n","            - Hybrid work (2 days office in Madrid)\n","            - Salary: ‚Ç¨75,000-85,000\n","\n","            The role involves building ML models for customer behavior prediction.\n","\n","            Interested?\n","\n","            Thanks,\n","            Roberto\n","\n","ü§ñ PROCESSING...\n","----------------------------------------\n","üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.95)\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["   üéØ Match Score: 65%\n","   ‚è∏Ô∏è State: STAND-BY (Manual review needed)\n","üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 1\n","\n","üí¨ FINAL RESPONSE:\n","<<START>>\n","\n","Hello,\n","\n","Thank you for sharing this opportunity. The role seems interesting and I appreciate you thinking of me.\n","\n","While the technical_skills, salary, work_arrangement, experience aspects look promising, some details need clarification.\n","\n","I'd like to take some time to review this opportunity more thoroughly and would prefer to discuss the details in a brief call before making any commitments.\n","\n","Could you provide a bit more information about the team structure, specific project details, and growth opportunities? This would help me better understand if it's the right fit.\n","\n","I'll get back to you within the next few days with my thoughts.\n","\n","Best regards,\n","Cristopher\n","<<END>>\n","\n","üìä ANALYSIS SUMMARY:\n","   üõ°Ô∏è Message Type: concrete_offer\n","   üéØ Final State: stand_by\n","   üìà Confidence: 0.95\n","   üèÜ Match Score: 65%\n","   ‚ö° Processing Time: 18.74s\n","   üîß Pipeline Stage: complete_rag_analysis\n","\n","üìã DETAILED MATCH BREAKDOWN:\n","   ‚Ä¢ Technical Skills: 80% - The candidate has experience with Python and machine learning, and the job offer requires these skills.\n","   ‚Ä¢ Role Type: 40% - The job offer is for a Data Scientist, while the candidate's profile is that of a Data Engineer. However, the job description mentions building ML models, which is a common task for both roles.\n","   ‚Ä¢ Salary: 60% - The offered salary range is higher than the candidate's expectations.\n","   ‚Ä¢ Work Arrangement: 80% - The job offer is hybrid, but the candidate prefers 100% remote or at least 90% if the job is related to AI or if the description fits their profile well.\n","   ‚Ä¢ Experience: 60% - The candidate has experience as a Data Engineer, but the job offer is for a Data Scientist, which is a more senior role.\n","\n","============================================================\n","\n","üì® SCENARIO #4: Low Match Concrete Offer\n","============================================================\n","Expected: Should be classified as CONCRETE_OFFER ‚Üí Low match score (<60%) ‚Üí State: finished\n","\n","Message:\n","Hi,\n","\n","            We have a Java Backend Developer position available.\n","\n","            Requirements:\n","            - 5+ years Java/Spring Boot experience\n","            - Microservices architecture\n","            - On-site work in London\n","            - Banking domain experience preferred\n","            - Competitive salary\n","\n","            Let me know if you're interested.\n","\n","            Best,\n","            John Smith\n","\n","ü§ñ PROCESSING...\n","----------------------------------------\n","üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.95)\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["   üéØ Match Score: 30%\n","   ‚ùå State: FINISHED (Declined)\n","üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚ö†Ô∏è Issues found: Uses third person reference: '\\bthe candidate\\b'; Uses third person reference: '\\bcandidate\\'s\\b'\n","‚ö†Ô∏è Correction error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback correction.\n","      üîÑ Iteration 2/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 2\n","\n","üí¨ FINAL RESPONSE:\n","<<START>>\n","Hello, Thank you for considering me for this opportunity. I appreciate you taking the time to reach out. After reviewing the position details, I don't think this role is the best fit for my current career direction and expertise. ‚Ä¢ Technical Skills: The job offer is for a Java Backend Developer, and my CV mentions AI & Data Engineering, with no clear Java experience. ‚Ä¢ Role Type: The job offer is for a Java Backend Developer, and my CV focuses on AI & Data Engineering. I'm primarily focused on opportunities involving Data Engineering and AI projects using Python, cloud technologies, and modern frameworks, ideally in a remote or hybrid setting. I'd be happy to stay connected for future opportunities that might be a better alignment. Thank you again for thinking of me! Best regards, Cristopher\n","<<END>>\n","\n","üìä ANALYSIS SUMMARY:\n","   üõ°Ô∏è Message Type: concrete_offer\n","   üéØ Final State: finished\n","   üìà Confidence: 0.95\n","   üèÜ Match Score: 30%\n","   ‚ö° Processing Time: 19.36s\n","   üîß Pipeline Stage: complete_rag_analysis\n","\n","üìã DETAILED MATCH BREAKDOWN:\n","   ‚Ä¢ Technical Skills: 0% - The job offer is for a Java Backend Developer, and the candidate's CV mentions AI & Data Engineering, with no clear Java experience.\n","   ‚Ä¢ Role Type: 0% - The job offer is for a Java Backend Developer, and the candidate's CV focuses on AI & Data Engineering.\n","   ‚Ä¢ Salary: 0% - The candidate's salary expectations are between ‚Ç¨60,000 and ‚Ç¨65,000 gross per year, while the job offer does not provide a specific salary range.\n","   ‚Ä¢ Work Arrangement: 0% - The candidate prefers 100% remote or at least 90% remote, while the job offer requires on-site work in London.\n","   ‚Ä¢ Experience: 0% - The candidate has experience in AI & Data Engineering, while the job offer is for a Java Backend Developer.\n","\n","============================================================\n","\n","‚úÖ TESTING COMPLETE!\n","================================================================================\n","üìä Results: 4/4 scenarios processed successfully\n","üéØ State Distribution:\n","   ‚Ä¢ pending_details: 1 scenario(s)\n","   ‚Ä¢ passed: 1 scenario(s)\n","   ‚Ä¢ stand_by: 1 scenario(s)\n","   ‚Ä¢ finished: 1 scenario(s)\n","\n","üßπ STEP 3: Cleaning up GPU memory...\n","‚úÖ GPU memory cleared\n","\n","üéâ DEMO COMPLETED SUCCESSFULLY!\n","================================================================================\n","‚úÖ All business logic scenarios tested\n","‚úÖ Guardrail integration working correctly\n","‚úÖ RAG pipeline functioning properly\n","‚úÖ Match scoring system operational\n","‚úÖ State management implemented\n","\n","üöÄ Ready for Stage 4: Application Integration!\n","\n","üí° For custom testing:\n","test_custom_message('Your recruiter message here', 'Custom Test')\n"]}],"source":["print(\"üéØ AI RECRUITER ASSISTANT WITH GUARDRAIL\")\n","print(\"=\"*80)\n","\n","print(\"\\nüõ°Ô∏è SYSTEM COMPONENTS:\")\n","print(\"‚úÖ Input Guardrail: Generic vs Concrete classification\")\n","print(\"‚úÖ RAG Knowledge Base: CV + Job expectations vector store\")\n","print(\"‚úÖ Match Scoring System: Detailed 5-criteria evaluation\")\n","print(\"‚úÖ State Management: pending_details, analyzing, passed, stand_by, finished\")\n","print(\"‚úÖ Response Generation: Context-aware, state-specific templates\")\n","\n","print(\"\\nüìä BUSINESS LOGIC IMPLEMENTED:\")\n","print(\"‚Ä¢ Generic messages ‚Üí State: 'pending_details' ‚Üí Request details\")\n","print(\"‚Ä¢ Concrete offers ‚Üí RAG analysis ‚Üí Match scoring ‚Üí State-based response\")\n","print(\"‚Ä¢ High match (>80%) ‚Üí State: 'passed' ‚Üí Schedule call\")\n","print(\"‚Ä¢ Medium match (60-80%) ‚Üí State: 'stand_by' ‚Üí Manual review\")\n","print(\"‚Ä¢ Low match (<60%) ‚Üí State: 'finished' ‚Üí Polite decline\")\n","\n","print(\"\\nüé≠ TEST SCENARIOS:\")\n","print(\"1. Generic networking message\")\n","print(\"2. High match concrete offer (Data Engineer, Python, Remote, ‚Ç¨60-65k)\")\n","print(\"3. Medium match concrete offer (Data Scientist, different domain)\")\n","print(\"4. Low match concrete offer (Java Developer, on-site)\")\n","\n","# Set to True to run the complete demo\n","RUN_COMPLETE_DEMO = True\n","\n","if RUN_COMPLETE_DEMO:\n","    print(f\"\\nüöÄ STARTING COMPLETE SYSTEM DEMO...\\n\")\n","\n","    try:\n","        results = run_complete_demo()\n","\n","        print(f\"\\nüéâ DEMO COMPLETED SUCCESSFULLY!\")\n","        print(\"=\"*80)\n","        print(\"‚úÖ All business logic scenarios tested\")\n","        print(\"‚úÖ Guardrail integration working correctly\")\n","        print(\"‚úÖ RAG pipeline functioning properly\")\n","        print(\"‚úÖ Match scoring system operational\")\n","        print(\"‚úÖ State management implemented\")\n","        print(\"\\nüöÄ Ready for Stage 4: Application Integration!\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Demo failed with error: {str(e)}\")\n","        print(f\"üí° Check models are loaded and try again\")\n","\n","else:\n","    print(f\"\\n‚è∏Ô∏è Set RUN_COMPLETE_DEMO = True to start the demo\")\n","    print(f\"Or use: run_complete_demo()\")\n","\n","print(\"\\nüí° For custom testing:\")\n","print(\"test_custom_message('Your recruiter message here', 'Custom Test')\")\n"]},{"cell_type":"markdown","metadata":{"id":"c24hPrS9-zZs"},"source":["### üöÄ TEST COMPLETE SYSTEM WITH OUTPUT GUARDRAIL\n","Test the same message that previously had problems to verify the output guardrail fixes them\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20449,"status":"ok","timestamp":1753579557976,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"x9AJUUq2-zZs","outputId":"176e0dd8-f744-4213-eb89-712d907b336d"},"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üß™ TESTING COMPLETE SYSTEM WITH OUTPUT GUARDRAIL\n","================================================================================\n","üì® TEST MESSAGE:\n","Exciting REMOTE GenAI Opportunity ‚Äì Long-Term Contract with Virtusa\n","Hi Cristopher,\n","\n","I hope you're doing well!\n","\n","I'm reaching out regarding an exciting opportunity for a Spanish-speaking GenAI Engineer with GCP on a long-term B2B contract with Virtusa. This is a REMOTE role and it's an urgent requirement.\n","\n","We're specifically looking for someone with experience in:\n","\n","Dialogflow CX and GCP\n","Contact Center AI (CCAI)\n","Visual flow design\n","Native GCP integration (BigQuery, Cloud Functions, etc.)\n","Fluent Spanish ‚Äì This is a must-have\n","\n","If this sounds like a good fit or if you know someone in your network who might be interested, I'd love to connect and share more details.\n","\n","Looking forward to hearing from you!!\n","\n","Best regards,\n","Priyanka\n","\n","============================================================\n","üöÄ PROCESSING WITH OUTPUT GUARDRAIL...\n","Expected: Should generate natural first-person response without 'the candidate' references\n","\n","üîß Processing...\n","üß™ TESTING: Output Guardrail Test\n","============================================================\n","Message:\n","Exciting REMOTE GenAI Opportunity ‚Äì Long-Term Contract with Virtusa\n","Hi Cristopher,\n","\n","I hope you're doing well!\n","\n","I'm reaching out regarding an exciting opportunity for a Spanish-speaking GenAI Engineer with GCP on a long-term B2B contract with Virtusa. This is a REMOTE role and it's an urgent requirement.\n","\n","We're specifically looking for someone with experience in:\n","\n","Dialogflow CX and GCP\n","Contact Center AI (CCAI)\n","Visual flow design\n","Native GCP integration (BigQuery, Cloud Functions, etc.)\n","Fluent Spanish ‚Äì This is a must-have\n","\n","If this sounds like a good fit or if you know someone in your network who might be interested, I'd love to connect and share more details.\n","\n","Looking forward to hearing from you!!\n","\n","Best regards,\n","Priyanka\n","\n","üîß Processing through complete system...\n","üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.95)\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["   üéØ Match Score: 85%\n","   ‚úÖ State: PASSED (Schedule call)\n","üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚ö†Ô∏è Issues found: Uses third person reference: '\\bthe candidate\\b'; Uses third person reference: '\\bcandidate\\'s\\b'\n","‚ö†Ô∏è Correction error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback correction.\n","      üîÑ Iteration 2/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 2\n","\n","üí¨ RESPONSE:\n","<<START>>\n","Hello, Thank you for this excellent opportunity! This position looks like a fantastic fit for my background and experience. ‚Ä¢ I has experience with GCP, BigQuery, and Apache Airflow (Composer), which aligns with the job offer's requirements for GCP, BigQuery, and Native GCP integration. ‚Ä¢ my profile as an AI & Data Engineer with experience in Generative AI, cloud platforms (Azure, GCP), and ELT processes matches the job offer's GenAI Engineer role. ‚Ä¢ I prefers 100% remote, and the job offer is a remote role. I'm very interested in moving forward. Could we schedule a conversation to discuss this further? I'm generally available for: - Quick introductory call (15-30 minutes) - Technical discussion session - Meet-and-greet with the team Please let me know your availability and preferred format for our initial conversation. Looking forward to connecting! Best regards, Cristopher\n","<<END>>\n","\n","üìä DETAILED ANALYSIS:\n","   üõ°Ô∏è Message Classification: concrete_offer\n","   üéØ Final State: passed\n","   üìà Classification Confidence: 0.95\n","   üèÜ Match Score: 85%\n","   üìã Match Reasoning: The candidate's technical skills, role type, and work arrangement match the job offer, with a slight discrepancy in salary expectations. However, the candidate's strong\n","   ‚ö° Total Processing Time: 20.41s\n","   üîß Pipeline Stage: complete_rag_analysis\n","\n","‚úÖ TESTING COMPLETE!\n","üîç Check the response above to verify it uses 'I', 'my', 'me' instead of 'the candidate'\n","\n","================================================================================\n","üéØ OUTPUT GUARDRAIL FUNCTIONALITY:\n","‚Ä¢ Validates first-person usage (I, my, me)\n","‚Ä¢ Removes placeholders like [recruiter name]\n","‚Ä¢ Iterates up to k=5 times for improvement\n","‚Ä¢ Provides fallback correction if needed\n","‚Ä¢ Uses Meta-Llama-3-8B-Instruct for validation\n","================================================================================\n"]}],"source":["\n","print(\"üß™ TESTING COMPLETE SYSTEM WITH OUTPUT GUARDRAIL\")\n","print(\"=\"*80)\n","\n","# Test the message that previously generated \"the candidate's\" instead of \"my\"\n","test_message = \"\"\"\n","Exciting REMOTE GenAI Opportunity ‚Äì Long-Term Contract with Virtusa\n","Hi Cristopher,\n","\n","I hope you're doing well!\n","\n","I'm reaching out regarding an exciting opportunity for a Spanish-speaking GenAI Engineer with GCP on a long-term B2B contract with Virtusa. This is a REMOTE role and it's an urgent requirement.\n","\n","We're specifically looking for someone with experience in:\n","\n","Dialogflow CX and GCP\n","Contact Center AI (CCAI)\n","Visual flow design\n","Native GCP integration (BigQuery, Cloud Functions, etc.)\n","Fluent Spanish ‚Äì This is a must-have\n","\n","If this sounds like a good fit or if you know someone in your network who might be interested, I'd love to connect and share more details.\n","\n","Looking forward to hearing from you!!\n","\n","Best regards,\n","Priyanka\n","\"\"\"\n","\n","print(\"üì® TEST MESSAGE:\")\n","print(test_message.strip())\n","print(\"\\n\" + \"=\"*60)\n","\n","print(\"üöÄ PROCESSING WITH OUTPUT GUARDRAIL...\")\n","print(\"Expected: Should generate natural first-person response without 'the candidate' references\")\n","print(\"\\nüîß Processing...\")\n","\n","try:\n","    # This will now go through the complete pipeline including output guardrail\n","    result = test_custom_message(test_message, \"Output Guardrail Test\")\n","\n","    print(\"\\n‚úÖ TESTING COMPLETE!\")\n","    print(\"üîç Check the response above to verify it uses 'I', 'my', 'me' instead of 'the candidate'\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error during test: {str(e)}\")\n","    print(\"üí° Make sure to run ai_assistant.load_models() first if models aren't loaded\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üéØ OUTPUT GUARDRAIL FUNCTIONALITY:\")\n","print(\"‚Ä¢ Validates first-person usage (I, my, me)\")\n","print(\"‚Ä¢ Removes placeholders like [recruiter name]\")\n","print(\"‚Ä¢ Iterates up to k=5 times for improvement\")\n","print(\"‚Ä¢ Provides fallback correction if needed\")\n","print(f\"‚Ä¢ Uses '{output_guardrail_model_name}' for validation\")\n","print(\"=\"*80)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1753579519321,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"V9gIeEgX-zZs","outputId":"c570bb53-e7bf-4512-f047-307e2fe9d5a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================================\n","üéâ AI RECRUITER ASSISTANT WITH OUTPUT GUARDRAIL - SYSTEM READY!\n","==========================================================================================\n","\n","üèóÔ∏è SYSTEM ARCHITECTURE IMPLEMENTED:\n","   üì® Recruiter Message\n","      ‚Üì\n","   üõ°Ô∏è Input Guardrail (Phi-3-mini) ‚Üí Generic/Concrete Classification\n","      ‚Üì\n","   üß† Main Generator (Mistral-7B) ‚Üí RAG Analysis + Response Generation\n","      ‚Üì\n","   üõ°Ô∏è Output Guardrail (Meta-Llama-3-8B) ‚Üí Naturalness Validation\n","      ‚Üì\n","   üí¨ Final Natural Response\n","\n","‚úÖ COMPONENTS READY:\n","   üõ°Ô∏è Input Guardrail: microsoft/Phi-3-mini-4k-instruct\n","   üß† Main Generator: mistralai/Mistral-7B-Instruct-v0.3\n","   üõ°Ô∏è Output Guardrail: microsoft/Phi-3-mini-4k-instruct\n","   üß† RAG Embeddings: sentence-transformers/all-MiniLM-L6-v2\n","\n","üöÄ HOW TO USE:\n","==================================================\n","\n","1Ô∏è‚É£ LOAD MODELS (Required first step):\n","   ai_assistant.load_models()\n","\n","2Ô∏è‚É£ TEST INDIVIDUAL MESSAGE:\n","   message = 'Your recruiter message here'\n","   result = test_custom_message(message, 'Test Name')\n","\n","3Ô∏è‚É£ RUN COMPLETE DEMO (All 4 scenarios):\n","   results = run_complete_demo()\n","\n","üõ°Ô∏è OUTPUT GUARDRAIL FEATURES:\n","   ‚Ä¢ Converts 'the candidate' ‚Üí 'I'\n","   ‚Ä¢ Converts 'candidate's' ‚Üí 'my'\n","   ‚Ä¢ Removes '[recruiter name]' placeholders\n","   ‚Ä¢ Iterates up to k=5 times for improvement\n","   ‚Ä¢ Provides fallback correction if needed\n","\n","üí° EXAMPLE USAGE:\n","==================================================\n","# Step 1: Load models\n","ai_assistant.load_models()\n","\n","# Step 2: Test message\n","test_message = '''\n","Hi! We have a Data Engineer position using Python and RAG.\n","‚Ç¨65k salary, 100% remote. Interested?\n","'''\n","result = test_custom_message(test_message, 'Quick Test')\n","\n","==========================================================================================\n","üéØ SYSTEM READY - NO MORE SETUP NEEDED!\n","==========================================================================================\n"]}],"source":["# üéØ FINAL SYSTEM READY - HOW TO USE\n","print(\"=\"*90)\n","print(\"üéâ AI RECRUITER ASSISTANT WITH OUTPUT GUARDRAIL - SYSTEM READY!\")\n","print(\"=\"*90)\n","\n","print(\"\\nüèóÔ∏è SYSTEM ARCHITECTURE IMPLEMENTED:\")\n","print(\"   üì® Recruiter Message\")\n","print(\"      ‚Üì\")\n","print(\"   üõ°Ô∏è Input Guardrail (Phi-3-mini) ‚Üí Generic/Concrete Classification\")\n","print(\"      ‚Üì\")\n","print(\"   üß† Main Generator (Mistral-7B) ‚Üí RAG Analysis + Response Generation\")\n","print(\"      ‚Üì\")\n","print(\"   üõ°Ô∏è Output Guardrail (Meta-Llama-3-8B) ‚Üí Naturalness Validation\")\n","print(\"      ‚Üì\")\n","print(\"   üí¨ Final Natural Response\")\n","\n","print(\"\\n‚úÖ COMPONENTS READY:\")\n","print(f\"   üõ°Ô∏è Input Guardrail: {input_guardrail_model_name}\")\n","print(f\"   üß† Main Generator: {selected_text_generator_model}\")\n","print(f\"   üõ°Ô∏è Output Guardrail: {output_guardrail_model_name}\")\n","print(f\"   üß† RAG Embeddings: {embedding_model_name}\")\n","\n","print(\"\\nüöÄ HOW TO USE:\")\n","print(\"=\"*50)\n","\n","print(\"\\n1Ô∏è‚É£ LOAD MODELS (Required first step):\")\n","print(\"   ai_assistant.load_models()\")\n","\n","print(\"\\n2Ô∏è‚É£ TEST INDIVIDUAL MESSAGE:\")\n","print(\"   message = 'Your recruiter message here'\")\n","print(\"   result = test_custom_message(message, 'Test Name')\")\n","\n","print(\"\\n3Ô∏è‚É£ RUN COMPLETE DEMO (All 4 scenarios):\")\n","print(\"   results = run_complete_demo()\")\n","\n","print(\"\\nüõ°Ô∏è OUTPUT GUARDRAIL FEATURES:\")\n","print(\"   ‚Ä¢ Converts 'the candidate' ‚Üí 'I'\")\n","print(\"   ‚Ä¢ Converts 'candidate's' ‚Üí 'my'\")\n","print(\"   ‚Ä¢ Removes '[recruiter name]' placeholders\")\n","print(\"   ‚Ä¢ Iterates up to k=5 times for improvement\")\n","print(\"   ‚Ä¢ Provides fallback correction if needed\")\n","\n","print(\"\\nüí° EXAMPLE USAGE:\")\n","print(\"=\"*50)\n","print(\"# Step 1: Load models\")\n","print(\"ai_assistant.load_models()\")\n","print(\"\")\n","print(\"# Step 2: Test message\")\n","print(\"test_message = '''\")\n","print(\"Hi! We have a Data Engineer position using Python and RAG.\")\n","print(\"‚Ç¨65k salary, 100% remote. Interested?\")\n","print(\"'''\")\n","print(\"result = test_custom_message(test_message, 'Quick Test')\")\n","\n","print(\"\\n\" + \"=\"*90)\n","print(\"üéØ SYSTEM READY - NO MORE SETUP NEEDED!\")\n","print(\"=\"*90)\n"]},{"cell_type":"markdown","metadata":{"id":"mQJ4MRbJtlyl"},"source":["# **Stage 4: Application Integration**"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1753579519357,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"0LKUf1XjP6VB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"ai_venv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"88ad41200a5b4c02a897b55af13b9078":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ec005f618ed486ba0b0d607e0ca040d","IPY_MODEL_feaaf14d8bb3489ebedc897e0059e871","IPY_MODEL_7a20ff6d93ff4af98306b048e7289abe"],"layout":"IPY_MODEL_66af426a4a28454e9252140eeeeb8985"}},"3ec005f618ed486ba0b0d607e0ca040d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3e65832b96b429489aceaa500e35845","placeholder":"‚Äã","style":"IPY_MODEL_35528ee2c7994847a4059abbaa763705","value":"modules.json:‚Äá100%"}},"feaaf14d8bb3489ebedc897e0059e871":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b01d89e676864bc89fe631b78e374b81","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2f46f7d0e3c4ba899858fa25725fff5","value":349}},"7a20ff6d93ff4af98306b048e7289abe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e936be7d40c476396d3804a260406e7","placeholder":"‚Äã","style":"IPY_MODEL_2e6277696fba47ecbb7b534e52233bb4","value":"‚Äá349/349‚Äá[00:00&lt;00:00,‚Äá36.5kB/s]"}},"66af426a4a28454e9252140eeeeb8985":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3e65832b96b429489aceaa500e35845":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35528ee2c7994847a4059abbaa763705":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b01d89e676864bc89fe631b78e374b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2f46f7d0e3c4ba899858fa25725fff5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e936be7d40c476396d3804a260406e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e6277696fba47ecbb7b534e52233bb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6912cca2be3a4f03b99d34073a5f3c07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18b11cf5a7f34e1fae9a1e776eea8363","IPY_MODEL_a3dddeb2d78c4d21bd93fd4724bffd58","IPY_MODEL_e732d81842f34aca933b40f5bac40f5a"],"layout":"IPY_MODEL_b3ae7d5875694fd0bc2ca7db43d65dc4"}},"18b11cf5a7f34e1fae9a1e776eea8363":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70d65876d7954873844fc309e909bc56","placeholder":"‚Äã","style":"IPY_MODEL_42fc51267ffd4a928f08bbbc8181af34","value":"config_sentence_transformers.json:‚Äá100%"}},"a3dddeb2d78c4d21bd93fd4724bffd58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_411327e1ec644e32bdedce0d827ce584","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a3b87fb514b43fcbdc2cbba598a0e6a","value":116}},"e732d81842f34aca933b40f5bac40f5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73ccdb9caa274bc0a9d9a58c687d50e8","placeholder":"‚Äã","style":"IPY_MODEL_31e9b252f95b457ab43161b6d4af53ef","value":"‚Äá116/116‚Äá[00:00&lt;00:00,‚Äá15.8kB/s]"}},"b3ae7d5875694fd0bc2ca7db43d65dc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70d65876d7954873844fc309e909bc56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42fc51267ffd4a928f08bbbc8181af34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"411327e1ec644e32bdedce0d827ce584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a3b87fb514b43fcbdc2cbba598a0e6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73ccdb9caa274bc0a9d9a58c687d50e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e9b252f95b457ab43161b6d4af53ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1fe8320637c481f873f0b4505b3a5ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51412af0c299412a87b917f59ea014f7","IPY_MODEL_363f05f5ec2d43058dea34279d0dc874","IPY_MODEL_4887dd4c75564f7a8d70ef3a890a0a87"],"layout":"IPY_MODEL_7000bd193c8b425aa277443f8f5fd706"}},"51412af0c299412a87b917f59ea014f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54a68d5e970d43a19ca8be700af15289","placeholder":"‚Äã","style":"IPY_MODEL_ba151cadfd134288b8814080e2e181cb","value":"README.md:‚Äá"}},"363f05f5ec2d43058dea34279d0dc874":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c46d15055068469981a1c5777c7da791","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b424fd760e6a4ac6aa3202bd6e8581b5","value":1}},"4887dd4c75564f7a8d70ef3a890a0a87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d9834bf1b9e4e1b99cb5239c2f71ae4","placeholder":"‚Äã","style":"IPY_MODEL_f7338316b6f4441db1506ad93eb64f35","value":"‚Äá10.5k/?‚Äá[00:00&lt;00:00,‚Äá895kB/s]"}},"7000bd193c8b425aa277443f8f5fd706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54a68d5e970d43a19ca8be700af15289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba151cadfd134288b8814080e2e181cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c46d15055068469981a1c5777c7da791":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"b424fd760e6a4ac6aa3202bd6e8581b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d9834bf1b9e4e1b99cb5239c2f71ae4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7338316b6f4441db1506ad93eb64f35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4fe373642244480bfbea95082970ece":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a28cebb5daa34bb687a99f27824f647d","IPY_MODEL_4d18362b11d94368a79bc74da294cead","IPY_MODEL_1d4a6e524e4e47a783a78721175448e1"],"layout":"IPY_MODEL_e264a7653dfd4214bf921da761fc2999"}},"a28cebb5daa34bb687a99f27824f647d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af45793708fc4473ae46bc47ef80bfc0","placeholder":"‚Äã","style":"IPY_MODEL_5db7362ee91b43d6a9d7f4c1cd6a4f37","value":"sentence_bert_config.json:‚Äá100%"}},"4d18362b11d94368a79bc74da294cead":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cd1ce90c7d142f6bad3fa45ffdc392e","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb813db52057469c9ec4bd12ab6c059a","value":53}},"1d4a6e524e4e47a783a78721175448e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_095b4055c9834374aa200f4f9e5f0957","placeholder":"‚Äã","style":"IPY_MODEL_71daa99d833c44dfbcb28378f19296db","value":"‚Äá53.0/53.0‚Äá[00:00&lt;00:00,‚Äá7.08kB/s]"}},"e264a7653dfd4214bf921da761fc2999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af45793708fc4473ae46bc47ef80bfc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5db7362ee91b43d6a9d7f4c1cd6a4f37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cd1ce90c7d142f6bad3fa45ffdc392e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb813db52057469c9ec4bd12ab6c059a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"095b4055c9834374aa200f4f9e5f0957":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71daa99d833c44dfbcb28378f19296db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bbf9e1bd5b24737bf6680cb4d5151e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_547ffd650a8a44ecb516f117c541ba90","IPY_MODEL_b121c94fc0a94c55b664bf3b262550fd","IPY_MODEL_483283404aa546b19b8906eb12f0f448"],"layout":"IPY_MODEL_de713b474d7c447da363fea6d6142a45"}},"547ffd650a8a44ecb516f117c541ba90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3addd8ce190740c2b5a78407d0e9c39d","placeholder":"‚Äã","style":"IPY_MODEL_0280cff583a647f482fe7b6ca70d5e89","value":"config.json:‚Äá100%"}},"b121c94fc0a94c55b664bf3b262550fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b0ab7ae7bd74efe8613e9c20e70a1c5","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e87457a60974a86a2a44e110d25aa8a","value":612}},"483283404aa546b19b8906eb12f0f448":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_666f5274ca8e4db1b15edf4f700de8f5","placeholder":"‚Äã","style":"IPY_MODEL_c9c3149e276a466380d3de30581b45fc","value":"‚Äá612/612‚Äá[00:00&lt;00:00,‚Äá70.2kB/s]"}},"de713b474d7c447da363fea6d6142a45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3addd8ce190740c2b5a78407d0e9c39d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0280cff583a647f482fe7b6ca70d5e89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b0ab7ae7bd74efe8613e9c20e70a1c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e87457a60974a86a2a44e110d25aa8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"666f5274ca8e4db1b15edf4f700de8f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9c3149e276a466380d3de30581b45fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0127423fcaec406581629df48ab81e01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c821a32319048a595f74032cb693f93","IPY_MODEL_1a1e1d3cc2684110b8bb34ca91353963","IPY_MODEL_852f4e41f68a428ea1cc4c26accc1707"],"layout":"IPY_MODEL_e530e9f1dafa443eaac43ec821811265"}},"7c821a32319048a595f74032cb693f93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abbe450404d24715b2f5da0371ede16c","placeholder":"‚Äã","style":"IPY_MODEL_4002cf9bf1904aa2984e5fc276ccc849","value":"model.safetensors:‚Äá100%"}},"1a1e1d3cc2684110b8bb34ca91353963":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c6705da58d946e5a08d6633cc3b2625","max":90868376,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0522083c62340bc9b5500eb57b19004","value":90868376}},"852f4e41f68a428ea1cc4c26accc1707":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_251996909e61455980c4296fcaa8f180","placeholder":"‚Äã","style":"IPY_MODEL_bdc80d4883a1434fafcfc6d8ab4acbb3","value":"‚Äá90.9M/90.9M‚Äá[00:01&lt;00:00,‚Äá99.2MB/s]"}},"e530e9f1dafa443eaac43ec821811265":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abbe450404d24715b2f5da0371ede16c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4002cf9bf1904aa2984e5fc276ccc849":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c6705da58d946e5a08d6633cc3b2625":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0522083c62340bc9b5500eb57b19004":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"251996909e61455980c4296fcaa8f180":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdc80d4883a1434fafcfc6d8ab4acbb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a51617abe5f4406796270d5469608ef2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_570e4d3d11b440b4845cb3c7d63d44ca","IPY_MODEL_197ab49a288e480abbe28b9a9b55047e","IPY_MODEL_019f8f93a3624c23a780d42dbefaba8c"],"layout":"IPY_MODEL_5785bc0676a34cb1acb0bf2d1cf52f65"}},"570e4d3d11b440b4845cb3c7d63d44ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a1c60a2e9a14b228d3c5ce82312f7b9","placeholder":"‚Äã","style":"IPY_MODEL_9276e237f66e4b6884e3dc892dd67a09","value":"tokenizer_config.json:‚Äá100%"}},"197ab49a288e480abbe28b9a9b55047e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b6cc761986940e9a7a1b5bda1ff3341","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_899d595aa7e6441790a7d8e613bf007c","value":350}},"019f8f93a3624c23a780d42dbefaba8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc31cf19e2a345daa79ad45a332c4408","placeholder":"‚Äã","style":"IPY_MODEL_a0e642c398ea4f39a55f3bd92a5402e5","value":"‚Äá350/350‚Äá[00:00&lt;00:00,‚Äá44.6kB/s]"}},"5785bc0676a34cb1acb0bf2d1cf52f65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a1c60a2e9a14b228d3c5ce82312f7b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9276e237f66e4b6884e3dc892dd67a09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b6cc761986940e9a7a1b5bda1ff3341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"899d595aa7e6441790a7d8e613bf007c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc31cf19e2a345daa79ad45a332c4408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0e642c398ea4f39a55f3bd92a5402e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c84a84fdba54e23abe612a8607dddb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_738e9212723a487daef475abaf14fc41","IPY_MODEL_0b3bf52614154d46b96fbdde28315f12","IPY_MODEL_d32fd62bd15d4d03ac8e5623760eae85"],"layout":"IPY_MODEL_cdd8d6eeeefd45aca0b4f25b79a596ee"}},"738e9212723a487daef475abaf14fc41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_066aaa418bc64998980af903ab2be2df","placeholder":"‚Äã","style":"IPY_MODEL_195227ab123f4f5b9cde73cff9b5985b","value":"vocab.txt:‚Äá"}},"0b3bf52614154d46b96fbdde28315f12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f3dc38a073a46168ff81fdf6c338919","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a215cb8a1ed0441489d165446ad22c17","value":1}},"d32fd62bd15d4d03ac8e5623760eae85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b3f0d727d324f639686c72f83bf8a93","placeholder":"‚Äã","style":"IPY_MODEL_1c913c750bbc49d5a5ac09d4353bedfd","value":"‚Äá232k/?‚Äá[00:00&lt;00:00,‚Äá6.93MB/s]"}},"cdd8d6eeeefd45aca0b4f25b79a596ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"066aaa418bc64998980af903ab2be2df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"195227ab123f4f5b9cde73cff9b5985b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f3dc38a073a46168ff81fdf6c338919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a215cb8a1ed0441489d165446ad22c17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b3f0d727d324f639686c72f83bf8a93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c913c750bbc49d5a5ac09d4353bedfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac840310aaa04785934534dc2371cd4c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a354b1d0c82c45878008cf118b043851","IPY_MODEL_109f5257cd74447facfa59eab4a653b7","IPY_MODEL_e0388114435745d6a013430deaaac0fa"],"layout":"IPY_MODEL_c6ef72c9cc3645548ded6bd3cf17d851"}},"a354b1d0c82c45878008cf118b043851":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7aeaa6e122c94488885e8b3c9aaf7e1c","placeholder":"‚Äã","style":"IPY_MODEL_ee5762e0cfb34efea8e08f1cd1c27d7b","value":"tokenizer.json:‚Äá"}},"109f5257cd74447facfa59eab4a653b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48e24943db7e4823bf6a34b67ba75d14","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f873f7fc149e4032b1db2c06a08597e6","value":1}},"e0388114435745d6a013430deaaac0fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0366b69500ec416582b6b69ff4294ad3","placeholder":"‚Äã","style":"IPY_MODEL_d545ddf812f442d69c2aafd7f20107a6","value":"‚Äá466k/?‚Äá[00:00&lt;00:00,‚Äá20.8MB/s]"}},"c6ef72c9cc3645548ded6bd3cf17d851":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7aeaa6e122c94488885e8b3c9aaf7e1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee5762e0cfb34efea8e08f1cd1c27d7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48e24943db7e4823bf6a34b67ba75d14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f873f7fc149e4032b1db2c06a08597e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0366b69500ec416582b6b69ff4294ad3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d545ddf812f442d69c2aafd7f20107a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d924c60193e84619aa8f0a4039626226":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71198e1856c64e668f61331b6b857ccf","IPY_MODEL_4d3f3801a056454e820e5be4073d0d7e","IPY_MODEL_07ec1400d71e40ddafd6908e5566e020"],"layout":"IPY_MODEL_e85ac3efc7a144f1a1b0b86f46bee851"}},"71198e1856c64e668f61331b6b857ccf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90c5e6a23c45404a821bf687380fbd2f","placeholder":"‚Äã","style":"IPY_MODEL_aef9a274f745449d934510b95d27f6ab","value":"special_tokens_map.json:‚Äá100%"}},"4d3f3801a056454e820e5be4073d0d7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fe95f389b474e649c274ed16b4efdf0","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6f0c0dc2ede411a9eb84c593836e35c","value":112}},"07ec1400d71e40ddafd6908e5566e020":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc1e2236d65844c0a01cdff8491ce438","placeholder":"‚Äã","style":"IPY_MODEL_c6ee30804e6f414d911e00a69b23fb79","value":"‚Äá112/112‚Äá[00:00&lt;00:00,‚Äá13.0kB/s]"}},"e85ac3efc7a144f1a1b0b86f46bee851":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90c5e6a23c45404a821bf687380fbd2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aef9a274f745449d934510b95d27f6ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fe95f389b474e649c274ed16b4efdf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6f0c0dc2ede411a9eb84c593836e35c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc1e2236d65844c0a01cdff8491ce438":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6ee30804e6f414d911e00a69b23fb79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae85ffa860e74c9db1e7888a27a33c43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b27e0b9992db494d8932954b671f3ed2","IPY_MODEL_cc82c2bdfe98471f9bcb0ab1411e3209","IPY_MODEL_8db66b18b37044008ee54262de7da4c0"],"layout":"IPY_MODEL_a6b63db1325f44c2abd1577c8551fbc4"}},"b27e0b9992db494d8932954b671f3ed2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_385cc9eadb3a4cc1bc75bbfd07701379","placeholder":"‚Äã","style":"IPY_MODEL_8f15b256783a428e804a1025c2c142e5","value":"config.json:‚Äá100%"}},"cc82c2bdfe98471f9bcb0ab1411e3209":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a61a0c009b24ddd8d19c95c62ea58a1","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab58961c70c4481cb3909a3a0b11cde4","value":190}},"8db66b18b37044008ee54262de7da4c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cb4e4202273415395fcb55cf4b9aa59","placeholder":"‚Äã","style":"IPY_MODEL_255f55a4bfa044fb8f8ca1d69fb46e52","value":"‚Äá190/190‚Äá[00:00&lt;00:00,‚Äá23.4kB/s]"}},"a6b63db1325f44c2abd1577c8551fbc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"385cc9eadb3a4cc1bc75bbfd07701379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f15b256783a428e804a1025c2c142e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a61a0c009b24ddd8d19c95c62ea58a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab58961c70c4481cb3909a3a0b11cde4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2cb4e4202273415395fcb55cf4b9aa59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"255f55a4bfa044fb8f8ca1d69fb46e52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5eea575db1054af5923b2e51dc0ee722":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e14a3b54661048b5a1ee65aef92a711e","IPY_MODEL_b79defc13a3146bbaf781ebd6a772463","IPY_MODEL_2a1c7136a9404766ba1065b09c114512"],"layout":"IPY_MODEL_b49c09519ec44bd7b6f1a587e96e387b"}},"e14a3b54661048b5a1ee65aef92a711e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22419fa9b7eb49f5ae5ea436ad861359","placeholder":"‚Äã","style":"IPY_MODEL_e79a68fcd08c4de59efa95c339f7066c","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"b79defc13a3146bbaf781ebd6a772463":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b085a7b1af5b4f24b3877b1d9d884f0f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7c0a3e696b44c7f83adac9c495652cd","value":2}},"2a1c7136a9404766ba1065b09c114512":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e18e80f1abab4681ac154054b4dd45ca","placeholder":"‚Äã","style":"IPY_MODEL_be9a20040432475e95dc25b349a62b28","value":"‚Äá2/2‚Äá[01:57&lt;00:00,‚Äá54.82s/it]"}},"b49c09519ec44bd7b6f1a587e96e387b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22419fa9b7eb49f5ae5ea436ad861359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e79a68fcd08c4de59efa95c339f7066c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b085a7b1af5b4f24b3877b1d9d884f0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7c0a3e696b44c7f83adac9c495652cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e18e80f1abab4681ac154054b4dd45ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be9a20040432475e95dc25b349a62b28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9baa2e3e9ed142d490ecaeb5013d61f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2689bca02e24219878ead9adf15816a","IPY_MODEL_78d990e387db42dcabda11c6e19c47a2","IPY_MODEL_46f702c7e5724469bfdd6cbb6c2de20e"],"layout":"IPY_MODEL_66ce915bc922430aac0c8a2fcda5a06a"}},"d2689bca02e24219878ead9adf15816a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_090c78301ea3423d8ad53c62842a2f30","placeholder":"‚Äã","style":"IPY_MODEL_1c576d2089dd4194abde2e3ba3c2e56c","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"78d990e387db42dcabda11c6e19c47a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_574671cf3e394d83a055d36602fc49d0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4399466f51bd4668b6f1b04ece427b44","value":2}},"46f702c7e5724469bfdd6cbb6c2de20e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b90c7ff3547e4c51a8c06178be68dcc0","placeholder":"‚Äã","style":"IPY_MODEL_21466de595e94b8bab58229a56330a44","value":"‚Äá2/2‚Äá[00:12&lt;00:00,‚Äá‚Äá5.77s/it]"}},"66ce915bc922430aac0c8a2fcda5a06a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"090c78301ea3423d8ad53c62842a2f30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c576d2089dd4194abde2e3ba3c2e56c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"574671cf3e394d83a055d36602fc49d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4399466f51bd4668b6f1b04ece427b44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b90c7ff3547e4c51a8c06178be68dcc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21466de595e94b8bab58229a56330a44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f3b6e68c50d46659319ce60327265c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba6ab4df840f4966bc7b6cd731762aad","IPY_MODEL_b13155fbf1a84917b87d56cd8de804da","IPY_MODEL_485743dfc965452194c2ba5a8aff40ec"],"layout":"IPY_MODEL_7aa7629544104148a8b15338e6f1b35e"}},"ba6ab4df840f4966bc7b6cd731762aad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d17d6745eee4398a1e1ccf30beed82a","placeholder":"‚Äã","style":"IPY_MODEL_a727e29492c74e70a7f50912064fa6d1","value":"Fetching‚Äá3‚Äáfiles:‚Äá100%"}},"b13155fbf1a84917b87d56cd8de804da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8edebef7c61146e482949022c0fc86b0","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01d6c8f752ea492095546f924c8103f7","value":3}},"485743dfc965452194c2ba5a8aff40ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2bfd659b86d483a9342c1f66ea14840","placeholder":"‚Äã","style":"IPY_MODEL_15472f2d466c46f890c48ac81050a978","value":"‚Äá3/3‚Äá[01:18&lt;00:00,‚Äá26.29s/it]"}},"7aa7629544104148a8b15338e6f1b35e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d17d6745eee4398a1e1ccf30beed82a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a727e29492c74e70a7f50912064fa6d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8edebef7c61146e482949022c0fc86b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01d6c8f752ea492095546f924c8103f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2bfd659b86d483a9342c1f66ea14840":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15472f2d466c46f890c48ac81050a978":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"780800b6e4a54117922de501a45162c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_17e4a5d69a7e4963bcc81f4b52b58311","IPY_MODEL_0714c86b9ab24c2b8ee82db4fa9cbbeb","IPY_MODEL_110355386e5344978a213fe05a28112c"],"layout":"IPY_MODEL_16e086a6b36345b4b48aa271083eb3c6"}},"17e4a5d69a7e4963bcc81f4b52b58311":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23959bf794d4446f8f0798610b8e41eb","placeholder":"‚Äã","style":"IPY_MODEL_a9e26f11efac4cab97e8c36887383324","value":"model-00003-of-00003.safetensors:‚Äá100%"}},"0714c86b9ab24c2b8ee82db4fa9cbbeb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47b203e9b6ae4c3e99aee3814462cc38","max":4546807800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d40a31f850e4594a60c2ed8fd6c8b1d","value":4546807800}},"110355386e5344978a213fe05a28112c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e5aaed31e8f45e98494be1e2821d629","placeholder":"‚Äã","style":"IPY_MODEL_b6082e7f5ed94e2b99f06ebf3a534bab","value":"‚Äá4.55G/4.55G‚Äá[01:18&lt;00:00,‚Äá335MB/s]"}},"16e086a6b36345b4b48aa271083eb3c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23959bf794d4446f8f0798610b8e41eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e26f11efac4cab97e8c36887383324":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47b203e9b6ae4c3e99aee3814462cc38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d40a31f850e4594a60c2ed8fd6c8b1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e5aaed31e8f45e98494be1e2821d629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6082e7f5ed94e2b99f06ebf3a534bab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85185aafaba6471498910a12a054a68b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac40a13964454ee8aa507b47b8e7c293","IPY_MODEL_1e94504dffba4ecd829adc824b31d8bd","IPY_MODEL_99171dbfeae54b9d9ee89b800e6fb6e5"],"layout":"IPY_MODEL_ff8634fc12f74240a595122af299d539"}},"ac40a13964454ee8aa507b47b8e7c293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5a52ca842c44f6e9003c2941849bd62","placeholder":"‚Äã","style":"IPY_MODEL_746c3d779a804eed88e2f9d6a1bea06f","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"1e94504dffba4ecd829adc824b31d8bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c60444a66f041568e5f3535a2dba0bd","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b2ae5f0f92e4dd293062c5f08a8d003","value":3}},"99171dbfeae54b9d9ee89b800e6fb6e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b04e5220abd24f7db25af45ed5adb512","placeholder":"‚Äã","style":"IPY_MODEL_a28f223811fa4976b172d8f78994837b","value":"‚Äá3/3‚Äá[02:14&lt;00:00,‚Äá36.24s/it]"}},"ff8634fc12f74240a595122af299d539":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5a52ca842c44f6e9003c2941849bd62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"746c3d779a804eed88e2f9d6a1bea06f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c60444a66f041568e5f3535a2dba0bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b2ae5f0f92e4dd293062c5f08a8d003":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b04e5220abd24f7db25af45ed5adb512":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a28f223811fa4976b172d8f78994837b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}