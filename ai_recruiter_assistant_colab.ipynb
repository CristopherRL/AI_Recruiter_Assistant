{"cells":[{"cell_type":"markdown","metadata":{"id":"B5E0iaKT7cPG"},"source":["# **AI Recruiter Assistant 🤖**\n","\n","A conversational chatbot to pre-screen job offers from recruiters using advanced AI techniques.\n","\n","## Project Overview\n","- **Goal**: Automate initial screening of job offers from recruiters\n","- **Technology**: RAG-first approach with open-source LLM and advanced prompt engineering\n","- **Interface**: Gradio web application for real-time conversations\n","- **Timeline**: 1 week structured development\n","- **Methodology**: Following systematic Generative AI project lifecycle\n","\n","## 🎯 **AI Project Methodology**\n","\n","This project follows a **structured 4-stage Generative AI lifecycle** for systematic development and evaluation:\n","\n","### **Stage 1: Define the Scope** ✅ COMPLETED\n","- **Problem Identification**: Manual screening of recruiter messages is time-consuming and inconsistent\n","- **Desired Outcome**: Automated system that analyzes job offers and responds appropriately based on profile match\n","- **Data Requirements**: CV, job expectations, LinkedIn conversation history for context retrieval\n","- **Feasibility**: Generative AI is ideal for this conversational task with contextual decision-making\n","\n","### **Stage 2: Select Models** ✅ COMPLETED\n","- **Research Models**: Compare 4 open-source models from Hugging Face\n","- **Benchmark Performance**: Test speed, memory usage, and response quality\n","- **Model Selection**: Choose optimal model based on performance metrics\n","- **Cache Management**: Efficient model storage and loading from Google Drive\n","\n","### **Stage 3: Adapt & Align Model** ✅ COMPLETED\n","- **RAG Pipeline Implementation**: ✅ Build core retrieval system with FAISS vector database\n","- **Context-Aware Prompt Engineering**: ✅ Design system prompts for effective context utilization\n","- **Implementing Guardrails**: ✅ Add safety and formatting checks for reliable output\n","- **Performance Evaluation**: ✅ Manual and qualitative assessment of retrieval and response quality\n","- **Prompt Engineering Optimization**: ✅ Refine and improve prompt effectiveness\n","- **Fine-Tuning (Postponed)**: Deferred to future iterations, focusing on RAG optimization first\n","\n","### **Stage 4: Application Integration & Deployment** 🔄 CURRENT\n","- **Gradio Interface Implementation**: ✅ Deploy web application for real-time testing\n","- **Hugging Face Spaces Deployment**: 📋 Step-by-step deployment to production\n","- **End-to-end Testing**: 📋 Comprehensive system validation\n","- **Final Summary & Review**: 📋 Complete project documentation and next steps\n","\n","## 🏗️ **System Architecture** (RAG-First Approach)\n","\n","```mermaid\n","graph LR\n","    A[📨 Recruiter Message] --> B[🔍 Intent Detection]\n","    B --> C[📊 RAG Analysis]\n","    C --> D[🎯 Match Scoring]\n","    D --> E[🧠 State Management]\n","    E --> F[💬 Response Generation]\n","    \n","    G[📄 CV + Job Expectations] --> H[🔍 Vector Embeddings]\n","    H --> I[💾 FAISS Database]\n","    I --> C\n","    \n","    J[🧠 Base LLM] --> F\n","    K[🎯 Prompt Engineering] --> J\n","    L[🛡️ Guardrails] --> F\n","```\n","\n","---\n","\n","**🚀 Currently working on Stage 4: Gradio Interface Implementation - Production deployment phase**\n"]},{"cell_type":"markdown","metadata":{"id":"hyj76RMWpRAk"},"source":["# **Stage 1: Define the Scope**"]},{"cell_type":"markdown","metadata":{"id":"_uEAC7f_pdh7"},"source":["## ***Phase 1:*** Configuration"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14471,"status":"ok","timestamp":1753663008773,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"5p_hhDIH7cPJ","outputId":"f448401a-8b70-4ae0-8f90-3a25b09e6cff"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ All dependencies loaded!\n","🔥 CUDA available: True\n","🎮 GPU: NVIDIA A100-SXM4-40GB\n","💾 GPU Memory: 42.5 GB\n"]}],"source":["# Install required packages\n","!pip install transformers>=4.36.0 torch>=2.0.0 peft>=0.7.0 bitsandbytes>=0.41.0 accelerate>=0.24.0\n","!pip install langchain>=0.1.0 langchain-community>=0.0.10 faiss-cpu>=1.7.4 sentence-transformers>=2.2.0\n","!pip install gradio>=4.0.0 pandas>=2.0.0 numpy>=1.24.0 tqdm>=4.65.0 datasets>=2.14.0\n","\n","# Core imports\n","import os, json, torch, pandas as pd, numpy as np, time, psutil\n","from pathlib import Path\n","from typing import List, Dict, Tuple, Optional\n","from dataclasses import dataclass\n","from enum import Enum\n","\n","# ML/AI imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, GenerationConfig\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# RAG imports\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.schema import Document\n","\n","print(\"✅ All dependencies loaded!\")\n","print(f\"🔥 CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","\n","# # Disk space tip\n","# print(f\"\\\\n💡 DISK SPACE TIP (100GB+ usage):\")\n","# print(f\"!rm -rf /tmp/* ~/.cache/pip/* ~/.cache/huggingface/*\")\n"]},{"cell_type":"markdown","metadata":{"id":"m6YQjLRYuTUM"},"source":["## ***Phase 2:*** Data Requirements"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23757,"status":"ok","timestamp":1753663032532,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"vsPjCQa47cPK","outputId":"83f5d806-cb85-4f31-93f3-17f903dc811f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Google Drive mounted\n","Main paths\n","📁 Project path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\n","🗂️ Cache path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/huggingface_cache\n","Loaded documents\n","✅ CV loaded: 7032 characters\n","✅ Job expectations loaded: 326 characters\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up project paths\n","project_path = \"/content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\"\n","cache_path = f\"{project_path}/huggingface_cache\"\n","\n","# Create directories\n","os.makedirs(cache_path, exist_ok=True)\n","os.makedirs(f\"{project_path}/data\", exist_ok=True)\n","\n","print(f\"✅ Google Drive mounted\")\n","print(f\"Main paths\")\n","print(f\"📁 Project path: {project_path}\")\n","print(f\"🗂️ Cache path: {cache_path}\")\n","\n","# Load data files\n","def load_documents():\n","    try:\n","        with open(f'{project_path}/RAG/cv.md', 'r', encoding='utf-8') as f:\n","            cv_content = f.read()\n","        with open(f'{project_path}/RAG/job_expectations.md', 'r', encoding='utf-8') as f:\n","            expectations_content = f.read()\n","\n","        print(f\"Loaded documents\")\n","        print(f\"✅ CV loaded: {len(cv_content)} characters\")\n","        print(f\"✅ Job expectations loaded: {len(expectations_content)} characters\")\n","        return cv_content, expectations_content\n","    except FileNotFoundError as e:\n","        print(f\"❌ Error loading documents: {e}\")\n","        return None, None\n","\n","cv_content, expectations_content = load_documents()\n"]},{"cell_type":"markdown","metadata":{"id":"U8cWQ1hcpXZt"},"source":["# **Stage 2:** Select Models"]},{"cell_type":"markdown","metadata":{"id":"iYVdYpjEpl07"},"source":["## ***--> Phase 1:*** check cached models"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1753663032583,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"x5bPz0Mm7cPK","outputId":"95e1a4b5-bf9a-43e0-a411-427247e36d8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Cache detection functions ready\n"]}],"source":["def check_cached_models(cache_path):\n","    \"\"\"Check for cached models\"\"\"\n","    cached_models = []\n","    if not os.path.exists(cache_path):\n","        return cached_models\n","\n","    try:\n","        items = os.listdir(cache_path)\n","        for item in items:\n","            item_path = os.path.join(cache_path, item)\n","            if os.path.isdir(item_path) and not item.startswith('.'):\n","                try:\n","                    contents = os.listdir(item_path)\n","                    # Simple check: does it contain ANY folder starting with \"models--\"?\n","                    has_models_folder = any(f.startswith('models--') for f in contents if os.path.isdir(os.path.join(item_path, f)))\n","                    if has_models_folder:\n","                        cached_models.append(item)\n","                except Exception:\n","                    continue\n","    except Exception:\n","        pass\n","\n","    return cached_models\n","\n","def create_model_cache_dir(model_name: str, cache_path: str) -> str:\n","    \"\"\"Create clean cache directory for a model\"\"\"\n","    model_folder = model_name.replace('/', ' ')\n","    model_cache_dir = os.path.join(cache_path, model_folder)\n","    os.makedirs(model_cache_dir, exist_ok=True)\n","    return model_cache_dir\n","\n","def display_cache_status(candidate_models):\n","    \"\"\"Display cached models status\"\"\"\n","    print(\"\\n🔍 CACHE STATUS:\")\n","    print(\"=\"*80 )\n","    cached_models = check_cached_models(cache_path)\n","\n","    if cached_models:\n","        print(f\"✅ Found {len(cached_models)} cached models:\")\n","        for model in cached_models:\n","            model_name = model.replace(' ', '/')\n","            print(f\"\\t⚡ {model_name}\")\n","    else:\n","        print(\"📭 No cached models found\")\n","\n","    # Show download vs cache status for candidate models\n","    print(f\"\\n⬇️Download vs 💻Cache status:\")\n","    for model_name in candidate_models:\n","        model_folder_space = model_name.replace('/', ' ')\n","        if model_folder_space in cached_models:\n","            print(f\"\\t⚡ {model_name} - 💻 Will be load from cache\")\n","        else:\n","            print(f\"\\t📥 {model_name} - ⬇️ Will be download\")\n","\n","    return cached_models\n","\n","print(\"✅ Cache detection functions ready\")\n"]},{"cell_type":"markdown","metadata":{"id":"KqVQ4-0xqBVJ"},"source":["## **--> Phase 2**: detect environment configuration, download required models, apply quantizacion, create benchmark process"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":202,"status":"ok","timestamp":1753663032811,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"pNHpxHdk7cPK","outputId":"934fe7c6-8e28-4eb2-a3c5-355c41adef3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Multi-scenario benchmark ready!\n","🎯 3 recruiter scenarios per model\n","📊 Test scenarios: ['Perfect Match', 'Generic Message', 'Wrong Match']\n"]}],"source":["class CacheAwareModelBenchmark:\n","\n","    def __init__(self):\n","        self.candidate_models = [\n","            \"mistralai/Mistral-7B-Instruct-v0.3\",\n","            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","            \"microsoft/Phi-3-mini-4k-instruct\",\n","            \"google/gemma-3-4b-it\"\n","        ]\n","\n","        self.model_specs = {\n","            \"mistralai/Mistral-7B-Instruct-v0.3\": {\"size\": \"7B\", \"context_length\": \"32K\"},\n","            \"meta-llama/Meta-Llama-3-8B-Instruct\": {\"size\": \"8B\", \"context_length\": \"8K\"},\n","            \"microsoft/Phi-3-mini-4k-instruct\": {\"size\": \"3.8B\", \"context_length\": \"4K\"},\n","            \"google/gemma-3-4b-it\": {\"size\": \"4B\", \"context_length\": \"8K\", \"features\": \"multimodal\"}\n","        }\n","\n","        self.results = []\n","\n","        # Benchmark iteration tracking\n","        from datetime import datetime\n","        self.benchmark_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.results_folder = f\"{project_path}/benchmark_iterations\"\n","        os.makedirs(self.results_folder, exist_ok=True)\n","\n","        # Capture environment configuration\n","        self.environment_config = self.detect_environment_config()\n","\n","        # System prompt for all interactions\n","        self.system_prompt = \"\"\"\n","        You are my highly intelligent personal assistant.\n","        Your mission is to engage in a continuous role-playing conversation where you will act as me.\n","        From this moment forward, the user will be playing the role of various recruiters contacting me.\n","        You must analyze the messages and generate appropriate responses as if you were me talking with a recruiter.\n","        Do not break character. Do not mention that you are an AI or an assistant in your replies to the recruiters.\n","        Analyze this job offer and tell me if it matches my profile:\n","        ***I am a Data and AI Engineer with extensive experience in building robust ELT pipelines and developing cutting-edge Generative AI solutions.\n","        My key skills include designing systems with Retrieval-Augmented Generation (RAG) and AI Agents using frameworks like Semantic Kernel and LangChain.\n","        I am proficient in data engineering with PySpark, SQL, and Apache Airflow,\n","        and highly skilled in cloud platforms, particularly Microsoft Azure (Data Factory, Databricks, AI Services) and GCP (BigQuery, Composer).\n","        I also possess strong backend development experience using Python and FastAPI to build and deploy services.***\n","        \"\"\"\n","\n","        # 3 different recruiter scenarios to test\n","        self.test_prompts = [\n","            {\n","            \"name\": \"Perfect Match\",\n","            \"message\": \"\"\"\n","            Hi! I hope you are doing well.\n","            I came across your profile and I am impressed by your background in AI and data engineering.\n","            I have an exciting opportunity for a Senior Data Engineer position at a fast-growing fintech company.\n","            The role involves working with Python, cloud technologies, and building ML pipelines.\n","            The salary range is €60,000-65,000 and it is 100% remote.\n","            Would you be interested in learning more?\n","            \"\"\"\n","            },\n","            {\n","            \"name\": \"Generic Message\",\n","            \"message\": \"\"\"\n","            Hello, are you currently open to new opportunities?\n","            \"\"\"\n","            },\n","            {\n","            \"name\": \"Wrong Match\",\n","            \"message\": \"\"\"\n","            We have a Java developer position available at our company.\n","            5 years experience required, on-site work in London, competitive salary.\n","            Interested?\n","            \"\"\"\n","            }\n","        ]\n","\n","    def detect_environment_config(self):\n","        \"\"\"Detect comprehensive environment configuration for Google Colab\"\"\"\n","        import platform\n","        import sys\n","        import subprocess\n","        import pkg_resources\n","        import re\n","\n","        config = {\n","            \"platform\": {\n","                \"system\": platform.system(),\n","                \"release\": platform.release(),\n","                \"machine\": platform.machine(),\n","                \"processor\": platform.processor(),\n","                \"python_version\": sys.version.split()[0],\n","                \"python_implementation\": platform.python_implementation()\n","            },\n","            \"hardware\": {},\n","            \"software\": {},\n","            \"colab_specific\": {}\n","        }\n","\n","        # GPU Information\n","        if torch.cuda.is_available():\n","            gpu_count = torch.cuda.device_count()\n","            config[\"hardware\"][\"gpu\"] = {\n","                \"available\": True,\n","                \"count\": gpu_count,\n","                \"devices\": []\n","            }\n","\n","            for i in range(gpu_count):\n","                gpu_props = torch.cuda.get_device_properties(i)\n","                gpu_info = {\n","                    \"index\": i,\n","                    \"name\": gpu_props.name,\n","                    \"memory_total_gb\": round(gpu_props.total_memory / 1e9, 2),\n","                    \"memory_total_mb\": gpu_props.total_memory // (1024 * 1024),\n","                    \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\",\n","                    \"multiprocessor_count\": gpu_props.multi_processor_count\n","                }\n","                config[\"hardware\"][\"gpu\"][\"devices\"].append(gpu_info)\n","\n","            # Current GPU memory usage\n","            try:\n","                config[\"hardware\"][\"gpu\"][\"current_memory_allocated_gb\"] = round(torch.cuda.memory_allocated() / 1e9, 2)\n","                config[\"hardware\"][\"gpu\"][\"current_memory_reserved_gb\"] = round(torch.cuda.memory_reserved() / 1e9, 2)\n","            except:\n","                pass\n","        else:\n","            config[\"hardware\"][\"gpu\"] = {\"available\": False}\n","\n","        # CPU Information\n","        try:\n","            cpu_count = os.cpu_count()\n","            config[\"hardware\"][\"cpu\"] = {\n","                \"count\": cpu_count,\n","                \"architecture\": platform.architecture()[0]\n","            }\n","\n","            # Try to get more detailed CPU info\n","            try:\n","                with open('/proc/cpuinfo', 'r') as f:\n","                    cpuinfo = f.read()\n","                    if 'model name' in cpuinfo:\n","                        cpu_model = cpuinfo.split(\"model name\")[1].split(':')[1].split('\\n')[0].strip()\n","                        config[\"hardware\"][\"cpu\"][\"model\"] = cpu_model\n","            except:\n","                pass\n","        except:\n","            config[\"hardware\"][\"cpu\"] = {\"count\": \"unknown\"}\n","\n","        # Memory Information\n","        try:\n","            import psutil\n","            memory = psutil.virtual_memory()\n","            config[\"hardware\"][\"memory\"] = {\n","                \"total_gb\": round(memory.total / 1e9, 2),\n","                \"available_gb\": round(memory.available / 1e9, 2),\n","                \"used_gb\": round(memory.used / 1e9, 2),\n","                \"percentage_used\": memory.percent\n","            }\n","        except ImportError:\n","            # Fallback without psutil\n","            try:\n","                with open('/proc/meminfo', 'r') as f:\n","                    meminfo = f.read()\n","                    total_match = re.search(r'MemTotal:\\s+(\\d+)', meminfo)\n","                    if total_match:\n","                        total_kb = int(total_match.group(1))\n","                        config[\"hardware\"][\"memory\"] = {\n","                            \"total_gb\": round(total_kb / 1e6, 2),\n","                            \"source\": \"proc_meminfo\"\n","                        }\n","            except:\n","                config[\"hardware\"][\"memory\"] = {\"total_gb\": \"unknown\"}\n","\n","        # CUDA Information\n","        if torch.cuda.is_available():\n","            config[\"software\"][\"cuda\"] = {\n","                \"version\": torch.version.cuda,\n","                \"cudnn_version\": torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None,\n","                \"cudnn_available\": torch.backends.cudnn.is_available()\n","            }\n","\n","        # Key Library Versions\n","        key_libraries = ['torch', 'transformers', 'accelerate', 'bitsandbytes', 'peft', 'numpy', 'pandas']\n","        config[\"software\"][\"libraries\"] = {}\n","\n","        for lib in key_libraries:\n","            try:\n","                version = pkg_resources.get_distribution(lib).version\n","                config[\"software\"][\"libraries\"][lib] = version\n","            except:\n","                config[\"software\"][\"libraries\"][lib] = \"not_found\"\n","\n","        # Google Colab Specific Detection\n","        try:\n","            # Check if running in Colab\n","            import google.colab\n","            config[\"colab_specific\"][\"environment\"] = \"google_colab\"\n","            config[\"colab_specific\"][\"is_colab\"] = True\n","\n","            # Try to get Colab VM info\n","            try:\n","                result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n","                                      capture_output=True, text=True, timeout=10)\n","                if result.returncode == 0:\n","                    gpu_info = result.stdout.strip().split(\", \")\n","                    if len(gpu_info) >= 2:\n","                        config[\"colab_specific\"][\"nvidia_smi\"] = {\n","                            \"gpu_name\": gpu_info[0],\n","                            \"memory_total_mb\": gpu_info[1]\n","                        }\n","            except:\n","                pass\n","\n","            # Detect Colab GPU type from GPU name\n","            if config[\"hardware\"][\"gpu\"][\"available\"]:\n","                gpu_name = config[\"hardware\"][\"gpu\"][\"devices\"][0][\"name\"].lower()\n","                if \"tesla t4\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"T4\"\n","                elif \"tesla k80\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"K80\"\n","                elif \"tesla v100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"V100\"\n","                elif \"tesla p4\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"P4\"\n","                elif \"tesla p100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"P100\"\n","                elif \"a100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"A100\"\n","                else:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"Unknown\"\n","\n","        except ImportError:\n","            config[\"colab_specific\"][\"is_colab\"] = False\n","            config[\"colab_specific\"][\"environment\"] = \"local_or_other\"\n","\n","        # Disk Space (for cache management)\n","        try:\n","            import shutil\n","            cache_disk_usage = shutil.disk_usage(cache_path)\n","            config[\"hardware\"][\"disk\"] = {\n","                \"cache_path_total_gb\": round(cache_disk_usage.total / 1e9, 2),\n","                \"cache_path_free_gb\": round(cache_disk_usage.free / 1e9, 2),\n","                \"cache_path_used_gb\": round((cache_disk_usage.total - cache_disk_usage.free) / 1e9, 2)\n","            }\n","        except:\n","            config[\"hardware\"][\"disk\"] = {\"status\": \"unable_to_detect\"}\n","\n","        return config\n","\n","    def display_environment_config(self):\n","\n","        print(\"\\n🖥️ ENVIRONMENT CONFIGURATION:\")\n","        print(\"=\" * 60)\n","\n","        \"\"\"Display current environment configuration\"\"\"\n","        config = self.environment_config\n","        # GPU Information\n","        if config[\"hardware\"][\"gpu\"][\"available\"]:\n","            gpu = config['hardware']['gpu']['devices'][0]\n","            print(f\"🎮 GPU: {gpu['name']}\")\n","            print(f\"💾 GPU Memory: {gpu['memory_total_gb']} GB ({gpu['memory_total_mb']} MB)\")\n","            print(f\"🔧 Compute Capability: {gpu['compute_capability']}\")\n","\n","            if config[\"colab_specific\"][\"is_colab\"]:\n","                colab_gpu = config[\"colab_specific\"].get(\"colab_gpu_type\", \"Unknown\")\n","                print(f\"☁️ Colab GPU Type: {colab_gpu}\")\n","        else:\n","            print(\"❌ GPU: Not available\")\n","\n","        # CPU & Memory\n","        print(f\"🧠 CPU: {config['hardware']['cpu']['count']} cores\")\n","        if 'model' in config['hardware']['cpu']:\n","            print(f\"🔍 CPU Model: {config['hardware']['cpu']['model']}\")\n","\n","        if 'memory' in config['hardware']:\n","            mem = config['hardware']['memory']\n","            print(f\"💿 RAM: {mem['total_gb']} GB total\")\n","            if 'available_gb' in mem:\n","                print(f\"📊 RAM Usage: {mem['used_gb']}/{mem['total_gb']} GB ({mem['percentage_used']:.1f}%)\")\n","\n","        # Software\n","        print(f\"🐍 Python: {config['platform']['python_version']}\")\n","        print(f\"🔥 PyTorch: {config['software']['libraries']['torch']}\")\n","        print(f\"🤗 Transformers: {config['software']['libraries']['transformers']}\")\n","\n","        if config['hardware']['gpu']['available']:\n","            print(f\"⚡ CUDA: {config['software']['cuda']['version']}\")\n","\n","        # Environment\n","        env_type = \"Google Colab\" if config[\"colab_specific\"][\"is_colab\"] else \"Local/Other\"\n","        print(f\"🌐 Environment: {env_type}\")\n","\n","    def load_model_with_quantization(self, model_name: str):\n","        \"\"\"Load model with cache detection, corruption handling, and optimized configurations\"\"\"\n","        cached_models = check_cached_models(cache_path)\n","        model_folder_space = model_name.replace('/', ' ')\n","\n","        is_cached = model_folder_space in cached_models\n","        cache_corrupted = False  # Flag to track if we need to retry\n","\n","        if is_cached:\n","            print(f\"⚡ Loading {model_name} from cache...\")\n","            model_cache_dir = os.path.join(cache_path, model_folder_space)\n","        else:\n","            print(f\"📥 Downloading {model_name} ...\")\n","            model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        def _attempt_load():\n","            \"\"\"Helper function to attempt model loading\"\"\"\n","            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n","            if tokenizer.pad_token is None:\n","                tokenizer.pad_token = tokenizer.eos_token\n","\n","            # Model-specific configurations to avoid warnings\n","            model_kwargs = {\n","                \"quantization_config\": bnb_config,\n","                \"device_map\": \"auto\",\n","                \"trust_remote_code\": True,\n","                \"cache_dir\": model_cache_dir\n","            }\n","\n","            # Fix Phi-3 flash-attention warnings\n","            if \"phi-3\" in model_name.lower():\n","                model_kwargs[\"attn_implementation\"] = \"eager\"\n","                print(f\"   🔧 Using eager attention for Phi-3 (avoiding flash-attention warnings)\")\n","\n","            model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n","            return model, tokenizer\n","\n","        try:\n","            model, tokenizer = _attempt_load()\n","            print(f\"\\t✅ {model_name} loaded successfully!\")\n","            return model, tokenizer\n","\n","        except Exception as e:\n","            error_msg = str(e).lower()\n","\n","            # Check for cache corruption errors\n","            if any(corruption_keyword in error_msg for corruption_keyword in\n","                   ['headertoosmall', 'header too small', 'corrupt', 'safetensors',\n","                    'invalid', 'decode', 'deserializing']):\n","\n","                if is_cached and not cache_corrupted:\n","                    print(f\"\\t🔧 Cache corruption detected: {str(e)}\")\n","                    print(f\"\\t🗑️ Clearing corrupted cache and retrying download...\")\n","\n","                    # Remove corrupted cache directory\n","                    import shutil\n","                    try:\n","                        if os.path.exists(model_cache_dir):\n","                            shutil.rmtree(model_cache_dir)\n","                            print(f\"\\t✅ Corrupted cache removed: {model_cache_dir}\")\n","                    except Exception as remove_error:\n","                        print(f\"\\t⚠️ Could not remove cache: {str(remove_error)}\")\n","\n","                    # Create new cache directory and retry download\n","                    model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","                    print(f\"\\t📥 Re-downloading {model_name}...\")\n","                    cache_corrupted = True  # Mark as corrupted to avoid infinite retry\n","\n","                    try:\n","                        model, tokenizer = _attempt_load()\n","                        print(f\"\\t✅ {model_name} loaded successfully after cache cleanup!\")\n","                        return model, tokenizer\n","                    except Exception as retry_error:\n","                        print(f\"\\t❌ Failed even after cache cleanup: {str(retry_error)}\")\n","                        raise retry_error\n","                else:\n","                    print(f\"\\t❌ Cache corruption persists or already retried: {str(e)}\")\n","                    raise\n","            else:\n","                print(f\"\\t❌ Error loading {model_name}: {str(e)}\")\n","                raise\n","\n","    def test_single_prompt(self, model, tokenizer, model_name: str, test_prompt: dict):\n","        \"\"\"Test a single prompt and return response\"\"\"\n","        # Combine system prompt with user message\n","        full_prompt = f\"{self.system_prompt}\\\\n\\\\nRecruiter: {test_prompt['message']}\\\\n\\\\nResponse:\"\n","\n","        try:\n","            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n","\n","            device = next(model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                inference_start = time.time()\n","\n","                if \"phi-3\" in model_name.lower():\n","                    # Fix for Phi-3 DynamicCache issue\n","                    outputs = model.generate(\n","                        **inputs,  # This already contains attention_mask\n","                        max_new_tokens=100,\n","                        do_sample=True,\n","                        temperature=0.7,\n","                        pad_token_id=tokenizer.eos_token_id,\n","                        use_cache=False  # Disable problematic cache\n","                    )\n","                else:\n","                    outputs = model.generate(\n","                        **inputs,  # This already contains attention_mask\n","                        max_new_tokens=100,\n","                        do_sample=True,\n","                        temperature=0.7,\n","                        pad_token_id=tokenizer.eos_token_id\n","                    )\n","                inference_time = time.time() - inference_start\n","\n","            # Decode response and clean up\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            # Extract only the response part (remove the prompt)\n","            response = response.replace(full_prompt, \"\").strip()\n","\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"inference_time\": inference_time,\n","                \"response\": response, # [:300] + \"...\" if len(response) > 300 else response,\n","                \"status\": \"success\"\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": str(e)\n","            }\n","\n","    def benchmark_model(self, model_name: str):\n","        \"\"\"Test model with multiple recruiter scenarios\"\"\"\n","        print(f\"\\n🧪 Testing {model_name}...\")\n","\n","        start_time = time.time()\n","\n","        try:\n","            model, tokenizer = self.load_model_with_quantization(model_name)\n","            load_time = time.time() - start_time\n","\n","            # Test all prompts\n","            prompt_results = []\n","            total_inference_time = 0\n","\n","            for i, test_prompt in enumerate(self.test_prompts):\n","                print(f\"\\tTesting scenario {i+1}/3: {test_prompt['name']}\")\n","                prompt_result = self.test_single_prompt(model, tokenizer, model_name, test_prompt)\n","                prompt_results.append(prompt_result)\n","                if prompt_result[\"status\"] == \"success\":\n","                    total_inference_time += prompt_result[\"inference_time\"]\n","\n","            # Calculate memory usage\n","            if torch.cuda.is_available():\n","                memory_used = torch.cuda.max_memory_allocated() / 1e9\n","                torch.cuda.reset_peak_memory_stats()\n","            else:\n","                memory_used = 0\n","\n","            device = next(model.parameters()).device\n","            avg_inference_time = total_inference_time / len([r for r in prompt_results if r[\"status\"] == \"success\"])\n","\n","            result = {\n","                \"model_name\": model_name,\n","                \"status\": \"success\",\n","                \"load_time\": load_time,\n","                \"avg_inference_time\": avg_inference_time,\n","                \"total_inference_time\": total_inference_time,\n","                \"memory_gb\": memory_used,\n","                \"device\": str(device),\n","                \"prompt_results\": prompt_results,\n","                \"specifications\": self.model_specs.get(model_name, {})\n","            }\n","\n","            successful_prompts = len([r for r in prompt_results if r[\"status\"] == \"success\"])\n","            print(f\"\\t✅ Load: {load_time:.1f}s | Avg Inference: {avg_inference_time:.2f}s | Memory: {memory_used:.1f}GB\")\n","            print(f\"\\t🎯 Successful prompts: {successful_prompts}/3 | Device: {device}\")\n","\n","            # Clean up\n","            del model, tokenizer\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","            return result\n","\n","        except Exception as e:\n","            print(f\"\\t⚠️ Model failed to load: {str(e)}\")\n","            return {\n","                \"model_name\": model_name,\n","                \"status\": \"failed\",\n","                \"error\": str(e),\n","                \"specifications\": self.model_specs.get(model_name, {})\n","            }\n","\n","    def benchmark_all_models(self):\n","\n","        \"\"\"Benchmark all models with multiple scenarios\"\"\"\n","        print(\"🚀 STARTING MULTI-SCENARIO ANALYSIS\")\n","        print(\"🎯 Testing 3 recruiter scenarios per model\")\n","\n","        for model_name in self.candidate_models:\n","            print(\"-\" * 60)\n","            result = self.benchmark_model(model_name)\n","            self.results.append(result)\n","\n","        print(\"\\n✅ Multi-scenario benchmark complete!\")\n","\n","    def save_benchmark_iteration(self):\n","        \"\"\"Save current benchmark iteration with timestamp\"\"\"\n","        from datetime import datetime\n","\n","        # Create detailed iteration data\n","        iteration_data = {\n","            \"metadata\": {\n","                \"timestamp\": self.benchmark_timestamp,\n","                \"datetime_readable\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","                \"total_models\": len(self.candidate_models),\n","                \"successful_models\": len([r for r in self.results if r.get(\"status\") == \"success\"]),\n","                \"failed_models\": len([r for r in self.results if r.get(\"status\") == \"failed\"]),\n","                \"test_scenarios\": len(self.test_prompts)\n","            },\n","            \"environment_config\": self.environment_config,\n","            \"test_scenarios\": self.test_prompts,\n","            \"model_specifications\": self.model_specs,\n","            \"detailed_results\": self.results,\n","            \"summary\": self.generate_benchmark_summary()\n","        }\n","\n","        # Save detailed results\n","        iteration_file = f\"{self.results_folder}/benchmark_{self.benchmark_timestamp}.json\"\n","        with open(iteration_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(iteration_data, f, indent=2, default=str, ensure_ascii=False)\n","\n","        print(f\"\\n💾 Benchmark iteration saved: {iteration_file}\")\n","\n","        # Update comparison history\n","        self.update_comparison_history(iteration_data)\n","\n","        return iteration_file\n","\n","    def generate_benchmark_summary(self):\n","        \"\"\"Generate concise summary of benchmark results\"\"\"\n","        successful_results = [r for r in self.results if r.get('status') == 'success']\n","\n","        if not successful_results:\n","            return {\"status\": \"no_successful_models\"}\n","\n","        # Performance rankings\n","        by_speed = sorted(successful_results, key=lambda x: x[\"avg_inference_time\"])\n","        by_memory = sorted(successful_results, key=lambda x: x[\"memory_gb\"])\n","        by_load_time = sorted(successful_results, key=lambda x: x[\"load_time\"])\n","\n","        summary = {\n","            \"rankings\": {\n","                \"fastest_inference\": {\n","                    \"model\": by_speed[0][\"model_name\"],\n","                    \"time_seconds\": by_speed[0][\"avg_inference_time\"]\n","                },\n","                \"least_memory\": {\n","                    \"model\": by_memory[0][\"model_name\"],\n","                    \"memory_gb\": by_memory[0][\"memory_gb\"]\n","                },\n","                \"fastest_loading\": {\n","                    \"model\": by_load_time[0][\"model_name\"],\n","                    \"load_time_seconds\": by_load_time[0][\"load_time\"]\n","                }\n","            },\n","            \"overall_stats\": {\n","                \"avg_inference_time\": sum(r[\"avg_inference_time\"] for r in successful_results) / len(successful_results),\n","                \"avg_memory_usage\": sum(r[\"memory_gb\"] for r in successful_results) / len(successful_results),\n","                \"avg_load_time\": sum(r[\"load_time\"] for r in successful_results) / len(successful_results),\n","                \"total_successful_prompts\": sum(len([p for p in r[\"prompt_results\"] if p[\"status\"] == \"success\"]) for r in successful_results)\n","            },\n","            \"model_performance_scores\": []\n","        }\n","\n","        # Calculate performance scores (lower is better)\n","        for result in successful_results:\n","            score = (\n","                result[\"avg_inference_time\"] * 0.4 +  # 40% weight on inference speed\n","                result[\"memory_gb\"] * 0.3 +           # 30% weight on memory efficiency\n","                result[\"load_time\"] / 100 * 0.3       # 30% weight on load time (scaled)\n","            )\n","\n","            summary[\"model_performance_scores\"].append({\n","                \"model\": result[\"model_name\"],\n","                \"performance_score\": round(score, 2),\n","                \"successful_prompts\": len([p for p in result[\"prompt_results\"] if p[\"status\"] == \"success\"])\n","            })\n","\n","        # Sort by performance score\n","        summary[\"model_performance_scores\"].sort(key=lambda x: x[\"performance_score\"])\n","\n","\n","        return summary\n","\n","    def update_comparison_history(self, current_iteration):\n","        \"\"\"Update master comparison file with historical data\"\"\"\n","        comparison_file = f\"{self.results_folder}/benchmark_comparison_history.json\"\n","\n","        # Load existing history\n","        if os.path.exists(comparison_file):\n","            with open(comparison_file, 'r', encoding='utf-8') as f:\n","                history = json.load(f)\n","        else:\n","            history = {\n","                \"iterations\": [],\n","                \"models_tracked\": list(set(self.candidate_models)),\n","                \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","            }\n","\n","        # Add current iteration summary\n","        history[\"iterations\"].append({\n","            \"timestamp\": current_iteration[\"metadata\"][\"timestamp\"],\n","            \"datetime\": current_iteration[\"metadata\"][\"datetime_readable\"],\n","            \"summary\": current_iteration[\"summary\"],\n","            \"metadata\": current_iteration[\"metadata\"]\n","        })\n","\n","        # Keep only last 10 iterations\n","        history[\"iterations\"] = history[\"iterations\"][-10:]\n","        history[\"last_updated\"] = current_iteration[\"metadata\"][\"datetime_readable\"]\n","\n","        # Save updated history\n","        with open(comparison_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(history, f, indent=2, default=str, ensure_ascii=False)\n","\n","        print(f\"📈 Comparison history updated: {comparison_file}\")\n","\n","    def display_detailed_results(self):\n","        \"\"\"Display comprehensive benchmark results and save iteration\"\"\"\n","        print(\"\\n\\n📊 DETAILED BENCHMARK RESULTS\")\n","        print(\"=\" * 60)\n","\n","        successful_results = [r for r in self.results if r.get('status') == 'success']\n","        failed_results = [r for r in self.results if r.get('status') == 'failed']\n","\n","        if successful_results:\n","            print(f\"\\n✅ {len(successful_results)} models tested successfully:\")\n","\n","            # Model comparison table\n","            print(f\"\\n\\n📈 MODEL PERFORMANCE COMPARISON:\")\n","            print(\"-\" * 80)\n","            print(f\"{'Model':<30} {'Size':<8} {'Load(s)':<8} {'Avg Inf(s)':<10} {'Memory(GB)':<10} {'Features':<12}\")\n","            print(\"-\" * 80)\n","\n","            for result in successful_results:\n","                specs = result['specifications']\n","                features = specs.get('features', 'text-only')\n","                model_short = result['model_name'].split('/')[-1][:28]\n","\n","                print(f\"{model_short:<30} {specs['size']:<8} {result['load_time']:<8.1f} {result['avg_inference_time']:<10.2f} {result['memory_gb']:<10.1f} {features:<12}\")\n","\n","            # Find best models\n","            fastest_model = min(successful_results, key=lambda x: x['avg_inference_time'])\n","            least_memory = min(successful_results, key=lambda x: x['memory_gb'])\n","\n","            print(\"-\" * 80)\n","            print(f\"🏆 FASTEST: {fastest_model['model_name'].split('/')[-1]} ({fastest_model['avg_inference_time']:.2f}s)\")\n","            print(f\"💾 LEAST MEMORY: {least_memory['model_name'].split('/')[-1]} ({least_memory['memory_gb']:.1f}GB)\")\n","\n","            # Performance scoring\n","            summary = self.generate_benchmark_summary()\n","            print(f\"\\n\\n🎯 OVERALL PERFORMANCE RANKING:\")\n","            print(\"-\" * 80)\n","            print(\"\\nSCORE = 40% weight on inference speed + 30% weight on memory efficiency + 30% weight on load time (scaled)\\n\")\n","            for i, model_score in enumerate(summary['model_performance_scores'], 1):\n","                model_short = model_score['model'].split('/')[-1]\n","                print(f\"{i}. {model_short:<35} Score: {model_score['performance_score']:<6} ({model_score['successful_prompts']}/3 prompts)\")\n","\n","            # Detailed responses per scenario\n","            print(\"=\" * 60)\n","            print(f\"\\n\\n🎭 RESPONSE QUALITY BY SCENARIO:\")\n","\n","            for i, scenario in enumerate(self.test_prompts):\n","                print(f\"\\n\\n📝 SCENARIO {i+1}: {scenario['name']}\")\n","                print(f\"Recruiter: {scenario['message'][:100]}...\")\n","                print(\"-\" * 60)\n","\n","                for result in successful_results:\n","                    if 'prompt_results' in result:\n","                        prompt_result = result['prompt_results'][i]\n","                        model_short = result['model_name'].split('/')[-1]\n","\n","                        if prompt_result['status'] == 'success':\n","                            print(f\"\\n🤖 {model_short}:\")\n","                            print(f\"\\t⚡ Time: {prompt_result['inference_time']:.2f}s\")\n","                            print(f\"\\t💬 Response:\\n<<START>>\\n {prompt_result['response']}\\n<<END>>\")\n","                        else:\n","                            print(f\"\\n❌ {model_short}: {prompt_result.get('error', 'Failed')}\")\n","\n","        if failed_results:\n","            print(f\"\\n❌ {len(failed_results)} models failed:\")\n","            for result in failed_results:\n","                print(f\"\\t• {result['model_name']}: {result['error']}\")\n","\n","        # Save this iteration\n","        self.save_benchmark_iteration()\n","\n","benchmark = CacheAwareModelBenchmark()\n","print(\"✅ Multi-scenario benchmark ready!\")\n","print(\"🎯 3 recruiter scenarios per model\")\n","print(f\"📊 Test scenarios: {[p['name'] for p in benchmark.test_prompts]}\")\n"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1753663032929,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"W9IRl9_O7SHF","outputId":"5e95cfda-a104-4a35-a2be-cb3d4d501398"},"outputs":[{"output_type":"stream","name":"stdout","text":["🧠 RAG Knowledge Base class initialized!\n","📚 Ready to process CV and job expectations\n"]}],"source":["# 🔧 RAG KNOWLEDGE BASE SETUP\n","# Creating vectorized knowledge base from CV and job expectations\n","\n","class RAGKnowledgeBase:\n","\n","    def __init__(self, project_path: str):\n","        self.project_path = project_path\n","        self.embeddings = None\n","        self.vectorstore = None\n","        self.documents = []\n","\n","        # Text splitter configuration for optimal chunking\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=50,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \". \"] #, \" \", \"\"]\n","            # separators=[\"##]\n","        )\n","\n","    def setup_embeddings(self):\n","        \"\"\"Initialize sentence transformer embeddings optimized for recruitment context\"\"\"\n","        print(\"🤖 Initializing embeddings model...\")\n","\n","        # Using a higher-quality model optimized for semantic similarity and retrieval\n","        self.embeddings = HuggingFaceEmbeddings(\n","            model_name=embedding_model_name,\n","            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n","            encode_kwargs={\"normalize_embeddings\": True}\n","        )\n","        print(\"✅ Embeddings model loaded successfully\")\n","        return self.embeddings\n","\n","    def load_and_process_documents(self, cv_content: str, expectations_content: str):\n","        \"\"\"Load and process CV and job expectations into document chunks\"\"\"\n","        print(\"📄 Processing documents for RAG...\")\n","        # Create documents with metadata\n","        documents = [\n","            Document(\n","                page_content=cv_content,\n","                metadata={\"source\": \"cv\", \"type\": \"professional_profile\"}\n","            ),\n","            Document(\n","                page_content=expectations_content,\n","                metadata={\"source\": \"job_expectations\", \"type\": \"requirements\"}\n","            )\n","        ]\n","        # Split documents into chunks\n","        self.documents = self.text_splitter.split_documents(documents)\n","\n","        print(f\"✅ Created {len(self.documents)} document chunks\")\n","        for doc in self.documents:\n","            print(f\"   📋 {doc.metadata['source']}: {len(doc.page_content)} chars\")\n","\n","        return self.documents\n","\n","    def create_vectorstore(self):\n","        \"\"\"Create FAISS vectorstore from processed documents\"\"\"\n","        if not self.documents:\n","            raise ValueError(\"No documents loaded. Call load_and_process_documents first.\")\n","        if not self.embeddings:\n","            self.setup_embeddings()\n","        print(\"🔍 Creating FAISS vectorstore...\")\n","\n","        # Create vectorstore\n","        self.vectorstore = FAISS.from_documents(\n","            documents=self.documents,\n","            embedding=self.embeddings\n","        )\n","        print(\"✅ Vectorstore created successfully\")\n","        return self.vectorstore\n","\n","    def search_relevant_context(self, query: str, k: int = 3):\n","        \"\"\"Search for relevant context given a query\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vectorstore not created. Call create_vectorstore first.\")\n","\n","        # Perform similarity search\n","        relevant_docs = self.vectorstore.similarity_search(query, k=k)\n","\n","        return relevant_docs\n","\n","    def get_context_string(self, query: str, k: int = 3):\n","        \"\"\"Get formatted context string for prompt injection with job_expectations always included\"\"\"\n","        relevant_docs = self.search_relevant_context(query, k)\n","\n","        context_parts = []\n","\n","        # ALWAYS include job_expectations first (critical but small)\n","        job_expectations_included = False\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source == \"job_expectations\":\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                job_expectations_included = True\n","                break\n","\n","        # If job_expectations wasn't in the retrieved docs, find and add it\n","        if not job_expectations_included:\n","            for doc in self.documents:\n","                if doc.metadata.get(\"source\") == \"job_expectations\":\n","                    content = doc.page_content.strip()\n","                    context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                    break\n","\n","        # Add other relevant documents\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source != \"job_expectations\":  # Skip if already added\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[{source.upper()}]: {content}\")\n","\n","        return \"\\n\\n\".join(context_parts)\n","\n","# Initialize RAG knowledge base\n","rag_kb = RAGKnowledgeBase(project_path)\n","\n","print(\"🧠 RAG Knowledge Base class initialized!\")\n","print(\"📚 Ready to process CV and job expectations\")\n"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1753663032935,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"Ej8MrFV_7SHF","outputId":"174984fc-683e-4528-8176-d982c6df436f"},"outputs":[{"output_type":"stream","name":"stdout","text":["🧠 RAG Knowledge Base class initialized!\n","📚 Ready to process CV and job expectations\n"]}],"source":["# 🔧 RAG KNOWLEDGE BASE SETUP\n","# Creating vectorized knowledge base from CV and job expectations\n","\n","class RAGKnowledgeBase:\n","\n","    def __init__(self, project_path: str):\n","        self.project_path = project_path\n","        self.embeddings = None\n","        self.vectorstore = None\n","        self.documents = []\n","\n","        # Text splitter configuration for optimal chunking\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=50,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \". \"] #, \" \", \"\"]\n","            # separators=[\"##]\n","        )\n","\n","    def setup_embeddings(self):\n","        \"\"\"Initialize sentence transformer embeddings optimized for recruitment context\"\"\"\n","        print(\"🤖 Initializing embeddings model...\")\n","\n","        # Using a higher-quality model optimized for semantic similarity and retrieval\n","        self.embeddings = HuggingFaceEmbeddings(\n","            model_name=embedding_model_name,\n","            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n","            encode_kwargs={\"normalize_embeddings\": True}\n","        )\n","        print(\"✅ Embeddings model loaded successfully\")\n","        return self.embeddings\n","\n","    def load_and_process_documents(self, cv_content: str, expectations_content: str):\n","        \"\"\"Load and process CV and job expectations into document chunks\"\"\"\n","        print(\"📄 Processing documents for RAG...\")\n","        # Create documents with metadata\n","        documents = [\n","            Document(\n","                page_content=cv_content,\n","                metadata={\"source\": \"cv\", \"type\": \"professional_profile\"}\n","            ),\n","            Document(\n","                page_content=expectations_content,\n","                metadata={\"source\": \"job_expectations\", \"type\": \"requirements\"}\n","            )\n","        ]\n","        # Split documents into chunks\n","        self.documents = self.text_splitter.split_documents(documents)\n","\n","        print(f\"✅ Created {len(self.documents)} document chunks\")\n","        for doc in self.documents:\n","            print(f\"   📋 {doc.metadata['source']}: {len(doc.page_content)} chars\")\n","\n","        return self.documents\n","\n","    def create_vectorstore(self):\n","        \"\"\"Create FAISS vectorstore from processed documents\"\"\"\n","        if not self.documents:\n","            raise ValueError(\"No documents loaded. Call load_and_process_documents first.\")\n","        if not self.embeddings:\n","            self.setup_embeddings()\n","        print(\"🔍 Creating FAISS vectorstore...\")\n","\n","        # Create vectorstore\n","        self.vectorstore = FAISS.from_documents(\n","            documents=self.documents,\n","            embedding=self.embeddings\n","        )\n","        print(\"✅ Vectorstore created successfully\")\n","        return self.vectorstore\n","\n","    def search_relevant_context(self, query: str, k: int = 3):\n","        \"\"\"Search for relevant context given a query\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vectorstore not created. Call create_vectorstore first.\")\n","\n","        # Perform similarity search\n","        relevant_docs = self.vectorstore.similarity_search(query, k=k)\n","\n","        return relevant_docs\n","\n","    def get_context_string(self, query: str, k: int = 3):\n","        \"\"\"Get formatted context string for prompt injection with job_expectations always included\"\"\"\n","        relevant_docs = self.search_relevant_context(query, k)\n","\n","        context_parts = []\n","\n","        # ALWAYS include job_expectations first (critical but small)\n","        job_expectations_included = False\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source == \"job_expectations\":\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                job_expectations_included = True\n","                break\n","\n","        # If job_expectations wasn't in the retrieved docs, find and add it\n","        if not job_expectations_included:\n","            for doc in self.documents:\n","                if doc.metadata.get(\"source\") == \"job_expectations\":\n","                    content = doc.page_content.strip()\n","                    context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                    break\n","\n","        # Add other relevant documents\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source != \"job_expectations\":  # Skip if already added\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[{source.upper()}]: {content}\")\n","\n","        return \"\\n\\n\".join(context_parts)\n","\n","# Initialize RAG knowledge base\n","rag_kb = RAGKnowledgeBase(project_path)\n","\n","print(\"🧠 RAG Knowledge Base class initialized!\")\n","print(\"📚 Ready to process CV and job expectations\")\n"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1753663032940,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"vAfe7CJ05Icl","outputId":"97249e58-78fa-4128-bd3c-f2458444f2e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔧 ROBUST FILE SAVING PATCH APPLIED!\n","✅ Enhanced error handling for benchmark file operations\n","💾 Recovery mechanisms activated for failed saves\n","📊 Disk space monitoring enabled\n"]}],"source":["# 🔧 ROBUST FILE SAVING PATCH\n","# Aplicar patch a las funciones de guardado existentes para manejo robusto de errores\n","\n","def robust_save_json(file_path, data, description=\"file\"):\n","    \"\"\"Helper function for robust JSON file saving with error handling\"\"\"\n","    import shutil\n","    from datetime import datetime\n","    import traceback\n","\n","    try:\n","        # Ensure directory exists\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","        # Check disk space\n","        disk_usage = shutil.disk_usage(os.path.dirname(file_path))\n","        free_gb = disk_usage.free / 1e9\n","\n","        if free_gb < 0.1:  # Less than 100MB\n","            print(f\"⚠️ Warning: Low disk space ({free_gb:.1f} GB)\")\n","\n","        # Write file with verification\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, default=str, ensure_ascii=False)\n","\n","        # Verify file was created and has content\n","        if os.path.exists(file_path):\n","            file_size = os.path.getsize(file_path)\n","            print(f\"✅ {description} saved: {file_path} ({file_size} bytes)\")\n","            return True\n","        else:\n","            raise FileNotFoundError(f\"File was not created: {file_path}\")\n","\n","    except Exception as error:\n","        print(f\"❌ Error saving {description}: {str(error)}\")\n","\n","        # Try alternative filename with timestamp\n","        base_name, ext = os.path.splitext(file_path)\n","        alt_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        alt_file = f\"{base_name}_recovery_{alt_timestamp}{ext}\"\n","\n","        try:\n","            with open(alt_file, 'w', encoding='utf-8') as f:\n","                json.dump(data, f, indent=2, default=str, ensure_ascii=False)\n","            print(f\"✅ {description} saved to recovery file: {alt_file}\")\n","            return True\n","        except Exception as alt_error:\n","            print(f\"❌ Recovery save also failed: {str(alt_error)}\")\n","            print(f\"📍 Full traceback: {traceback.format_exc()}\")\n","            return False\n","\n","# Patch the existing save methods with robust error handling\n","original_save_iteration = benchmark.save_benchmark_iteration\n","original_update_history = benchmark.update_comparison_history\n","\n","def patched_save_benchmark_iteration(self):\n","    \"\"\"Enhanced save_benchmark_iteration with robust error handling\"\"\"\n","    from datetime import datetime\n","\n","    try:\n","        # Create detailed iteration data (same as original)\n","        iteration_data = {\n","            \"metadata\": {\n","                \"timestamp\": self.benchmark_timestamp,\n","                \"datetime_readable\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","                \"total_models\": len(self.candidate_models),\n","                \"successful_models\": len([r for r in self.results if r.get(\"status\") == \"success\"]),\n","                \"failed_models\": len([r for r in self.results if r.get(\"status\") == \"failed\"]),\n","                \"test_scenarios\": len(self.test_prompts)\n","            },\n","            \"environment_config\": self.environment_config,\n","            \"test_scenarios\": self.test_prompts,\n","            \"model_specifications\": self.model_specs,\n","            \"detailed_results\": self.results,\n","            \"summary\": self.generate_benchmark_summary()\n","        }\n","\n","        # Use robust save function\n","        iteration_file = f\"{self.results_folder}/benchmark_{self.benchmark_timestamp}.json\"\n","        success = robust_save_json(iteration_file, iteration_data, \"Benchmark iteration\")\n","\n","        if success:\n","            # Update comparison history\n","            self.update_comparison_history(iteration_data)\n","            return iteration_file\n","        else:\n","            print(f\"❌ Failed to save benchmark iteration\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"❌ Critical error in patched_save_benchmark_iteration: {str(e)}\")\n","        return None\n","\n","def patched_update_comparison_history(self, current_iteration):\n","    \"\"\"Enhanced update_comparison_history with robust error handling\"\"\"\n","    comparison_file = f\"{self.results_folder}/benchmark_comparison_history.json\"\n","\n","    try:\n","        # Load existing history with error handling\n","        if os.path.exists(comparison_file):\n","            try:\n","                with open(comparison_file, 'r', encoding='utf-8') as f:\n","                    history = json.load(f)\n","                print(f\"📚 Loaded existing history with {len(history.get('iterations', []))} iterations\")\n","            except Exception as load_error:\n","                print(f\"⚠️ Error loading history: {str(load_error)}. Creating new history.\")\n","                history = {\n","                    \"iterations\": [],\n","                    \"models_tracked\": list(set(self.candidate_models)),\n","                    \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","                }\n","        else:\n","            print(f\"📝 Creating new history file\")\n","            history = {\n","                \"iterations\": [],\n","                \"models_tracked\": list(set(self.candidate_models)),\n","                \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","            }\n","\n","        # Add current iteration summary\n","        history[\"iterations\"].append({\n","            \"timestamp\": current_iteration[\"metadata\"][\"timestamp\"],\n","            \"datetime\": current_iteration[\"metadata\"][\"datetime_readable\"],\n","            \"summary\": current_iteration[\"summary\"],\n","            \"metadata\": current_iteration[\"metadata\"]\n","        })\n","\n","        # Keep only last 10 iterations\n","        history[\"iterations\"] = history[\"iterations\"][-10:]\n","        history[\"last_updated\"] = current_iteration[\"metadata\"][\"datetime_readable\"]\n","\n","        # Use robust save function\n","        robust_save_json(comparison_file, history, \"Comparison history\")\n","\n","    except Exception as e:\n","        print(f\"❌ Critical error in patched_update_comparison_history: {str(e)}\")\n","\n","# Apply patches to the benchmark instance\n","benchmark.save_benchmark_iteration = patched_save_benchmark_iteration.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark.update_comparison_history = patched_update_comparison_history.__get__(benchmark, CacheAwareModelBenchmark)\n","\n","print(\"🔧 ROBUST FILE SAVING PATCH APPLIED!\")\n","print(\"✅ Enhanced error handling for benchmark file operations\")\n","print(\"💾 Recovery mechanisms activated for failed saves\")\n","print(\"📊 Disk space monitoring enabled\")\n"]},{"cell_type":"markdown","metadata":{"id":"qVZHz6ilu2_9"},"source":["## **--> Phase 3**: fix issues related with current models"]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1753663032979,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"14Zl6tLXxNl5"},"outputs":[],"source":["# 🔧 COMPREHENSIVE GEMMA DIVISION BY ZERO FIX\n","import warnings\n","import numpy as np\n","\n","# Global numpy error state configuration\n","old_err_state = np.seterr(divide='ignore', invalid='ignore', over='ignore', under='ignore')\n","\n","# Comprehensive warning suppression for numerical issues\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*torch.utils.checkpoint.*\")\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# Monkey patch the test_single_prompt method with comprehensive Gemma fixes\n","original_test_single_prompt = benchmark.test_single_prompt\n","\n","def gemma_safe_test_single_prompt(self, model, tokenizer, model_name: str, test_prompt: dict):\n","    \"\"\"Comprehensive Gemma-safe version with multiple fallback strategies\"\"\"\n","    full_prompt = f\"{self.system_prompt}\\n\\nRecruiter: {test_prompt['message']}\\n\\nResponse:\"\n","\n","    try:\n","        # Comprehensive protection for all models with special handling for Gemma\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            with np.errstate(all='ignore'):  # Ignore all numpy warnings\n","                return self._safe_generate_response(model, tokenizer, model_name, test_prompt, full_prompt)\n","\n","    except Exception as e:\n","        # Enhanced error handling with type detection\n","        error_msg = str(e).lower()\n","        if any(keyword in error_msg for keyword in ['division', 'divide', 'zero', 'nan', 'inf']):\n","            print(f\"     🔧 Numerical instability detected, applying fallback...\")\n","            try:\n","                return self._fallback_generation(model, tokenizer, model_name, test_prompt, full_prompt)\n","            except Exception as fallback_error:\n","                return {\n","                    \"prompt_name\": test_prompt[\"name\"],\n","                    \"status\": \"failed\",\n","                    \"error\": f\"Primary: {str(e)}, Fallback: {str(fallback_error)}\"\n","                }\n","        else:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": str(e)\n","            }\n","\n","def safe_generate_response(self, model, tokenizer, model_name: str, test_prompt: dict, full_prompt: str):\n","    \"\"\"Enhanced response generation with model-specific optimizations\"\"\"\n","\n","    # Enhanced tokenization with explicit parameters\n","    inputs = tokenizer(\n","        full_prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512,\n","        padding=True,\n","        add_special_tokens=True\n","    )\n","\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        inference_start = time.time()\n","\n","        # Enhanced model-specific generation parameters\n","        if \"phi-3\" in model_name.lower():\n","            generation_config = {\n","                **inputs,\n","                'max_new_tokens': 100,\n","                'do_sample': True,\n","                'temperature': 0.7,\n","                'pad_token_id': tokenizer.eos_token_id,\n","                'use_cache': False\n","            }\n","        elif \"gemma\" in model_name.lower():\n","            # Optimized Gemma configuration (cleaned invalid parameters)\n","            generation_config = {\n","                'input_ids': inputs['input_ids'],\n","                'attention_mask': inputs.get('attention_mask'),\n","                'max_new_tokens': 80,  # Reduced to avoid memory issues\n","                'do_sample': False,    # Use greedy decoding for stability\n","                'pad_token_id': tokenizer.pad_token_id or tokenizer.eos_token_id,\n","                'eos_token_id': tokenizer.eos_token_id,\n","                'repetition_penalty': 1.1,\n","                'use_cache': True     # Enable cache for Gemma\n","                # Removed: top_p, early_stopping (invalid for Gemma)\n","            }\n","        else:\n","            generation_config = {\n","                **inputs,\n","                'max_new_tokens': 100,\n","                'do_sample': True,\n","                'temperature': 0.7,\n","                'pad_token_id': tokenizer.eos_token_id\n","            }\n","\n","        # Generate with comprehensive error handling\n","        outputs = model.generate(**generation_config)\n","        inference_time = time.time() - inference_start\n","\n","    # Enhanced response decoding\n","    try:\n","        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        # Clean up response by removing prompt\n","        if full_prompt in response:\n","            response = response.replace(full_prompt, \"\").strip()\n","        elif response.startswith(full_prompt[:50]):  # Partial match fallback\n","            response = response[len(full_prompt):].strip()\n","    except Exception as decode_error:\n","        response = f\"[Decoding error: {str(decode_error)}]\"\n","\n","    return {\n","        \"prompt_name\": test_prompt[\"name\"],\n","        \"inference_time\": inference_time,\n","        \"response\": response, # [:300] + \"...\" if len(response) > 300 else response,\n","        \"status\": \"success\"\n","    }\n","\n","def fallback_generation(self, model, tokenizer, model_name: str, test_prompt: dict, full_prompt: str):\n","    \"\"\"Ultra-conservative fallback generation for problematic models\"\"\"\n","    print(f\"\\t\\t🆘 Using ultra-conservative fallback generation...\")\n","\n","    # Minimal tokenization\n","    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        inference_start = time.time()\n","\n","        # Ultra-conservative generation\n","        try:\n","            outputs = model.generate(\n","                inputs['input_ids'],\n","                max_new_tokens=50,     # Very limited output\n","                do_sample=False,       # Greedy only\n","                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n","                eos_token_id=tokenizer.eos_token_id,\n","                use_cache=False,       # Disable cache\n","                output_scores=False,   # Disable score computation\n","                return_dict_in_generate=False\n","            )\n","            inference_time = time.time() - inference_start\n","\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            response = response.replace(full_prompt, \"\").strip() or \"[Model generated empty response]\"\n","\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"inference_time\": inference_time,\n","                \"response\": f\"[FALLBACK] {response}\",\n","                \"status\": \"success\"\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": f\"Fallback failed: {str(e)}\"\n","            }\n","\n","# Apply comprehensive patches\n","benchmark.test_single_prompt = gemma_safe_test_single_prompt.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark._safe_generate_response = safe_generate_response.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark._fallback_generation = fallback_generation.__get__(benchmark, CacheAwareModelBenchmark)"]},{"cell_type":"markdown","metadata":{"id":"BoXIj5Huq2Ri"},"source":["## **--> Phase 4:** Execute benchmarking"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1753663032989,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"ZSD894ZJ7cPL","outputId":"9bc6e625-4299-45d2-cfca-b8014d4115fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["====================================================================================================\n","========================================= START OF PROCESS =========================================\n","====================================================================================================\n","\n","🖥️ ENVIRONMENT CONFIGURATION:\n","============================================================\n","🎮 GPU: NVIDIA A100-SXM4-40GB\n","💾 GPU Memory: 42.47 GB (40506 MB)\n","🔧 Compute Capability: 8.0\n","☁️ Colab GPU Type: A100\n","🧠 CPU: 12 cores\n","🔍 CPU Model: Intel(R) Xeon(R) CPU @ 2.20GHz\n","💿 RAM: 89.63 GB total\n","📊 RAM Usage: 7.65/89.63 GB (9.6%)\n","🐍 Python: 3.11.13\n","🔥 PyTorch: 2.6.0+cu124\n","🤗 Transformers: 4.53.3\n","⚡ CUDA: 12.4\n","🌐 Environment: Google Colab\n","\n","🔍 CACHE STATUS:\n","================================================================================\n","✅ Found 4 cached models:\n","\t⚡ microsoft/Phi-3-mini-4k-instruct\n","\t⚡ google/gemma-3-4b-it\n","\t⚡ meta-llama/Meta-Llama-3-8B-Instruct\n","\t⚡ mistralai/Mistral-7B-Instruct-v0.3\n","\n","⬇️Download vs 💻Cache status:\n","\t⚡ mistralai/Mistral-7B-Instruct-v0.3 - 💻 Will be load from cache\n","\t⚡ meta-llama/Meta-Llama-3-8B-Instruct - 💻 Will be load from cache\n","\t⚡ microsoft/Phi-3-mini-4k-instruct - 💻 Will be load from cache\n","\t⚡ google/gemma-3-4b-it - 💻 Will be load from cache\n","\n","\n","⌨️ SYSTEM PROMPT:\n","============================================================\n","\n","        You are my highly intelligent personal assistant.\n","        Your mission is to engage in a continuous role-playing conversation where you will act as me.\n","        From this moment forward, the user will be playing the role of various recruiters contacting me.\n","        You must analyze the messages and generate appropriate responses as if you were me talking with a recruiter.\n","        Do not break character. Do not mention that you are an AI or an assistant in your replies to the recruiters.\n","        Analyze this job offer and tell me if it matches my profile:\n","        ***I am a Data and AI Engineer with extensive experience in building robust ELT pipelines and developing cutting-edge Generative AI solutions.\n","        My key skills include designing systems with Retrieval-Augmented Generation (RAG) and AI Agents using frameworks like Semantic Kernel and LangChain.\n","        I am proficient in data engineering with PySpark, SQL, and Apache Airflow,\n","        and highly skilled in cloud platforms, particularly Microsoft Azure (Data Factory, Databricks, AI Services) and GCP (BigQuery, Composer).\n","        I also possess strong backend development experience using Python and FastAPI to build and deploy services.***\n","        \n","\n","\n","🎯 BENCHMARK SCENARIOS:\n","============================================================\n","  1. Perfect Match: \n","            Hi! I hope you are doing well.\n","            I came across your profile and I am impressed by your background in AI and data engineering.\n","            I have an exciting opportunity for a Senior Data Engineer position at a fast-growing fintech company.\n","            The role involves working with Python, cloud technologies, and building ML pipelines.\n","            The salary range is €60,000-65,000 and it is 100% remote.\n","            Would you be interested in learning more?\n","            \n","  2. Generic Message: \n","            Hello, are you currently open to new opportunities?\n","            \n","  3. Wrong Match: \n","            We have a Java developer position available at our company.\n","            5 years experience required, on-site work in London, competitive salary.\n","            Interested?\n","            \n","\n","⏸️ Set 'run_benchmark = True' above to start the OPTIMIZED benchmark\n","🎯 Multiple test scenarios: READY\n","📊 Detailed comparison: READY\n","🆘 Automatic fallback system: ACTIVE\n","💾 Your cached models will save significant time!\n","\n","====================================================================================================\n","========================================= END OF PROCESS =========================================\n","====================================================================================================\n"]}],"source":["print(\"=\"*100 )\n","print(\"=\"*41 + \" START OF PROCESS \" + \"=\"*41 )\n","print(\"=\"*100 )\n","\n","# Display environment configuration\n","benchmark.display_environment_config()\n","\n","# Check cache status\n","cached_models = display_cache_status(benchmark.candidate_models)\n","\n","# 🎯 SYSTEM PROMPT\n","print(f\"\\n\\n⌨️ SYSTEM PROMPT:\")\n","print(\"=\" * 60)\n","print(benchmark.system_prompt)\n","\n","\n","# 🎯 TEST SCENARIOS INFO\n","print(f\"\\n\\n🎯 BENCHMARK SCENARIOS:\")\n","print(\"=\" * 60)\n","\n","for i, prompt in enumerate(benchmark.test_prompts):\n","    print(f\"  {i+1}. {prompt['name']}: {prompt['message']}\")\n","\n","\n","######### 🚀 RUN COMPREHENSIVE BENCHMARK  #########\n","run_benchmark = False # ✅ Set to True to test\n","\n","if run_benchmark:\n","    print(\"\\n🔧 STARTING BENCHMARK\")\n","    print(f\"📁 Results will be saved to: {benchmark.results_folder}\")\n","    print(f\"🕒 Benchmark timestamp: {benchmark.benchmark_timestamp}\")\n","\n","    benchmark.benchmark_all_models()\n","    benchmark.display_detailed_results()\n","\n","    print(f\"\\n✅ BENCHMARK COMPLETE!\")\n","    print(f\"📊 Check {benchmark.results_folder}/ for detailed results and comparison history\")\n","else:\n","    print(\"\\n⏸️ Set 'run_benchmark = True' above to start the OPTIMIZED benchmark\")\n","\n","print(\"🎯 Multiple test scenarios: READY\")\n","print(\"📊 Detailed comparison: READY\")\n","print(\"🆘 Automatic fallback system: ACTIVE\")\n","print(\"💾 Your cached models will save significant time!\\n\")\n","\n","\n","print(\"=\"*100 )\n","print(\"=\"*41 + \" END OF PROCESS \" + \"=\"*41 )\n","print(\"=\"*100 )"]},{"cell_type":"markdown","metadata":{"id":"CSZQ5tdSvOyk"},"source":["## **Phase 5:** Analyzing results"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1753663033010,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"SVENij8tw-2o","outputId":"d0eff15b-fe87-44cb-8e18-98c71760bf6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔍 BENCHMARK EXPLORER READY!\n"]}],"source":["# 📊 BENCHMARK RESULTS EXPLORER\n","# Run this cell AFTER BECHMARKING to explore saved iterations\n","\n","# List all iterations\n","results_folder = f\"{project_path}/benchmark_iterations\"\n","iteration_files = [f for f in os.listdir(results_folder) if f.startswith('benchmark_') and f.endswith('.json')]\n","iteration_files.sort(reverse=True)  # Most recent first\n","\n","\n","def explore_benchmark_history():\n","    \"\"\"Explore saved benchmark iterations and history\"\"\"\n","\n","\n","    if not os.path.exists(results_folder):\n","        print(\"❌ No benchmark iterations found. Run the benchmark first!\")\n","        return\n","\n","    print(f\"📁 BENCHMARK ITERATIONS FOUND: {len(iteration_files)}\")\n","    print(\"-\" * 60)\n","\n","    if iteration_files:\n","        for i, filename in enumerate(iteration_files[:5], 1):  # Show last 5\n","            timestamp = filename.replace('benchmark_', '').replace('.json', '')\n","            # Parse timestamp\n","            from datetime import datetime\n","            try:\n","                dt = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')\n","                readable_date = dt.strftime('%Y-%m-%d %H:%M:%S')\n","                print(f\"{i}. {filename} ({readable_date})\")\n","            except:\n","                print(f\"{i}. {filename}\")\n","\n","    # Show comparison history if exists\n","    history_file = f\"{results_folder}/benchmark_comparison_history.json\"\n","    if os.path.exists(history_file):\n","        print(f\"\\n📈 COMPARISON HISTORY:\")\n","        with open(history_file, 'r', encoding='utf-8') as f:\n","            history = json.load(f)\n","\n","        print(f\"Total iterations tracked: {len(history['iterations'])}\")\n","        print(f\"Models tracked: {', '.join([m.split('/')[-1] for m in history['models_tracked']])}\")\n","        print(f\"Last updated: {history['last_updated']}\")\n","\n","        if history['iterations']:\n","            latest = history['iterations'][-1]\n","            if 'summary' in latest and 'model_performance_scores' in latest['summary']:\n","                print(f\"\\n🏆 LATEST PERFORMANCE RANKING:\")\n","                for i, score in enumerate(latest['summary']['model_performance_scores'][:3], 1):\n","                    model_name = score['model'].split('/')[-1]\n","                    print(f\"  {i}. {model_name} (Score: {score['performance_score']})\")\n","\n","    return results_folder, iteration_files\n","\n","def load_specific_iteration(timestamp_or_index=None):\n","    \"\"\"Load a specific benchmark iteration for detailed analysis\"\"\"\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    iteration_files = [f for f in os.listdir(results_folder) if f.startswith('benchmark_2') and f.endswith('.json')]\n","    iteration_files.sort(reverse=True)\n","\n","    if not iteration_files:\n","        print(\"❌ No iterations found!\")\n","        return None\n","\n","    # Determine which file to load\n","    if timestamp_or_index is None:\n","        # Load most recent\n","        target_file = iteration_files[0]\n","        print(f\"📄 Loading most recent iteration: {target_file}\")\n","    elif isinstance(timestamp_or_index, int):\n","        # Load by index (1-based)\n","        if 1 <= timestamp_or_index <= len(iteration_files):\n","            target_file = iteration_files[timestamp_or_index - 1]\n","            print(f\"📄 Loading iteration #{timestamp_or_index}: {target_file}\")\n","        else:\n","            print(f\"❌ Invalid index. Available: 1-{len(iteration_files)}\")\n","            return None\n","    else:\n","        # Load by timestamp\n","        target_file = f\"benchmark_{timestamp_or_index}.json\"\n","        if target_file not in iteration_files:\n","            print(f\"❌ Timestamp {timestamp_or_index} not found!\")\n","            return None\n","        print(f\"📄 Loading iteration: {target_file}\")\n","\n","    # Load the iteration data\n","    with open(f\"{results_folder}/{target_file}\", 'r', encoding='utf-8') as f:\n","        iteration_data = json.load(f)\n","\n","\n","    # Display environment config if available\n","    if 'environment_config' in iteration_data:\n","        env = iteration_data['environment_config']\n","        if env['hardware']['gpu']['available']:\n","            gpu = env['hardware']['gpu']['devices'][0]\n","            gpu_type = env[\"colab_specific\"].get(\"colab_gpu_type\", 'Unknown') if env[\"colab_specific\"][\"is_colab\"] else 'Local'\n","            print(f\"🎮 Environment: {gpu['name']} ({gpu['memory_total_gb']} GB) - {gpu_type}\")\n","        else:\n","            print(f\"🖥️ Environment: CPU-only\")\n","\n","        env_type = \"Google Colab\" if env[\"colab_specific\"][\"is_colab\"] else \"Local/Other\"\n","        print(f\"🌐 Platform: {env_type}\")\n","\n","    return iteration_data\n","\n","def compare_environments_performance():\n","    \"\"\"Compare performance across different environment configurations\"\"\"\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    history_file = f\"{results_folder}/benchmark_comparison_history.json\"\n","\n","    if not os.path.exists(history_file):\n","        print(\"❌ No comparison history found. Run benchmarks first!\")\n","        return\n","\n","    with open(history_file, 'r', encoding='utf-8') as f:\n","        history = json.load(f)\n","\n","    if len(history['iterations']) < 2:\n","        print(\"❌ Need at least 2 benchmark iterations to compare environments\")\n","        return\n","\n","    print(\"🔄 ENVIRONMENT PERFORMANCE COMPARISON\")\n","    print(\"=\" * 60)\n","\n","    # Group iterations by environment\n","    env_groups = {}\n","\n","    for iteration in history['iterations']:\n","        # Load full iteration data to get environment config\n","        iteration_file = f\"{results_folder}/benchmark_{iteration['timestamp']}.json\"\n","        if os.path.exists(iteration_file):\n","            with open(iteration_file, 'r', encoding='utf-8') as f:\n","                full_data = json.load(f)\n","\n","            if 'environment_config' in full_data:\n","                env = full_data['environment_config']\n","\n","                # Create environment signature\n","                if env['hardware']['gpu']['available']:\n","                    gpu_name = env['hardware']['gpu']['devices'][0]['name']\n","                    gpu_memory = env['hardware']['gpu']['devices'][0]['memory_total_gb']\n","                    colab_type = env[\"colab_specific\"].get(\"colab_gpu_type\", 'Unknown')\n","                    env_signature = f\"{colab_type} ({gpu_name}) - {gpu_memory}GB\"\n","                else:\n","                    env_signature = \"CPU-only\"\n","\n","                if env_signature not in env_groups:\n","                    env_groups[env_signature] = []\n","\n","                env_groups[env_signature].append({\n","                    'timestamp': iteration['timestamp'],\n","                    'datetime': iteration['datetime'],\n","                    'summary': iteration['summary']\n","                })\n","\n","    # Display comparison\n","    for env_sig, iterations in env_groups.items():\n","        print(f\"\\n🎮 {env_sig}:\")\n","        print(f\"\\t📊 Iterations: {len(iterations)}\")\n","\n","        if iterations and 'model_performance_scores' in iterations[-1]['summary']:\n","            latest_scores = iterations[-1]['summary']['model_performance_scores']\n","            print(f\"\\t🏆 Best Model: {latest_scores[0]['model'].split('/')[-1]} (Score: {latest_scores[0]['performance_score']})\")\n","\n","            # Calculate average performance across iterations for this environment\n","            if len(iterations) > 1:\n","                avg_inference_times = []\n","                for iter_data in iterations:\n","                    if 'overall_stats' in iter_data['summary']:\n","                        avg_inference_times.append(iter_data['summary']['overall_stats']['avg_inference_time'])\n","\n","                if avg_inference_times:\n","                    avg_time = sum(avg_inference_times) / len(avg_inference_times)\n","                    print(f\"   ⚡ Avg Inference Time: {avg_time:.2f}s (across {len(avg_inference_times)} runs)\")\n","\n","    # Show environment impact on specific models\n","    print(f\"\\n📈 MODEL PERFORMANCE BY ENVIRONMENT:\")\n","    print(\"-\" * 70)\n","\n","    model_env_performance = {}\n","    for env_sig, iterations in env_groups.items():\n","        for iteration in iterations:\n","            if \"model_performance_scores\" in iteration[\"summary\"]:\n","                for model_score in iteration[\"summary\"][\"model_performance_scores\"]:\n","                    model_name = model_score[\"model\"].split(\"/\")[-1]\n","                    if model_name not in model_env_performance:\n","                        model_env_performance[model_name] = {}\n","\n","                    if env_sig not in model_env_performance[model_name]:\n","                        model_env_performance[model_name][env_sig] = []\n","\n","                    model_env_performance[model_name][env_sig].append(model_score[\"performance_score\"])\n","\n","    # Display model performance across environments\n","    for model_name, env_scores in model_env_performance.items():\n","        print(f\"\\n🤖 {model_name}:\")\n","        for env_sig, scores in env_scores.items():\n","            avg_score = sum(scores) / len(scores)\n","            runs = len(scores)\n","            print(f\"   {env_sig}: {avg_score:.2f} avg score ({runs} run{'s' if runs != 1 else ''})\")\n","\n","# Quick exploration\n","print(\"🔍 BENCHMARK EXPLORER READY!\")"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1753663033016,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"mWpISuCxFwfE","outputId":"597ada64-8f4e-4f7c-e5c1-93e9dd60b095"},"outputs":[{"output_type":"stream","name":"stdout","text":["📁 BENCHMARK ITERATIONS FOUND: 5\n","------------------------------------------------------------\n","1. benchmark_comparison_history.json\n","2. benchmark_20250725_181432.json (2025-07-25 18:14:32)\n","3. benchmark_20250725_165345.json (2025-07-25 16:53:45)\n","4. benchmark_20250725_082023.json (2025-07-25 08:20:23)\n","5. benchmark_20250724_141407.json (2025-07-24 14:14:07)\n","\n","📈 COMPARISON HISTORY:\n","Total iterations tracked: 1\n","Models tracked: gemma-3-4b-it, Phi-3-mini-4k-instruct, Mistral-7B-Instruct-v0.3, Meta-Llama-3-8B-Instruct\n","Last updated: 2025-07-24 14:44:22\n","\n","🏆 LATEST PERFORMANCE RANKING:\n","  1. Mistral-7B-Instruct-v0.3 (Score: 6.0)\n","  2. Meta-Llama-3-8B-Instruct (Score: 7.22)\n","  3. Phi-3-mini-4k-instruct (Score: 8.02)\n"]}],"source":["want_to_explore_benchmark_history = True\n","\n","if want_to_explore_benchmark_history:\n","    # Auto-explore if results exist\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    if os.path.exists(results_folder):\n","        explore_benchmark_history()\n","    else:\n","        print(\"❌ No benchmark iterations found. Run the benchmark first!\")\n"]},{"cell_type":"markdown","metadata":{"id":"COdYWYdMtTGp"},"source":["# **Stage 3: Adapt & Align Model**"]},{"cell_type":"code","execution_count":70,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1753663033017,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"YMpUbXaoWf2q"},"outputs":[],"source":["# Selected LLM model\n","selected_text_generator_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","# selected_text_generator_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","# Selected RAG embedding model\n","embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n","# embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\"\n","\n","# Selected guardrail model\n","input_guardrail_model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n","\n","# Selected output guardrail model\n","# output_guardrail_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","output_guardrail_model_name = \"microsoft/Phi-3-mini-4k-instruct\""]},{"cell_type":"markdown","metadata":{"id":"FIEHKdMe-zZm"},"source":["## ***PHASE 1***- 🔧 RAG KNOWLEDGE BASE SETUP\n","Creating vectorized knowledge base from CV and job expectations"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1753663033042,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"YfB38JnsPBsT","outputId":"1ff10684-dd48-4702-8931-ebc978ba36a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["🧠 RAG Knowledge Base class initialized!\n","📚 Ready to process CV and job expectations\n"]}],"source":["class RAGKnowledgeBase:\n","\n","    def __init__(self, project_path: str):\n","        self.project_path = project_path\n","        self.embeddings = None\n","        self.vectorstore = None\n","        self.documents = []\n","\n","        # Text splitter configuration for optimal chunking\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=50,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \". \", \"##\"] #, \" \", \"\"]\n","            # separators=[\"##\"]\n","        )\n","\n","    def setup_embeddings(self):\n","        \"\"\"Initialize sentence transformer embeddings optimized for recruitment context\"\"\"\n","        print(\"🤖 Initializing embeddings model...\")\n","\n","        # Using a higher-quality model optimized for semantic similarity and retrieval\n","        self.embeddings = HuggingFaceEmbeddings(\n","            model_name=embedding_model_name,\n","            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n","            encode_kwargs={\"normalize_embeddings\": True}\n","        )\n","\n","        print(\"✅ Embeddings model loaded successfully\")\n","        return self.embeddings\n","\n","    def load_and_process_documents(self, cv_content: str, expectations_content: str):\n","        \"\"\"Load and process CV and job expectations into document chunks\"\"\"\n","        print(\"📄 Processing documents for RAG...\")\n","\n","        # Create documents with metadata\n","        documents = [\n","            Document(\n","                page_content=cv_content,\n","                metadata={\"source\": \"cv\", \"type\": \"professional_profile\"}\n","            ),\n","            Document(\n","                page_content=expectations_content,\n","                metadata={\"source\": \"job_expectations\", \"type\": \"requirements\"}\n","            )\n","        ]\n","\n","        # Split documents into chunks\n","        self.documents = self.text_splitter.split_documents(documents)\n","\n","        print(f\"✅ Created {len(self.documents)} document chunks\")\n","        for doc in self.documents:\n","            print(f\"   📋 {doc.metadata['source']}: {len(doc.page_content)} chars\")\n","\n","        return self.documents\n","\n","    def create_vectorstore(self):\n","        \"\"\"Create FAISS vectorstore from processed documents\"\"\"\n","        if not self.documents:\n","            raise ValueError(\"No documents loaded. Call load_and_process_documents first.\")\n","\n","        if not self.embeddings:\n","            self.setup_embeddings()\n","\n","        print(\"🔍 Creating FAISS vectorstore...\")\n","\n","        # Create vectorstore\n","        self.vectorstore = FAISS.from_documents(\n","            documents=self.documents,\n","            embedding=self.embeddings\n","        )\n","\n","        print(\"✅ Vectorstore created successfully\")\n","        return self.vectorstore\n","\n","    def search_relevant_context(self, query: str, k: int = 3):\n","        \"\"\"Search for relevant context given a query\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vectorstore not created. Call create_vectorstore first.\")\n","\n","        # Perform similarity search\n","        relevant_docs = self.vectorstore.similarity_search(query, k=k)\n","\n","        return relevant_docs\n","\n","    def get_context_string(self, query: str, k: int = 3):\n","        \"\"\"Get formatted context string for prompt injection with job_expectations always included\"\"\"\n","        relevant_docs = self.search_relevant_context(query, k)\n","\n","        context_parts = []\n","\n","        # ALWAYS include job_expectations first (critical but small)\n","        job_expectations_included = False\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source == \"job_expectations\":\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                job_expectations_included = True\n","                break\n","\n","        # If job_expectations wasn't in the retrieved docs, find and add it\n","        if not job_expectations_included:\n","            for doc in self.documents:\n","                if doc.metadata.get(\"source\") == \"job_expectations\":\n","                    content = doc.page_content.strip()\n","                    context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                    break\n","\n","        # Add other relevant documents\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source != \"job_expectations\":  # Skip if already added\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[{source.upper()}]: {content}\")\n","\n","        return \"\\n\\n\".join(context_parts)\n","\n","# Initialize RAG knowledge base\n","rag_kb = RAGKnowledgeBase(project_path)\n","\n","print(\"🧠 RAG Knowledge Base class initialized!\")\n","print(\"📚 Ready to process CV and job expectations\")\n"]},{"cell_type":"markdown","metadata":{"id":"8yzAGLDP-zZm"},"source":["## ***PHASE 2*** -  🚀 SETUP RAG KNOWLEDGE BASE\n","Load and process documents for retrieval"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":765,"status":"ok","timestamp":1753663033808,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"xzTD-RelPBsT","outputId":"207b04b6-a84f-4083-8d29-a1528a9ea7db"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔧 Setting up RAG Knowledge Base...\n","🤖 Initializing embeddings model...\n","✅ Embeddings model loaded successfully\n","📄 Processing documents for RAG...\n","✅ Created 24 document chunks\n","   📋 cv: 193 chars\n","   📋 cv: 18 chars\n","   📋 cv: 479 chars\n","   📋 cv: 239 chars\n","   📋 cv: 231 chars\n","   📋 cv: 412 chars\n","   📋 cv: 217 chars\n","   📋 cv: 371 chars\n","   📋 cv: 190 chars\n","   📋 cv: 451 chars\n","   📋 cv: 433 chars\n","   📋 cv: 340 chars\n","   📋 cv: 216 chars\n","   📋 cv: 401 chars\n","   📋 cv: 391 chars\n","   📋 cv: 348 chars\n","   📋 cv: 287 chars\n","   📋 cv: 487 chars\n","   📋 cv: 40 chars\n","   📋 cv: 378 chars\n","   📋 cv: 457 chars\n","   📋 cv: 156 chars\n","   📋 cv: 255 chars\n","   📋 job_expectations: 326 chars\n","🔍 Creating FAISS vectorstore...\n","✅ Vectorstore created successfully\n","\n","🧪 Testing retrieval system...\n","\n","❓ Query: What are my technical skills in data engineering?\n","📄 Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between €60,000 and €65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: ## I.T. Skills\n","Generative AI: RAG (Retrieval-Augmented Generation), AI Agents, LLMs, Prompt Engineering, Semantic Kernel, LangChain, Fine Tuning, OpenAI APIs..\n","Data Engineering: ETL/ELT Processes, Data Lake, Data Warehouse, Apache Airflow, Compositor (GCP), Spark, Hadoop, Hive, PySpark.\n","\n","[CV]: . An AI & Data Engineer with experience in Generative AI, cloud platforms (Azure, GCP), and ELT processes. Skilled in project management, APIs, and agile methodologies with a strong analytical mindset, a fast learner, and results-oriented....\n","\n","❓ Query: What technologies do I work with?\n","📄 Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between €60,000 and €65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: ## Languages\n","Spanish: Native\n","English: Professional Working Proficiency\n","\n","\n","## Main Projects\n","Generative AI apps with Semantic Kernel / Beam Suntory\n","Development of Digital Humans using LLMs / BeHumans\n","Cloud-based ETL pipeline with Databricks & Azure / Minsait\n","\n","[CV]: Project Manager\n","Digittude Technology Solutions SL\n","January 2024 - August 2024 (Madrid, Spain)\n","CLIENT: BE HUMANS\n","Led Generative AI initiatives for the development and deployment of Digital Human (DH) solutions for various clients.\n","Utilized the platform's API to technically configure and customize Digital Humans, adapting their functionalities to diverse client use cases....\n","\n","❓ Query: What type of job am I looking for?\n","📄 Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between €60,000 and €65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: . An AI & Data Engineer with experience in Generative AI, cloud platforms (Azure, GCP), and ELT processes. Skilled in project management, APIs, and agile methodologies with a strong analytical mindset, a fast learner, and results-oriented.\n","\n","[CV]: Passionate about Artificial Intelligence and Data Engineering, with experience in developing and deploying robust backend systems and cutting-edge solutions in Generative AI, Cloud Computing, and Big Data. Throughout my career at companies like Digittude Technology and Minsait, I have contributed to projects with advanced AI architectures like Retrieval-Augmented Generation (RAG) and AI agent-based systems, using orchestration frameworks such as Semantic Kernel and LangChain...\n","\n","✅ RAG Knowledge Base ready for AI Assistant!\n"]}],"source":["# Setup embeddings and process documents\n","print(\"🔧 Setting up RAG Knowledge Base...\")\n","rag_kb.setup_embeddings()\n","\n","# Load and process documents\n","if cv_content and expectations_content:\n","    rag_kb.load_and_process_documents(cv_content, expectations_content)\n","    rag_kb.create_vectorstore()\n","\n","    print(\"\\n🧪 Testing retrieval system...\")\n","\n","    # Test retrieval with sample queries\n","    test_queries = [\n","        \"What are my technical skills in data engineering?\",\n","        \"What technologies do I work with?\",\n","        \"What type of job am I looking for?\"\n","    ]\n","\n","    for query in test_queries:\n","        print(f\"\\n❓ Query: {query}\")\n","        context = rag_kb.get_context_string(query, k=2)\n","        # print(f\"📄 Retrieved context (first 200 chars): {context[:200]}...\")\n","        print(f\"📄 Retrieved context: \\n{context}...\")\n","\n","    print(\"\\n✅ RAG Knowledge Base ready for AI Assistant!\")\n","\n","    # # Initialize AI Recruiter Assistant with Natural Response Generation\n","    # ai_assistant = AIRecruiterAssistantNatural(\n","    #     model_name=selected_model,\n","    #     rag_knowledge_base=rag_kb,\n","    #     input_guardrail=input_guardrail,\n","    #     cache_path=cache_path\n","    # )\n","\n","    # print(\"\\n🤖 AI Recruiter Assistant with Natural Responses initialized!\")\n","    # print(\"🛡️ Input Guardrail: ✅ Natural response generation\")\n","    # print(\"🧠 RAG Knowledge Base: ✅ Ready\")\n","    # print(\"📊 Match Scoring System: ✅ Natural analysis\")\n","    # print(\"🎯 State Management: ✅ Natural responses\")\n","    # print(\"🚀 Ready for natural response testing!\")\n","\n","else:\n","    print(\"❌ CV or expectations content missing. Check data loading.\")\n"]},{"cell_type":"markdown","metadata":{"id":"p_er1dHPjShl"},"source":["## ***PHASE 3:*** Input Guardrail System\n","\n","Implementing an intelligent guardrail that performs **Intent Detection** to classify recruiter messages as either:\n","- **Generic messages** → State: \"pending_details\" → Request more information\n","- **Concrete job offers** → Pass to RAG system for analysis\n"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1753663033811,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"jpBOjD4oZMqo"},"outputs":[],"source":["from enum import Enum\n","from dataclasses import dataclass\n","from typing import Dict, Any, Tuple\n","\n","class MessageType(Enum):\n","    GENERIC = \"generic\"\n","    CONCRETE_OFFER = \"concrete_offer\"\n","\n","class GenericSubType(Enum):\n","    BASIC_INTRODUCTION = \"basic_introduction\"\n","    OPPORTUNITY_INQUIRY = \"opportunity_inquiry\"\n","\n","class ConversationState(Enum):\n","    PENDING_DETAILS = \"pending_details\"\n","    ANALYZING = \"analyzing\"\n","    PASSED = \"passed\"\n","    STAND_BY = \"stand_by\"\n","    FINISHED = \"finished\"\n","\n","@dataclass\n","class GuardrailResult:\n","    message_type: MessageType\n","    confidence: float\n","    state: ConversationState\n","    language: str = \"English\"\n","    generic_subtype: GenericSubType = None\n","    response: str = None\n","    should_continue_to_rag: bool = False"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1753663033861,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"EmU8lYjrVvx2","outputId":"1f5c337a-3dd6-4d55-e1d1-7eae3e375b32"},"outputs":[{"output_type":"stream","name":"stdout","text":["🛡️ Updated Input Guardrail system initialized!\n","📊 Message classification: GENERIC vs CONCRETE_OFFER\n","🎯 Natural response generation: BASIC_INTRODUCTION (brief) vs OPPORTUNITY_INQUIRY (detailed)\n","🌍 Language detection and natural responses\n","⚡ Using Phi-3-mini for classification and response generation\n"]}],"source":["# 🛡️ UPDATED INPUT GUARDRAIL WITH NATURAL RESPONSE GENERATION\n","# This replaces the previous InputGuardrail with natural response generation instead of templates\n","\n","class InputGuardrailNatural:\n","    \"\"\"\n","    Updated InputGuardrail that generates natural responses using the model\n","    instead of using fixed templates\n","    \"\"\"\n","\n","    def __init__(self, cache_path: str):\n","        self.cache_path = cache_path\n","        self.model_name = input_guardrail_model_name\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Load prompts from external files\n","        import sys\n","        import os\n","        import importlib\n","        sys.path.append(f\"{project_path}/prompts/app\")\n","\n","        # Reload prompt_loader module to get latest changes\n","        if 'prompt_loader' in sys.modules:\n","            importlib.reload(sys.modules['prompt_loader'])\n","\n","        from prompt_loader import PromptLoader\n","\n","        prompt_loader = PromptLoader(f\"{project_path}/prompts/app\")\n","        input_prompts = prompt_loader.load_input_guardrail_prompts()\n","\n","        self.classification_prompt = input_prompts[\"classification_prompt\"]\n","        self.generic_response_template = input_prompts[\"generic_response_template\"]\n","        self.basic_intro_prompt = input_prompts[\"basic_intro_prompt\"]\n","        self.opportunity_inquiry_prompt = input_prompts[\"opportunity_inquiry_prompt\"]\n","\n","    def load_model(self):\n","        \"\"\"Load Phi-3-mini model for fast classification\"\"\"\n","        print(f\"\\n🛡️ Loading guardrail model: {self.model_name}\")\n","\n","        # Check cache\n","        cached_models = check_cached_models(self.cache_path)\n","        model_folder_space = self.model_name.replace('/', ' ')\n","\n","        if model_folder_space in cached_models:\n","            print(\"⚡ Loading from cache...\")\n","            model_cache_dir = os.path.join(self.cache_path, model_folder_space)\n","        else:\n","            print(\"📥 Downloading model...\")\n","            model_cache_dir = create_model_cache_dir(self.model_name, self.cache_path)\n","\n","        # Quantization for efficiency\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(\n","                self.model_name,\n","                cache_dir=model_cache_dir\n","            )\n","            if self.tokenizer.pad_token is None:\n","                self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                self.model_name,\n","                quantization_config=bnb_config,\n","                device_map=\"auto\",\n","                trust_remote_code=True,\n","                cache_dir=model_cache_dir,\n","                attn_implementation=\"eager\"  # Phi-3 optimization\n","            )\n","\n","            print(\"✅ Guardrail model loaded successfully!\")\n","            return self.model, self.tokenizer\n","\n","        except Exception as e:\n","            print(f\"❌ Error loading guardrail model: {str(e)}\")\n","            raise\n","\n","    def classify_message(self, recruiter_message: str) -> Tuple[MessageType, float, str, GenericSubType]:\n","        \"\"\"Classify recruiter message and detect language\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Guardrail model not loaded. Call load_model() first.\")\n","\n","        # Format classification prompt\n","        prompt = self.classification_prompt.format(message=recruiter_message.strip())\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=80,\n","                    do_sample=False,\n","                    temperature=0.1,\n","                    pad_token_id=self.tokenizer.eos_token_id,\n","                    use_cache=False\n","                )\n","\n","            # Decode response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            classification_response = full_response.replace(prompt, \"\").strip()\n","\n","            # Parse classification result with new fields\n","            message_type, confidence, language, generic_subtype = self._parse_classification(classification_response)\n","\n","            return message_type, confidence, language, generic_subtype\n","\n","        except Exception as e:\n","            print(f\"⚠️ Classification error: {str(e)}\")\n","            # Fallback: basic keyword detection\n","            return self._fallback_classification(recruiter_message)\n","\n","    def _parse_classification(self, response: str) -> Tuple[MessageType, float, str, GenericSubType]:\n","        \"\"\"Parse model classification response including language and sub-classification\"\"\"\n","        import re\n","        response_lower = response.lower()\n","\n","        # Extract language\n","        language = \"English\"  # Default\n","        language_match = re.search(r'language:\\s*([^\\n\\r]+)', response_lower)\n","        if language_match:\n","            language = language_match.group(1).strip().title()\n","\n","        # Extract classification\n","        if \"concrete_offer\" in response_lower or \"concrete offer\" in response_lower:\n","            message_type = MessageType.CONCRETE_OFFER\n","            generic_subtype = None\n","        elif \"generic\" in response_lower:\n","            message_type = MessageType.GENERIC\n","\n","            # Extract sub-classification for generic messages\n","            generic_subtype = GenericSubType.OPPORTUNITY_INQUIRY  # Default\n","            if \"basic_introduction\" in response_lower or \"introduction\" in response_lower:\n","                generic_subtype = GenericSubType.BASIC_INTRODUCTION\n","            elif \"opportunity_inquiry\" in response_lower or \"opportunity\" in response_lower:\n","                generic_subtype = GenericSubType.OPPORTUNITY_INQUIRY\n","        else:\n","            # Fallback based on response content\n","            if any(keyword in response_lower for keyword in [\"specific\", \"detailed\", \"role\", \"position\", \"salary\"]):\n","                message_type = MessageType.CONCRETE_OFFER\n","                generic_subtype = None\n","            else:\n","                message_type = MessageType.GENERIC\n","                generic_subtype = GenericSubType.OPPORTUNITY_INQUIRY\n","\n","        # Extract confidence\n","        confidence = 0.8  # Default confidence\n","        try:\n","            confidence_match = re.search(r'confidence:\\s*([0-9.]+)', response_lower)\n","            if confidence_match:\n","                confidence = float(confidence_match.group(1))\n","                confidence = max(0.0, min(1.0, confidence))  # Clamp to [0,1]\n","        except:\n","            pass\n","\n","        return message_type, confidence, language, generic_subtype\n","\n","    def _fallback_classification(self, message: str) -> Tuple[MessageType, float, str, GenericSubType]:\n","        \"\"\"Fallback classification using keyword detection\"\"\"\n","        message_lower = message.lower()\n","\n","        # Simple language detection (basic heuristics)\n","        language = \"English\"  # Default\n","        if any(word in message_lower for word in [\"hola\", \"gracias\", \"trabajo\", \"oportunidad\"]):\n","            language = \"Spanish\"\n","        elif any(word in message_lower for word in [\"bonjour\", \"merci\", \"travail\", \"opportunité\"]):\n","            language = \"French\"\n","        elif any(word in message_lower for word in [\"hallo\", \"danke\", \"arbeit\", \"gelegenheit\"]):\n","            language = \"German\"\n","\n","        # Keywords indicating concrete offers\n","        concrete_keywords = [\n","            \"position\", \"role\", \"job\", \"salary\", \"€\", \"$\", \"£\", \"experience\",\n","            \"requirements\", \"responsibilities\", \"company\", \"team\", \"technologies\",\n","            \"remote\", \"on-site\", \"hybrid\", \"years\", \"senior\", \"junior\", \"developer\",\n","            \"engineer\", \"analyst\", \"manager\", \"python\", \"java\", \"javascript\",\n","            \"cloud\",\"aws\", \"azure\", \"gcp\", \"react\", \"angular\", \"node\", \"sql\", \"nosql\",\n","            \"data\", \"ai\"\n","        ]\n","\n","        # Keywords indicating generic messages\n","        generic_keywords = [\n","            \"open to\", \"opportunities\", \"connect\", \"network\", \"interested in\",\n","            \"catch up\", \"chat\", \"discuss\", \"explore\", \"available\", \"looking for\"\n","        ]\n","\n","        # Keywords for basic introductions\n","        intro_keywords = [\n","            \"my name is\", \"i am\", \"hello\", \"hi\", \"how are you\", \"nice to meet\",\n","            \"greetings\", \"good morning\", \"good afternoon\"\n","        ]\n","\n","        concrete_score = sum(1 for keyword in concrete_keywords if keyword in message_lower)\n","        generic_score = sum(1 for keyword in generic_keywords if keyword in message_lower)\n","        intro_score = sum(1 for keyword in intro_keywords if keyword in message_lower)\n","\n","        if concrete_score > generic_score and concrete_score >= 2:\n","            return MessageType.CONCRETE_OFFER, 0.7, language, None\n","        else:\n","            # Determine sub-type for generic messages\n","            if intro_score > 0 or len(message.strip().split()) < 8:  # Very short messages likely introductions\n","                subtype = GenericSubType.BASIC_INTRODUCTION\n","            else:\n","                subtype = GenericSubType.OPPORTUNITY_INQUIRY\n","            return MessageType.GENERIC, 0.6, language, subtype\n","\n","    def generate_natural_response(self, message: str, language: str, subtype: GenericSubType) -> str:\n","        \"\"\"Generate natural response using the model instead of templates\"\"\"\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Guardrail model not loaded. Call load_model() first.\")\n","\n","        # Choose appropriate prompt based on subtype\n","        if subtype == GenericSubType.BASIC_INTRODUCTION:\n","            prompt = self.basic_intro_prompt.format(message=message, language=language)\n","        else:  # OPPORTUNITY_INQUIRY\n","            prompt = self.opportunity_inquiry_prompt.format(\n","                message=message,\n","                language=language,\n","                generic_template=self.generic_response_template\n","            )\n","\n","        try:\n","            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=150,  # Enough for natural responses\n","                    do_sample=True,\n","                    temperature=0.7,  # More creative for natural responses\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id,\n","                    use_cache=False\n","                )\n","\n","            # Decode and clean response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            response = full_response.replace(prompt, \"\").strip()\n","\n","            return response\n","\n","        except Exception as e:\n","            print(f\"⚠️ Error generating natural response: {str(e)}\")\n","            # Fallback to basic response\n","            if subtype == GenericSubType.BASIC_INTRODUCTION:\n","                return \"Hello! Thanks for reaching out. Best regards.\"\n","            else:\n","                return self.generic_response_template\n","\n","    def process_message(self, recruiter_message: str) -> GuardrailResult:\n","        \"\"\"Process recruiter message through the guardrail system\"\"\"\n","\n","        print(\"🛡️ Processing message through guardrail...\")\n","\n","        # Step 1: Classify message type with language and sub-classification\n","        message_type, confidence, language, generic_subtype = self.classify_message(recruiter_message)\n","\n","        print(f\"   📊 Classification: {message_type.value} (confidence: {confidence:.2f})\")\n","        print(f\"   🌍 Language: {language}\")\n","        if generic_subtype:\n","            print(f\"   🔍 Sub-type: {generic_subtype.value}\")\n","\n","        # Step 2: Handle based on classification\n","        if message_type == MessageType.GENERIC:\n","            # Generate natural response using the model\n","            print(f\"   💬 Generating natural response for {generic_subtype.value} in {language}\")\n","            response = self.generate_natural_response(recruiter_message, language, generic_subtype)\n","\n","            return GuardrailResult(\n","                message_type=message_type,\n","                confidence=confidence,\n","                language=language,\n","                generic_subtype=generic_subtype,\n","                state=ConversationState.PENDING_DETAILS,\n","                response=response,\n","                should_continue_to_rag=False\n","            )\n","\n","        else:  # CONCRETE_OFFER\n","            # Concrete offer: pass to RAG system\n","            return GuardrailResult(\n","                message_type=message_type,\n","                confidence=confidence,\n","                language=language,\n","                generic_subtype=None,\n","                state=ConversationState.ANALYZING,\n","                response=None,\n","                should_continue_to_rag=True\n","            )\n","\n","# Initialize Updated Input Guardrail\n","input_guardrail = InputGuardrailNatural(cache_path=cache_path)\n","\n","print(\"🛡️ Updated Input Guardrail system initialized!\")\n","print(\"📊 Message classification: GENERIC vs CONCRETE_OFFER\")\n","print(\"🎯 Natural response generation: BASIC_INTRODUCTION (brief) vs OPPORTUNITY_INQUIRY (detailed)\")\n","print(\"🌍 Language detection and natural responses\")\n","print(\"⚡ Using Phi-3-mini for classification and response generation\")\n"]},{"cell_type":"markdown","metadata":{"id":"VHTP08eG-zZm"},"source":["## ***PHASE 4:*** 🛡️ OUTPUT GUARDRAIL SYSTEM\n","Validates and improves response naturalness"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1753663033910,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"xAvoOvVW-zZm","outputId":"0a0c4490-8d96-471b-fbf1-cecf78073548"},"outputs":[{"output_type":"stream","name":"stdout","text":["🛡️ Output Guardrail system initialized!\n","📝 Response validation: First person usage, no placeholders, natural tone\n","🔄 Iterative improvement: Up to k=5 iterations\n","🎯 Fallback generation: Guardrail creates corrected version if needed\n","⚡ Using microsoft/Phi-3-mini-4k-instruct for validation and correction\n","💡 Alternative models available: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\n"]}],"source":["from typing import List, Dict, Tuple\n","import re\n","\n","class OutputGuardrail:\n","    \"\"\"\n","    Output guardrail that validates response naturalness and ensures first-person perspective.\n","    \"\"\"\n","\n","    def __init__(self, cache_path: str):\n","        self.cache_path = cache_path\n","        self.model_name = output_guardrail_model_name #\"meta-llama/Meta-Llama-3-8B-Instruct\"\n","        # Alternative models for consideration:\n","        # self.model_name = \"google/gemma-3-4b-it\"  # Alternative option 1\n","        # self.model_name = \"microsoft/Phi-3-mini-4k-instruct\"  # Alternative option 2 (but already used in input guardrail)\n","\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Load prompts from external files\n","        import sys\n","        import os\n","        sys.path.append(f\"{project_path}/prompts/app\")\n","        from prompt_loader import PromptLoader\n","\n","        prompt_loader = PromptLoader(f\"{project_path}/prompts/app\")\n","        output_prompts = prompt_loader.load_output_guardrail_prompts()\n","\n","        self.validation_prompt = output_prompts[\"validation_prompt\"]\n","        self.correction_prompt = output_prompts[\"correction_prompt\"]\n","\n","    def load_model(self):\n","        \"\"\"Load Meta-Llama-3-8B-Instruct model for validation and correction\"\"\"\n","        print(f\"\\n🛡️ Loading output guardrail model: {self.model_name}\")\n","\n","        # Check cache\n","        cached_models = check_cached_models(self.cache_path)\n","        model_folder_space = self.model_name.replace('/', ' ')\n","\n","        if model_folder_space in cached_models:\n","            print(\"⚡ Loading from cache...\")\n","            model_cache_dir = os.path.join(self.cache_path, model_folder_space)\n","        else:\n","            print(\"📥 Downloading model...\")\n","            model_cache_dir = create_model_cache_dir(self.model_name, self.cache_path)\n","\n","        # Quantization for efficiency\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(\n","                self.model_name,\n","                cache_dir=model_cache_dir\n","            )\n","            if self.tokenizer.pad_token is None:\n","                self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                self.model_name,\n","                quantization_config=bnb_config,\n","                device_map=\"auto\",\n","                trust_remote_code=True,\n","                cache_dir=model_cache_dir\n","            )\n","\n","            print(\"✅ Output guardrail model loaded successfully!\")\n","            return self.model, self.tokenizer\n","\n","        except Exception as e:\n","            print(f\"❌ Error loading output guardrail model: {str(e)}\")\n","            raise\n","\n","    def validate_response(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Validate response for naturalness and first-person perspective\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            # Fallback to rule-based validation if model not loaded\n","            return self._rule_based_validation(response)\n","\n","        prompt = self.validation_prompt.format(response=response.strip())\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=100,\n","                    do_sample=False,\n","                    temperature=0.1,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and parse validation result\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            validation_response = full_response.replace(prompt, \"\").strip()\n","\n","            return self._parse_validation_result(validation_response)\n","\n","        except Exception as e:\n","            print(f\"⚠️ Validation error: {str(e)}. Using fallback validation.\")\n","            return self._rule_based_validation(response)\n","\n","    def _rule_based_validation(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Fallback rule-based validation\"\"\"\n","        issues = []\n","\n","        # Check for third person references\n","        third_person_patterns = [\n","            r'\\bthe candidate\\b',\n","            r'\\bcandidate\\'s\\b',\n","            r'\\bhis/her\\b',\n","            r'\\bhis or her\\b',\n","            r'\\bhis\\b(?!\\s+name)',  # \"his\" but not \"his name\"\n","            r'\\bher\\b(?!\\s+email)', # \"her\" but not \"her email\"\n","        ]\n","\n","        for pattern in third_person_patterns:\n","            if re.search(pattern, response, re.IGNORECASE):\n","                issues.append(f\"Uses third person reference: '{pattern}'\")\n","\n","        # Check for placeholders\n","        placeholder_patterns = [\n","            r'\\[recruiter name\\]',\n","            r'\\[name\\]',\n","            r'\\[.*?\\]',  # Any text in square brackets\n","        ]\n","\n","        for pattern in placeholder_patterns:\n","            if re.search(pattern, response, re.IGNORECASE):\n","                issues.append(f\"Contains placeholder: '{pattern}'\")\n","\n","        return len(issues) == 0, issues\n","\n","    def _parse_validation_result(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Parse model validation response\"\"\"\n","        response_lower = response.lower()\n","\n","        # Extract validation status\n","        is_valid = \"validation: pass\" in response_lower or \"pass\" in response_lower\n","\n","        # Extract issues\n","        issues = []\n","        if not is_valid:\n","            # Try to extract issues section\n","            issues_match = re.search(r'issues:\\s*(.+?)(?=\\n|$)', response, re.IGNORECASE | re.DOTALL)\n","            if issues_match:\n","                issues_text = issues_match.group(1).strip()\n","                if issues_text.lower() != \"none\":\n","                    # Split by common delimiters\n","                    issues = [issue.strip() for issue in re.split(r'[,;-]|\\n', issues_text) if issue.strip()]\n","\n","            # Fallback: use rule-based validation\n","            if not issues:\n","                _, issues = self._rule_based_validation(response)\n","\n","        return is_valid, issues\n","\n","    def correct_response(self, original_response: str, issues: List[str], recruiter_message: str) -> str:\n","        \"\"\"Generate corrected response using the model\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            return self._rule_based_correction(original_response)\n","\n","        issues_text = \"; \".join(issues) if issues else \"General naturalness improvements needed\"\n","\n","        prompt = self.correction_prompt.format(\n","            original_response=original_response,\n","            issues=issues_text,\n","            recruiter_message=recruiter_message\n","        )\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=600,\n","                    do_sample=True,\n","                    temperature=0.5,\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode corrected response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            corrected_response = full_response.replace(prompt, \"\").strip()\n","\n","            return corrected_response\n","\n","        except Exception as e:\n","            print(f\"⚠️ Correction error: {str(e)}. Using fallback correction.\")\n","            return self._rule_based_correction(original_response)\n","\n","    def _rule_based_correction(self, response: str) -> str:\n","        \"\"\"Fallback rule-based correction\"\"\"\n","        corrected = response.strip()\n","\n","        # Remove unwanted prefixes that shouldn't appear in final responses\n","        unwanted_prefixes = [\n","            r'^RESPONSE:\\s*',\n","            r'^Response:\\s*',\n","            r'^Cristopher:\\s*',\n","            r'^[A-Za-z]+:\\s*',  # Any \"Name:\" pattern at start\n","        ]\n","\n","        for prefix in unwanted_prefixes:\n","            corrected = re.sub(prefix, '', corrected, flags=re.IGNORECASE | re.MULTILINE)\n","\n","        # Fix third person references\n","        corrections = [\n","            (r'\\bthe candidate\\'s\\b', 'my'),\n","            (r'\\bthe candidate\\b', 'I'),\n","            (r'\\bcandidate\\'s\\b', 'my'),\n","            (r'\\bhis/her\\b', 'my'),\n","            (r'\\bhis or her\\b', 'my'),\n","        ]\n","\n","        for pattern, replacement in corrections:\n","            corrected = re.sub(pattern, replacement, corrected, flags=re.IGNORECASE)\n","\n","        # Remove ALL types of placeholders more aggressively\n","        placeholder_patterns = [\n","            r'\\[recruiter name\\]',\n","            r'\\[name\\]',\n","            r'\\[recruiter\\]',\n","            r'\\[[^\\]]*\\]',  # Any text in brackets\n","            r'Dear \\[.*?\\],?',  # \"Dear [...],\" patterns\n","            r'Hello \\[.*?\\],?',  # \"Hello [...],\" patterns\n","        ]\n","\n","        for pattern in placeholder_patterns:\n","            corrected = re.sub(pattern, '', corrected, flags=re.IGNORECASE)\n","\n","        # Clean up multiple spaces but preserve line breaks\n","        corrected = re.sub(r' +', ' ', corrected)\n","\n","        # Clean up multiple newlines but preserve single line breaks\n","        corrected = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', corrected)\n","\n","        # Cut everything after final signature to remove unwanted instructions/metadata\n","        # Multiple patterns to catch different signature formats and instruction leaks\n","        cutoff_patterns = [\n","            r'(.*?Best regards,\\s*Cristopher).*',  # Standard signature\n","            r'(.*?Cristopher)(?:\\s*\\n\\s*INSTRUCTIONS.*)',  # Instructions leak\n","            r'(.*?Cristopher)(?:\\s*\\n\\s*IF MATCH SCORE.*)',  # Prompt leak\n","            r'(.*?Cristopher)(?:\\s*\\n\\s*Generate a natural.*)',  # Generation instruction leak\n","            r'(.*?,\\s*Cristopher).*',  # Any comma + Cristopher format\n","        ]\n","\n","        for pattern in cutoff_patterns:\n","            match = re.search(pattern, corrected, re.DOTALL | re.IGNORECASE)\n","            if match:\n","                corrected = match.group(1).strip()\n","                break\n","\n","        # Remove leading/trailing whitespace from each line while preserving structure\n","        lines = corrected.split('\\n')\n","        cleaned_lines = [line.strip() for line in lines if line.strip()]\n","        corrected = '\\n'.join(cleaned_lines)\n","\n","        return corrected.strip()\n","\n","    def validate_and_improve_response(self, original_response: str, recruiter_message: str, max_iterations: int = 5) -> str:\n","        \"\"\"Main method: validate and iteratively improve response naturalness\"\"\"\n","\n","        print(f\"   🔍 Validating response naturalness...\")\n","\n","        current_response = original_response\n","        iteration = 0\n","        validation_errors = 0\n","\n","        while iteration < max_iterations:\n","            iteration += 1\n","            print(f\"      🔄 Iteration {iteration}/{max_iterations}\")\n","\n","            # Validate current response\n","            is_valid, issues = self.validate_response(current_response)\n","\n","            if is_valid:\n","                print(f\"      ✅ Response passed validation on iteration {iteration}\")\n","                return current_response\n","\n","            print(f\"      ⚠️ Issues found: {'; '.join(issues)}\")\n","\n","            # If validation keeps failing due to model errors, use rule-based correction\n","            if any(\"DynamicCache\" in str(issue) or \"get_max_length\" in str(issue) for issue in issues):\n","                validation_errors += 1\n","                if validation_errors >= 2:\n","                    print(f\"      🔧 Multiple validation errors detected. Using rule-based correction...\")\n","                    return self._rule_based_correction(current_response)\n","\n","            # Correct the response\n","            current_response = self.correct_response(current_response, issues, recruiter_message)\n","\n","        # If we reach here, we've exhausted max_iterations\n","        print(f\"      🚨 Max iterations ({max_iterations}) reached. Using rule-based correction...\")\n","\n","        # Use rule-based correction as final fallback\n","        final_response = self._rule_based_correction(original_response)\n","\n","        return final_response\n","\n","# Initialize Output Guardrail\n","output_guardrail = OutputGuardrail(cache_path=cache_path)\n","\n","print(\"🛡️ Output Guardrail system initialized!\")\n","print(\"📝 Response validation: First person usage, no placeholders, natural tone\")\n","print(\"🔄 Iterative improvement: Up to k=5 iterations\")\n","print(\"🎯 Fallback generation: Guardrail creates corrected version if needed\")\n","print(f\"⚡ Using {output_guardrail_model_name} for validation and correction\")\n","print(\"💡 Alternative models available: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\")\n"]},{"cell_type":"markdown","metadata":{"id":"1PXmGeOhjShl"},"source":["## ***PHASE 5:*** AI Assistant with Guardrail Integration\n","\n","Integrating the input guardrail with the RAG system and implementing the complete business logic including match scoring and state management.\n"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1753663033964,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"oJnkJMusgqNg","outputId":"3faf2383-0ba2-4880-f746-c2d0784bcd6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ CORRECTED AIRecruiterAssistantNatural class ready!\n","🔧 Fixed issues:\n","   • OutputGuardrail constructor: only takes cache_path\n","   • Method references: self.input_guardrail instead of self.guardrail\n","   • Constructor parameters: removed unnecessary output_guardrail param\n","🚀 Ready to create ai_assistant object!\n"]}],"source":["# 🔧 CORRECTED AI RECRUITER ASSISTANT NATURAL - FIXING CONSTRUCTOR ERRORS\n","# This fixes the OutputGuardrail constructor error and method references\n","\n","class AIRecruiterAssistantNatural:\n","    \"\"\"\n","    Corrected AI Assistant that generates natural responses for concrete offers\n","    instead of using fixed templates\n","    \"\"\"\n","\n","    def __init__(self, model_name: str, rag_knowledge_base: RAGKnowledgeBase,\n","                 input_guardrail: InputGuardrailNatural, cache_path: str):\n","        self.model_name = model_name\n","        self.rag_kb = rag_knowledge_base\n","        self.input_guardrail = input_guardrail  # Correct reference\n","        self.cache_path = cache_path\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Load prompts for natural response generation\n","        import sys\n","        import os\n","        import importlib\n","        sys.path.append(f\"{project_path}/prompts/app\")\n","\n","        # Reload prompt_loader module to get latest changes\n","        if 'prompt_loader' in sys.modules:\n","            importlib.reload(sys.modules['prompt_loader'])\n","\n","        from prompt_loader import PromptLoader\n","\n","        prompt_loader = PromptLoader(f\"{project_path}/prompts/app\")\n","        main_prompts = prompt_loader.load_main_generator_prompts()\n","\n","        self.match_scoring_prompt = main_prompts[\"match_scoring_prompt\"]\n","        self.natural_response_prompt = main_prompts[\"natural_response_prompt\"]\n","\n","        # Initialize output guardrail - FIXED: only pass cache_path\n","        self.output_guardrail = OutputGuardrail(cache_path=cache_path)\n","\n","    def load_models(self):\n","        \"\"\"Load both guardrail and main model\"\"\"\n","        print(\"🔧 Loading models...\")\n","\n","        # Load input guardrail model first\n","        self.input_guardrail.load_model()\n","\n","        # Load output guardrail model\n","        self.output_guardrail.load_model()\n","\n","        # Load main model using benchmark's logic\n","        print(f\"📥 Loading main model: {self.model_name}\")\n","        benchmark = CacheAwareModelBenchmark()\n","        self.model, self.tokenizer = benchmark.load_model_with_quantization(self.model_name)\n","\n","        print(\"✅ All models loaded successfully!\")\n","\n","    def calculate_match_score(self, recruiter_message: str) -> Dict[str, Any]:\n","        \"\"\"Calculate match score between job offer and profile using RAG\"\"\"\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Main model not loaded. Call load_models() first.\")\n","\n","        print(\"📊 Calculating match score...\")\n","\n","        try:\n","            # Get relevant context from RAG\n","            context = self.rag_kb.get_context_string(recruiter_message, k=3)\n","\n","            # Format scoring prompt\n","            scoring_prompt = self.match_scoring_prompt.format(\n","                context=context,\n","                job_offer=recruiter_message\n","            )\n","\n","            inputs = self.tokenizer(scoring_prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=300,\n","                    do_sample=True,\n","                    temperature=0.3,\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and parse scoring response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            scoring_response = full_response.replace(scoring_prompt, \"\").strip()\n","\n","            # Parse the scoring response\n","            match_data = self._parse_match_score(scoring_response)\n","            match_data[\"context_used\"] = context\n","\n","            return match_data\n","\n","        except Exception as e:\n","            print(f\"⚠️ Error calculating match score: {str(e)}\")\n","            # Fallback scoring based on keywords\n","            return self._fallback_match_scoring(recruiter_message)\n","\n","    def _parse_match_score(self, response: str) -> Dict[str, Any]:\n","        \"\"\"Parse match scoring response from the model\"\"\"\n","        import re\n","\n","        # Extract overall match score\n","        match_score = 50  # Default fallback\n","        match_pattern = re.search(r'MATCH_SCORE:\\s*(\\d+)', response)\n","        if match_pattern:\n","            match_score = int(match_pattern.group(1))\n","            match_score = max(0, min(100, match_score))  # Clamp to [0,100]\n","\n","        # Extract component scores\n","        components = {}\n","        component_patterns = {\n","            \"technical_skills\": r'TECHNICAL_SKILLS:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"role_type\": r'ROLE_TYPE:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"salary\": r'SALARY:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"work_arrangement\": r'WORK_ARRANGEMENT:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"experience\": r'EXPERIENCE:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)'\n","        }\n","\n","        for component, pattern in component_patterns.items():\n","            match = re.search(pattern, response, re.IGNORECASE)\n","            if match:\n","                score = int(match.group(1))\n","                reason = match.group(2).strip()\n","                components[component] = {\"score\": score, \"reason\": reason}\n","\n","        # Extract overall reasoning\n","        reasoning_pattern = re.search(r'OVERALL_REASONING:\\s*(.+?)(?=\\n\\n|$)', response, re.DOTALL)\n","        overall_reasoning = reasoning_pattern.group(1).strip() if reasoning_pattern else \"Good potential fit based on available information.\"\n","\n","        return {\n","            \"match_score\": match_score,\n","            \"components\": components,\n","            \"overall_reasoning\": overall_reasoning,\n","            \"raw_response\": response\n","        }\n","\n","    def _fallback_match_scoring(self, message: str) -> Dict[str, Any]:\n","        \"\"\"Fallback match scoring using keyword analysis\"\"\"\n","        message_lower = message.lower()\n","\n","        # Simple keyword-based scoring\n","        score = 50  # Base score\n","\n","        # Technical skills boost\n","        tech_keywords = [\"python\", \"ai\", \"data\", \"engineering\", \"cloud\", \"aws\", \"azure\", \"gcp\", \"langchain\", \"rag\"]\n","        tech_matches = sum(1 for keyword in tech_keywords if keyword in message_lower)\n","        score += min(tech_matches * 5, 25)\n","\n","        # Salary analysis\n","        if any(sal in message_lower for sal in [\"60k\", \"65k\", \"70k\", \"€60\", \"€65\", \"€70\"]):\n","            score += 15\n","        elif any(sal in message_lower for sal in [\"80k\", \"90k\", \"€80\", \"€90\"]):\n","            score += 10\n","\n","        # Remote work boost\n","        if \"remote\" in message_lower:\n","            score += 10\n","\n","        return {\n","            \"match_score\": min(score, 100),\n","            \"components\": {},\n","            \"overall_reasoning\": \"Fallback scoring based on keyword analysis\",\n","            \"raw_response\": \"Fallback analysis used\"\n","        }\n","\n","    def generate_natural_response_for_match(self, match_data: Dict[str, Any], recruiter_message: str, language: str) -> Tuple[str, ConversationState]:\n","        \"\"\"Generate natural response based on match score instead of using templates\"\"\"\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Main model not loaded. Call load_models() first.\")\n","\n","        match_score = match_data[\"match_score\"]\n","        print(f\"   🎯 Match Score: {match_score}%\")\n","\n","        # Determine state based on score\n","        if match_score > 80:\n","            state = ConversationState.PASSED\n","        elif match_score >= 60:\n","            state = ConversationState.STAND_BY\n","        else:\n","            state = ConversationState.FINISHED\n","\n","        # Format prompt for natural response generation\n","        prompt = self.natural_response_prompt.format(\n","            match_score=match_score,\n","            match_analysis=match_data[\"overall_reasoning\"],\n","            recruiter_message=recruiter_message,\n","            language=language\n","        )\n","\n","        try:\n","            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=600,  # Increased to prevent text truncation\n","                    do_sample=True,\n","                    temperature=0.7,  # Creative but controlled\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and clean response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            response = full_response.replace(prompt, \"\").strip()\n","\n","            return response, state\n","\n","        except Exception as e:\n","            print(f\"⚠️ Error generating natural response: {str(e)}\")\n","            # Fallback based on score\n","            if match_score > 80:\n","                return \"Thank you for this opportunity! This looks like an excellent fit for my background. I'd love to discuss this further. When would be a good time for a call?\", state\n","            elif match_score >= 60:\n","                return \"Thanks for reaching out! This opportunity seems interesting. I'd like to review the details more thoroughly. Could you provide some additional information?\", state\n","            else:\n","                return \"Thank you for thinking of me. While this opportunity doesn't align perfectly with my current focus, I appreciate you reaching out.\", state\n","\n","    def process_recruiter_message(self, recruiter_message: str) -> Dict[str, Any]:\n","        \"\"\"Complete end-to-end processing of recruiter message\"\"\"\n","\n","        print(\"🚀 Processing recruiter message through complete system...\")\n","        print(\"=\"*60)\n","\n","        start_time = time.time()\n","\n","        # Step 1: Input Guardrail Processing - FIXED: correct reference\n","        guardrail_result = self.input_guardrail.process_message(recruiter_message)\n","\n","        if not guardrail_result.should_continue_to_rag:\n","            # Generic message: return guardrail response\n","            total_time = time.time() - start_time\n","            return {\n","                \"final_response\": guardrail_result.response,\n","                \"state\": guardrail_result.state,\n","                \"message_type\": guardrail_result.message_type,\n","                \"confidence\": guardrail_result.confidence,\n","                \"language\": guardrail_result.language,\n","                \"generic_subtype\": guardrail_result.generic_subtype.value if guardrail_result.generic_subtype else None,\n","                \"processing_time\": total_time,\n","                \"pipeline_stage\": \"guardrail_only\"\n","            }\n","\n","        # Step 2: RAG + Match Scoring for concrete offers\n","        print(\"🔍 Proceeding to RAG analysis...\")\n","        match_data = self.calculate_match_score(recruiter_message)\n","\n","        # Step 3: Generate natural response based on score\n","        final_response, final_state = self.generate_natural_response_for_match(\n","            match_data, recruiter_message, guardrail_result.language\n","        )\n","\n","        # Step 4: Output Guardrail Processing (for concrete offers that got responses)\n","        print(\"🛡️ Processing through output guardrail...\")\n","        final_response = self.output_guardrail.validate_and_improve_response(\n","            final_response, recruiter_message, max_iterations=5\n","        )\n","\n","        total_time = time.time() - start_time\n","\n","        return {\n","            \"final_response\": final_response,\n","            \"state\": final_state,\n","            \"message_type\": guardrail_result.message_type,\n","            \"confidence\": guardrail_result.confidence,\n","            \"language\": guardrail_result.language,\n","            \"match_score\": match_data[\"match_score\"],\n","            \"match_details\": match_data,\n","            \"processing_time\": total_time,\n","            \"pipeline_stage\": \"complete_rag_analysis\"\n","        }\n","\n","print(\"✅ CORRECTED AIRecruiterAssistantNatural class ready!\")\n","print(\"🔧 Fixed issues:\")\n","print(\"   • OutputGuardrail constructor: only takes cache_path\")\n","print(\"   • Method references: self.input_guardrail instead of self.guardrail\")\n","print(\"   • Constructor parameters: removed unnecessary output_guardrail param\")\n","print(\"🚀 Ready to create ai_assistant object!\")\n"]},{"cell_type":"markdown","metadata":{"id":"KZgia5doOoJJ"},"source":["### 🔧 CREATE AI ASSISTANT OBJECT WITH ALL COMPONENTS\n","Now that all components are initialized, create the complete AI assistant"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1753663033970,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"I6EV5Eq5gqNg","outputId":"494b7faa-ce6a-49a7-e8e4-132c962ad1eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔧 Creating corrected AI Assistant...\n","✅ AI Assistant created successfully!\n","🔧 Corrections applied:\n","   • OutputGuardrail constructor: only takes cache_path\n","   • Method references: self.input_guardrail instead of self.guardrail\n","   • Removed unnecessary output_guardrail parameter from constructor\n","   • Output guardrail is now initialized internally\n","\n","🤖 AI ASSISTANT READY!\n","✅ Model: mistralai/Mistral-7B-Instruct-v0.3\n","✅ Input Guardrail: microsoft/Phi-3-mini-4k-instruct\n","✅ Output Guardrail: microsoft/Phi-3-mini-4k-instruct\n","✅ RAG Knowledge Base: Ready\n","\n","🚀 HOW TO USE:\n","1. ai_assistant.load_models()  # Load all models\n","2. test_custom_message('Your message', 'Test Name')  # Test single message\n","3. run_complete_demo()  # Test all scenarios\n"]}],"source":["# 🔧 FINAL CORRECTION - CREATE AI ASSISTANT WITH CORRECT PARAMETERS\n","# This ensures ai_assistant is created with the corrected class and parameters\n","\n","print(\"🔧 Creating corrected AI Assistant...\")\n","\n","try:\n","    # Ensure we have all required components\n","    if 'rag_kb' not in globals():\n","        raise NameError(\"rag_kb not found. Run RAG setup cells first.\")\n","    if 'input_guardrail' not in globals():\n","        raise NameError(\"input_guardrail not found. Run InputGuardrailNatural setup first.\")\n","\n","    # Create AI Assistant with corrected parameters (no output_guardrail parameter)\n","    ai_assistant = AIRecruiterAssistantNatural(\n","        model_name=selected_text_generator_model, #selected_model,\n","        rag_knowledge_base=rag_kb,\n","        input_guardrail=input_guardrail,\n","        cache_path=cache_path\n","    )\n","\n","    print(\"✅ AI Assistant created successfully!\")\n","    print(\"🔧 Corrections applied:\")\n","    print(\"   • OutputGuardrail constructor: only takes cache_path\")\n","    print(\"   • Method references: self.input_guardrail instead of self.guardrail\")\n","    print(\"   • Removed unnecessary output_guardrail parameter from constructor\")\n","    print(\"   • Output guardrail is now initialized internally\")\n","\n","    print(f\"\\n🤖 AI ASSISTANT READY!\")\n","    print(f\"✅ Model: {ai_assistant.model_name}\")\n","    print(f\"✅ Input Guardrail: {ai_assistant.input_guardrail.model_name}\")\n","    print(f\"✅ Output Guardrail: {ai_assistant.output_guardrail.model_name}\")\n","    print(f\"✅ RAG Knowledge Base: Ready\")\n","\n","    print(f\"\\n🚀 HOW TO USE:\")\n","    print(f\"1. ai_assistant.load_models()  # Load all models\")\n","    print(f\"2. test_custom_message('Your message', 'Test Name')  # Test single message\")\n","    print(f\"3. run_complete_demo()  # Test all scenarios\")\n","\n","except Exception as e:\n","    print(f\"❌ Error creating AI Assistant: {str(e)}\")\n","    print(\"💡 Make sure to run all required setup cells first\")\n","    print(\"   • RAG Knowledge Base setup\")\n","    print(\"   • InputGuardrailNatural setup\")\n","    print(\"   • Model configuration\")\n"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753663034004,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"jMdfKYU37SHK","outputId":"e7cc5555-010a-45c1-8442-ba15fb9fbec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","📂 ENHANCED PROMPT ORGANIZATION - EXTERNAL FILE STRUCTURE\n","================================================================================\n","\n","🎯 PROMPT ORGANIZATION COMPLETED!\n","✅ All prompts moved from hardcoded to external file structure\n","✅ Following the complete process flow order:\n","\n","📁 prompts/app/ structure:\n","   📂 01_input_guardrail/\n","      📄 classification_prompt.txt\n","      📄 generic_response_template.txt\n","   📂 02_main_generator/\n","      📄 match_scoring_prompt.txt\n","      📂 response_templates/\n","         📄 passed_template.txt\n","         📄 stand_by_template.txt\n","         📄 finished_template.txt\n","   📂 03_output_guardrail/\n","      📄 validation_prompt.txt\n","      📄 correction_prompt.txt\n","   📄 prompt_loader.py\n","\n","🔧 MODIFIED CLASSES:\n","✅ InputGuardrail - Now loads prompts from 01_input_guardrail/\n","✅ OutputGuardrail - Now loads prompts from 03_output_guardrail/\n","✅ AIRecruiterAssistant - Now loads prompts from 02_main_generator/\n","\n","🧪 KEPT IN CODE (for testing only):\n","✅ benchmark.system_prompt - Used for model benchmarking tests\n","✅ benchmark.test_prompts - Used for testing scenarios\n","\n","💡 BENEFITS:\n","• 🎯 Clear separation of concerns - prompts organized by process flow\n","• 📝 Easy prompt editing without code changes\n","• 🔄 Version control for prompt improvements\n","• 🚀 Faster iteration on prompt engineering\n","• 🧹 Cleaner, more maintainable code\n","\n","================================================================================\n","🎉 STAGE 3 PROMPT ENGINEERING OPTIMIZATION: COMPLETE!\n","================================================================================\n"]}],"source":["# 📂 PROMPTS ORGANIZATION - STAGE 3 IMPROVEMENT\n","print(\"=\"*80)\n","print(\"📂 ENHANCED PROMPT ORGANIZATION - EXTERNAL FILE STRUCTURE\")\n","print(\"=\"*80)\n","\n","print(\"\\n🎯 PROMPT ORGANIZATION COMPLETED!\")\n","print(\"✅ All prompts moved from hardcoded to external file structure\")\n","print(\"✅ Following the complete process flow order:\")\n","\n","print(\"\\n📁 prompts/app/ structure:\")\n","print(\"   📂 01_input_guardrail/\")\n","print(\"      📄 classification_prompt.txt\")\n","print(\"      📄 generic_response_template.txt\")\n","print(\"   📂 02_main_generator/\")\n","print(\"      📄 match_scoring_prompt.txt\")\n","print(\"      📂 response_templates/\")\n","print(\"         📄 passed_template.txt\")\n","print(\"         📄 stand_by_template.txt\")\n","print(\"         📄 finished_template.txt\")\n","print(\"   📂 03_output_guardrail/\")\n","print(\"      📄 validation_prompt.txt\")\n","print(\"      📄 correction_prompt.txt\")\n","print(\"   📄 prompt_loader.py\")\n","\n","print(\"\\n🔧 MODIFIED CLASSES:\")\n","print(\"✅ InputGuardrail - Now loads prompts from 01_input_guardrail/\")\n","print(\"✅ OutputGuardrail - Now loads prompts from 03_output_guardrail/\")\n","print(\"✅ AIRecruiterAssistant - Now loads prompts from 02_main_generator/\")\n","\n","print(\"\\n🧪 KEPT IN CODE (for testing only):\")\n","print(\"✅ benchmark.system_prompt - Used for model benchmarking tests\")\n","print(\"✅ benchmark.test_prompts - Used for testing scenarios\")\n","\n","print(\"\\n💡 BENEFITS:\")\n","print(\"• 🎯 Clear separation of concerns - prompts organized by process flow\")\n","print(\"• 📝 Easy prompt editing without code changes\")\n","print(\"• 🔄 Version control for prompt improvements\")\n","print(\"• 🚀 Faster iteration on prompt engineering\")\n","print(\"• 🧹 Cleaner, more maintainable code\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"🎉 STAGE 3 PROMPT ENGINEERING OPTIMIZATION: COMPLETE!\")\n","print(\"=\"*80)\n"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1753663034009,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"_XWLkB0N-zZn","outputId":"ceddd54d-1333-42f6-b929-4886c7af407c"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔍 VERIFYING AI ASSISTANT OBJECT...\n","❌ ai_assistant object incomplete: Input guardrail missing\n"]}],"source":["# 🔧 VERIFY AI ASSISTANT OBJECT\n","# Confirm that ai_assistant object was created correctly\n","\n","print(\"🔍 VERIFYING AI ASSISTANT OBJECT...\")\n","\n","try:\n","    # Check if ai_assistant exists and has all required components\n","    assert hasattr(ai_assistant, 'guardrail'), \"Input guardrail missing\"\n","    assert hasattr(ai_assistant, 'output_guardrail'), \"Output guardrail missing\"\n","    assert hasattr(ai_assistant, 'rag_kb'), \"RAG knowledge base missing\"\n","    assert hasattr(ai_assistant, 'model_name'), \"Model name missing\"\n","\n","    print(\"✅ AI Assistant object verified successfully!\")\n","    print(f\"✅ Model: {ai_assistant.model_name}\")\n","    print(f\"✅ Input Guardrail: {ai_assistant.guardrail.model_name}\")\n","    print(f\"✅ Output Guardrail: {ai_assistant.output_guardrail.model_name}\")\n","    print(f\"✅ RAG Embeddings: {embedding_model_name}\")\n","\n","    print(f\"\\n🚀 AI ASSISTANT READY FOR USE!\")\n","    print(f\"• Call ai_assistant.load_models() to load all models\")\n","    print(f\"• Call test_custom_message() to test individual messages\")\n","    print(f\"• Call run_complete_demo() to test all scenarios\")\n","\n","except NameError:\n","    print(\"❌ ai_assistant object not found - check previous cells\")\n","except AssertionError as e:\n","    print(f\"❌ ai_assistant object incomplete: {str(e)}\")\n","except Exception as e:\n","    print(f\"❌ Error verifying ai_assistant: {str(e)}\")\n"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1753663034061,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"tEIe8QRz-zZl","outputId":"0e632122-4110-4bf1-c76e-2b5c650c76c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","✅ ENHANCED AI RECRUITER ASSISTANT - COMPLETE PIPELINE READY!\n","================================================================================\n","\n","🛡️ INPUT GUARDRAIL (Phi-3-mini-4k-instruct):\n","   • Generic vs Concrete message classification\n","   • Automatic response for generic messages\n","   • Pass concrete offers to RAG analysis\n","\n","🧠 MAIN GENERATOR (Mistral-7B-Instruct-v0.3):\n","   • RAG-powered match scoring analysis\n","   • State-based decision making (passed/stand_by/finished)\n","   • Context-aware professional response generation\n","\n","🛡️ OUTPUT GUARDRAIL (Meta-Llama-3-8B-Instruct):\n","   • Validates response naturalness (first person usage)\n","   • Removes placeholders like '[recruiter name]'\n","   • Iterative improvement (up to k=5 iterations)\n","   • Fallback correction if max iterations reached\n","\n","📊 BUSINESS LOGIC IMPLEMENTED:\n","   • Generic messages → 'pending_details' → Request more information\n","   • High match (>80%) → 'passed' → Schedule call\n","   • Medium match (60-80%) → 'stand_by' → Manual review\n","   • Low match (<60%) → 'finished' → Polite decline\n","\n","🔧 ALTERNATIVE MODELS AVAILABLE:\n","   • Output Guardrail alternatives: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\n","   • All models cached for faster loading\n","\n","🚀 SYSTEM READY FOR TESTING!\n","================================================================================\n"]}],"source":["# 📋 COMPLETE SYSTEM SUMMARY\n","print(\"=\"*80)\n","print(\"✅ ENHANCED AI RECRUITER ASSISTANT - COMPLETE PIPELINE READY!\")\n","print(\"=\"*80)\n","\n","print(\"\\n🛡️ INPUT GUARDRAIL (Phi-3-mini-4k-instruct):\")\n","print(\"   • Generic vs Concrete message classification\")\n","print(\"   • Automatic response for generic messages\")\n","print(\"   • Pass concrete offers to RAG analysis\")\n","\n","print(\"\\n🧠 MAIN GENERATOR (Mistral-7B-Instruct-v0.3):\")\n","print(\"   • RAG-powered match scoring analysis\")\n","print(\"   • State-based decision making (passed/stand_by/finished)\")\n","print(\"   • Context-aware professional response generation\")\n","\n","print(\"\\n🛡️ OUTPUT GUARDRAIL (Meta-Llama-3-8B-Instruct):\")\n","print(\"   • Validates response naturalness (first person usage)\")\n","print(\"   • Removes placeholders like '[recruiter name]'\")\n","print(\"   • Iterative improvement (up to k=5 iterations)\")\n","print(\"   • Fallback correction if max iterations reached\")\n","\n","print(\"\\n📊 BUSINESS LOGIC IMPLEMENTED:\")\n","print(\"   • Generic messages → 'pending_details' → Request more information\")\n","print(\"   • High match (>80%) → 'passed' → Schedule call\")\n","print(\"   • Medium match (60-80%) → 'stand_by' → Manual review\")\n","print(\"   • Low match (<60%) → 'finished' → Polite decline\")\n","\n","print(\"\\n🔧 ALTERNATIVE MODELS AVAILABLE:\")\n","print(\"   • Output Guardrail alternatives: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\")\n","print(\"   • All models cached for faster loading\")\n","\n","print(\"\\n🚀 SYSTEM READY FOR TESTING!\")\n","print(\"=\"*80)\n"]},{"cell_type":"markdown","metadata":{"id":"pMsQrWNljShm"},"source":["## ***PHASE 6:*** Complete System Testing\n","\n","Testing the full pipeline with different recruiter message scenarios to validate the guardrail and RAG integration.\n"]},{"cell_type":"markdown","metadata":{"id":"iPkWvNZa-zZn"},"source":["### 🧪 COMPREHENSIVE TESTING SUITE\n","Testing all scenarios: Generic messages, High/Medium/Low match concrete offers"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1753663034070,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"C2kasTW4jShm","outputId":"677bd340-b0be-41e0-99b1-e6516b50a6f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["🧪 Complete Testing Suite Ready!\n","\n","🔧 Available functions:\n","   • run_complete_demo() - Complete demo with all 4 scenarios\n","   • test_custom_message(message, name) - Test specific message\n","\n","💡 Example usage:\n","   run_complete_demo()\n","   test_custom_message('Your message here', 'My Test')\n"]}],"source":["def run_complete_demo():\n","    \"\"\"Complete demonstration of the AI assistant with all business logic scenarios\"\"\"\n","\n","    print(\"=\"*80)\n","    print(\"🚀 AI RECRUITER ASSISTANT - COMPLETE BUSINESS LOGIC DEMO\")\n","    print(\"=\"*80)\n","\n","    # Load all models\n","    print(\"\\n📥 STEP 1: Loading models...\")\n","    print(\"=\"*80)\n","    ai_assistant.load_models()\n","\n","    # Test scenarios covering all business logic paths\n","    test_scenarios = [\n","        {\n","            \"name\": \"Generic Networking Message\",\n","            \"message\": \"\"\"\n","            Hi Cristopher,\n","\n","            I hope you're doing well! I came across your profile and was impressed by your background.\n","\n","            Are you currently open to new opportunities? I'd love to connect and discuss some exciting possibilities.\n","\n","            Best regards,\n","            Sarah\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as GENERIC → State: pending_details\"\n","        },\n","\n","        {\n","            \"name\": \"High Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hi Cristopher,\n","\n","            We have an exciting Senior Data Engineer position at our AI-focused fintech startup.\n","\n","            Role details:\n","            - Building ETL/ELT pipelines with Python and Apache Airflow\n","            - Developing RAG systems using Semantic Kernel\n","            - Working with cloud platforms (Azure/GCP)\n","            - 100% remote work\n","            - Salary: €60,000-65,000 gross\n","            - Team: 15 engineers, very collaborative culture\n","\n","            Would you be interested in discussing this opportunity?\n","\n","            Best regards,\n","            Maria Rodriguez\n","            Technical Recruiter\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER → High match score (>80%) → State: passed\"\n","        },\n","\n","        {\n","            \"name\": \"Medium Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hello,\n","\n","            We're looking for a Data Scientist for our e-commerce platform.\n","\n","            Requirements:\n","            - 3+ years experience with Python and machine learning\n","            - Experience with recommendation systems\n","            - SQL and data analysis skills\n","            - Hybrid work (2 days office in Madrid)\n","            - Salary: €75,000-85,000\n","\n","            The role involves building ML models for customer behavior prediction.\n","\n","            Interested?\n","\n","            Thanks,\n","            Roberto\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER → Medium match score (60-80%) → State: stand_by\"\n","        },\n","\n","        {\n","            \"name\": \"Low Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hi,\n","\n","            We have a Java Backend Developer position available.\n","\n","            Requirements:\n","            - 5+ years Java/Spring Boot experience\n","            - Microservices architecture\n","            - On-site work in London\n","            - Banking domain experience preferred\n","            - Competitive salary\n","\n","            Let me know if you're interested.\n","\n","            Best,\n","            John Smith\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER → Low match score (<60%) → State: finished\"\n","        }\n","    ]\n","\n","    print(f\"\\n🧪 STEP 2: Testing {len(test_scenarios)} scenarios...\")\n","    print(\"=\"*80)\n","\n","    results = []\n","\n","    for i, scenario in enumerate(test_scenarios, 1):\n","        print(f\"\\n📨 SCENARIO #{i}: {scenario['name']}\")\n","        print(\"=\"*60)\n","        print(f\"Expected: {scenario['expected_outcome']}\")\n","        print(\"\\nMessage:\")\n","        print(scenario['message'].strip())\n","\n","        print(f\"\\n🤖 PROCESSING...\")\n","        print(\"-\"*40)\n","\n","        # Process through complete system\n","        try:\n","            result = ai_assistant.process_recruiter_message(scenario['message'])\n","            results.append({**result, \"scenario_name\": scenario['name']})\n","\n","            # Display results\n","            print(f\"\\n💬 FINAL RESPONSE:\")\n","            print(\"<<START>>\")\n","            print(result['final_response'])\n","            print(\"<<END>>\")\n","\n","            print(f\"\\n📊 ANALYSIS SUMMARY:\")\n","            print(f\"   🛡️ Message Type: {result['message_type'].value}\")\n","            print(f\"   🎯 Final State: {result['state'].value}\")\n","            print(f\"   📈 Confidence: {result['confidence']:.2f}\")\n","            if 'match_score' in result:\n","                print(f\"   🏆 Match Score: {result['match_score']}%\")\n","            print(f\"   ⚡ Processing Time: {result['processing_time']:.2f}s\")\n","            print(f\"   🔧 Pipeline Stage: {result['pipeline_stage']}\")\n","\n","            # Show detailed match analysis for concrete offers\n","            if result['pipeline_stage'] == 'complete_rag_analysis' and 'match_details' in result:\n","                match_details = result['match_details']\n","                if 'components' in match_details and match_details['components']:\n","                    print(f\"\\n📋 DETAILED MATCH BREAKDOWN:\")\n","                    for component, data in match_details['components'].items():\n","                        component_name = component.replace('_', ' ').title()\n","                        score = data.get('score', 'N/A')\n","                        reason = data.get('reason', 'No details')\n","                        print(f\"   • {component_name}: {score}% - {reason}\")\n","\n","        except Exception as e:\n","            print(f\"❌ Error processing scenario: {str(e)}\")\n","            results.append({\n","                \"scenario_name\": scenario['name'],\n","                \"error\": str(e),\n","                \"final_response\": f\"Error: {str(e)}\",\n","                \"state\": \"error\"\n","            })\n","\n","        print(\"\\n\" + \"=\"*60)\n","\n","    # Summary\n","    print(f\"\\n✅ TESTING COMPLETE!\")\n","    print(\"=\"*80)\n","\n","    successful_tests = len([r for r in results if 'error' not in r])\n","    print(f\"📊 Results: {successful_tests}/{len(test_scenarios)} scenarios processed successfully\")\n","\n","    # State distribution\n","    states = {}\n","    for result in results:\n","        if 'error' not in result:\n","            state = result['state'].value if hasattr(result['state'], 'value') else str(result['state'])\n","            states[state] = states.get(state, 0) + 1\n","\n","    print(f\"🎯 State Distribution:\")\n","    for state, count in states.items():\n","        print(f\"   • {state}: {count} scenario(s)\")\n","\n","    print(f\"\\n🧹 STEP 3: Cleaning up GPU memory...\")\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        print(\"✅ GPU memory cleared\")\n","\n","    return results\n","\n","def test_custom_message(message: str, scenario_name: str = \"Custom Test\"):\n","    \"\"\"Test a single message through the complete system with detailed analysis\"\"\"\n","\n","    print(f\"🧪 TESTING: {scenario_name}\")\n","    print(\"=\"*60)\n","    print(\"Message:\")\n","    print(message.strip())\n","\n","    print(\"\\n🔧 Processing through complete system...\")\n","\n","    try:\n","        result = ai_assistant.process_recruiter_message(message)\n","\n","        print(f\"\\n💬 RESPONSE:\")\n","        print(\"<<START>>\")\n","        print(result['final_response'])\n","        print(\"<<END>>\")\n","\n","        print(f\"\\n📊 DETAILED ANALYSIS:\")\n","        print(f\"   🛡️ Message Classification: {result['message_type'].value}\")\n","        print(f\"   🎯 Final State: {result['state'].value}\")\n","        print(f\"   📈 Classification Confidence: {result['confidence']:.2f}\")\n","\n","        if 'match_score' in result:\n","            print(f\"   🏆 Match Score: {result['match_score']}%\")\n","            print(f\"   📋 Match Reasoning: {result['match_details'].get('overall_reasoning', 'N/A')}\")\n","\n","        print(f\"   ⚡ Total Processing Time: {result['processing_time']:.2f}s\")\n","        print(f\"   🔧 Pipeline Stage: {result['pipeline_stage']}\")\n","\n","        return result\n","\n","    except Exception as e:\n","        print(f\"❌ Error: {str(e)}\")\n","        return {\"error\": str(e)}\n","\n","print(\"🧪 Complete Testing Suite Ready!\")\n","print(\"\\n🔧 Available functions:\")\n","print(\"   • run_complete_demo() - Complete demo with all 4 scenarios\")\n","print(\"   • test_custom_message(message, name) - Test specific message\")\n","print(\"\\n💡 Example usage:\")\n","print(\"   run_complete_demo()\")\n","print(\"   test_custom_message('Your message here', 'My Test')\")\n"]},{"cell_type":"markdown","metadata":{"id":"1PHTT9YC-zZr"},"source":["### 🚀 EXECUTE COMPLETE ENHANCED SYSTEM DEMO\n","Run the full demonstration of the guardrail + RAG system"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753663034075,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"unr9R1sRjShn","outputId":"127ed1c5-9841-4db8-f540-af69968775cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["🎯 AI RECRUITER ASSISTANT WITH GUARDRAIL\n","================================================================================\n","\n","🛡️ SYSTEM COMPONENTS:\n","✅ Input Guardrail: Generic vs Concrete classification\n","✅ RAG Knowledge Base: CV + Job expectations vector store\n","✅ Match Scoring System: Detailed 5-criteria evaluation\n","✅ State Management: pending_details, analyzing, passed, stand_by, finished\n","✅ Response Generation: Context-aware, state-specific templates\n","\n","📊 BUSINESS LOGIC IMPLEMENTED:\n","• Generic messages → State: 'pending_details' → Request details\n","• Concrete offers → RAG analysis → Match scoring → State-based response\n","• High match (>80%) → State: 'passed' → Schedule call\n","• Medium match (60-80%) → State: 'stand_by' → Manual review\n","• Low match (<60%) → State: 'finished' → Polite decline\n","\n","🎭 TEST SCENARIOS:\n","1. Generic networking message\n","2. High match concrete offer (Data Engineer, Python, Remote, €60-65k)\n","3. Medium match concrete offer (Data Scientist, different domain)\n","4. Low match concrete offer (Java Developer, on-site)\n","\n","⏸️ Set RUN_COMPLETE_DEMO = True to start the demo\n","Or use: run_complete_demo()\n","\n","💡 For custom testing:\n","test_custom_message('Your recruiter message here', 'Custom Test')\n"]}],"source":["print(\"🎯 AI RECRUITER ASSISTANT WITH GUARDRAIL\")\n","print(\"=\"*80)\n","\n","print(\"\\n🛡️ SYSTEM COMPONENTS:\")\n","print(\"✅ Input Guardrail: Generic vs Concrete classification\")\n","print(\"✅ RAG Knowledge Base: CV + Job expectations vector store\")\n","print(\"✅ Match Scoring System: Detailed 5-criteria evaluation\")\n","print(\"✅ State Management: pending_details, analyzing, passed, stand_by, finished\")\n","print(\"✅ Response Generation: Context-aware, state-specific templates\")\n","\n","print(\"\\n📊 BUSINESS LOGIC IMPLEMENTED:\")\n","print(\"• Generic messages → State: 'pending_details' → Request details\")\n","print(\"• Concrete offers → RAG analysis → Match scoring → State-based response\")\n","print(\"• High match (>80%) → State: 'passed' → Schedule call\")\n","print(\"• Medium match (60-80%) → State: 'stand_by' → Manual review\")\n","print(\"• Low match (<60%) → State: 'finished' → Polite decline\")\n","\n","print(\"\\n🎭 TEST SCENARIOS:\")\n","print(\"1. Generic networking message\")\n","print(\"2. High match concrete offer (Data Engineer, Python, Remote, €60-65k)\")\n","print(\"3. Medium match concrete offer (Data Scientist, different domain)\")\n","print(\"4. Low match concrete offer (Java Developer, on-site)\")\n","\n","# Set to True to run the complete demo\n","RUN_COMPLETE_DEMO = False # True #\n","\n","if RUN_COMPLETE_DEMO:\n","    print(f\"\\n🚀 STARTING COMPLETE SYSTEM DEMO...\\n\")\n","\n","    try:\n","        results = run_complete_demo()\n","\n","        print(f\"\\n🎉 DEMO COMPLETED SUCCESSFULLY!\")\n","        print(\"=\"*80)\n","        print(\"✅ All business logic scenarios tested\")\n","        print(\"✅ Guardrail integration working correctly\")\n","        print(\"✅ RAG pipeline functioning properly\")\n","        print(\"✅ Match scoring system operational\")\n","        print(\"✅ State management implemented\")\n","        print(\"\\n🚀 Ready for Stage 4: Application Integration!\")\n","\n","    except Exception as e:\n","        print(f\"❌ Demo failed with error: {str(e)}\")\n","        print(f\"💡 Check models are loaded and try again\")\n","\n","else:\n","    print(f\"\\n⏸️ Set RUN_COMPLETE_DEMO = True to start the demo\")\n","    print(f\"Or use: run_complete_demo()\")\n","\n","print(\"\\n💡 For custom testing:\")\n","print(\"test_custom_message('Your recruiter message here', 'Custom Test')\")\n"]},{"cell_type":"markdown","metadata":{"id":"c24hPrS9-zZs"},"source":["### 🚀 TEST COMPLETE SYSTEM WITH OUTPUT GUARDRAIL\n","Test the same message that previously had problems to verify the output guardrail fixes them\n"]},{"cell_type":"code","execution_count":83,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1753663034076,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"aTOmT2kF8X0v"},"outputs":[],"source":["RUN_TEST_CUSTOM_MESSAGE = False"]},{"cell_type":"code","execution_count":84,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1753663034086,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"x9AJUUq2-zZs"},"outputs":[],"source":["if RUN_TEST_CUSTOM_MESSAGE:\n","    print(\"🧪 TESTING COMPLETE SYSTEM WITH OUTPUT GUARDRAIL\")\n","    print(\"=\"*80)\n","\n","    # Test the message that previously generated \"the candidate's\" instead of \"my\"\n","    test_message = \"\"\"\n","    Exciting REMOTE GenAI Opportunity – Long-Term Contract with Virtusa\n","    Hi Cristopher,\n","\n","    I hope you're doing well!\n","\n","    I'm reaching out regarding an exciting opportunity for a Spanish-speaking GenAI Engineer with GCP on a long-term B2B contract with Virtusa. This is a REMOTE role and it's an urgent requirement.\n","\n","    We're specifically looking for someone with experience in:\n","\n","    Dialogflow CX and GCP\n","    Contact Center AI (CCAI)\n","    Visual flow design\n","    Native GCP integration (BigQuery, Cloud Functions, etc.)\n","    Fluent Spanish – This is a must-have\n","\n","    If this sounds like a good fit or if you know someone in your network who might be interested, I'd love to connect and share more details.\n","\n","    Looking forward to hearing from you!!\n","\n","    Best regards,\n","    Priyanka\n","    \"\"\"\n","\n","    print(\"📨 TEST MESSAGE:\")\n","    print(test_message.strip())\n","    print(\"\\n\" + \"=\"*60)\n","\n","    print(\"🚀 PROCESSING WITH OUTPUT GUARDRAIL...\")\n","    print(\"Expected: Should generate natural first-person response without 'the candidate' references\")\n","    print(\"\\n🔧 Processing...\")\n","\n","    try:\n","        # This will now go through the complete pipeline including output guardrail\n","        result = test_custom_message(test_message, \"Output Guardrail Test\")\n","\n","        print(\"\\n✅ TESTING COMPLETE!\")\n","        print(\"🔍 Check the response above to verify it uses 'I', 'my', 'me' instead of 'the candidate'\")\n","\n","    except Exception as e:\n","        print(f\"❌ Error during test: {str(e)}\")\n","        print(\"💡 Make sure to run ai_assistant.load_models() first if models aren't loaded\")\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"🎯 OUTPUT GUARDRAIL FUNCTIONALITY:\")\n","    print(\"• Validates first-person usage (I, my, me)\")\n","    print(\"• Removes placeholders like [recruiter name]\")\n","    print(\"• Iterates up to k=5 times for improvement\")\n","    print(\"• Provides fallback correction if needed\")\n","    print(f\"• Uses '{output_guardrail_model_name}' for validation\")\n","    print(\"=\"*80)\n"]},{"cell_type":"code","execution_count":85,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1753663034104,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"V9gIeEgX-zZs","outputId":"e78a60fe-07b3-4a09-9d86-dd359e3ee693"},"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================================\n","🎉 AI RECRUITER ASSISTANT WITH OUTPUT GUARDRAIL - SYSTEM READY!\n","==========================================================================================\n","\n","🏗️ SYSTEM ARCHITECTURE IMPLEMENTED:\n","   📨 Recruiter Message\n","      ↓\n","   🛡️ Input Guardrail → Generic/Concrete Classification\n","      ↓\n","   🧠 Main Generator → RAG Analysis + Response Generation\n","      ↓\n","   🛡️ Output Guardrail → Naturalness Validation\n","      ↓\n","   💬 Final Natural Response\n","\n","✅ COMPONENTS READY:\n","   🛡️ Input Guardrail: microsoft/Phi-3-mini-4k-instruct\n","   🧠 Main Generator: mistralai/Mistral-7B-Instruct-v0.3\n","   🛡️ Output Guardrail: microsoft/Phi-3-mini-4k-instruct\n","   🧠 RAG Embeddings: sentence-transformers/all-MiniLM-L6-v2\n","\n","🚀 HOW TO USE:\n","==================================================\n","\n","1️⃣ LOAD MODELS (Required first step):\n","   ai_assistant.load_models()\n","\n","2️⃣ TEST INDIVIDUAL MESSAGE:\n","   message = 'Your recruiter message here'\n","   result = test_custom_message(message, 'Test Name')\n","\n","3️⃣ RUN COMPLETE DEMO (All 4 scenarios):\n","   results = run_complete_demo()\n","\n","🛡️ OUTPUT GUARDRAIL FEATURES:\n","   • Converts 'the candidate' → 'I'\n","   • Converts 'candidate's' → 'my'\n","   • Removes '[recruiter name]' placeholders\n","   • Iterates up to k=5 times for improvement\n","   • Provides fallback correction if needed\n","\n","💡 EXAMPLE USAGE:\n","==================================================\n","# Step 1: Load models\n","ai_assistant.load_models()\n","\n","# Step 2: Test message\n","test_message = '''\n","Hi! We have a Data Engineer position using Python and RAG.\n","€65k salary, 100% remote. Interested?\n","'''\n","result = test_custom_message(test_message, 'Quick Test')\n","\n","==========================================================================================\n","🎯 SYSTEM READY - NO MORE SETUP NEEDED!\n","==========================================================================================\n"]}],"source":["# 🎯 FINAL SYSTEM READY - HOW TO USE\n","print(\"=\"*90)\n","print(\"🎉 AI RECRUITER ASSISTANT WITH OUTPUT GUARDRAIL - SYSTEM READY!\")\n","print(\"=\"*90)\n","\n","print(\"\\n🏗️ SYSTEM ARCHITECTURE IMPLEMENTED:\")\n","print(\"   📨 Recruiter Message\")\n","print(\"      ↓\")\n","print(\"   🛡️ Input Guardrail → Generic/Concrete Classification\")\n","print(\"      ↓\")\n","print(\"   🧠 Main Generator → RAG Analysis + Response Generation\")\n","print(\"      ↓\")\n","print(\"   🛡️ Output Guardrail → Naturalness Validation\")\n","print(\"      ↓\")\n","print(\"   💬 Final Natural Response\")\n","\n","print(\"\\n✅ COMPONENTS READY:\")\n","print(f\"   🛡️ Input Guardrail: {input_guardrail_model_name}\")\n","print(f\"   🧠 Main Generator: {selected_text_generator_model}\")\n","print(f\"   🛡️ Output Guardrail: {output_guardrail_model_name}\")\n","print(f\"   🧠 RAG Embeddings: {embedding_model_name}\")\n","\n","print(\"\\n🚀 HOW TO USE:\")\n","print(\"=\"*50)\n","\n","print(\"\\n1️⃣ LOAD MODELS (Required first step):\")\n","print(\"   ai_assistant.load_models()\")\n","\n","print(\"\\n2️⃣ TEST INDIVIDUAL MESSAGE:\")\n","print(\"   message = 'Your recruiter message here'\")\n","print(\"   result = test_custom_message(message, 'Test Name')\")\n","\n","print(\"\\n3️⃣ RUN COMPLETE DEMO (All 4 scenarios):\")\n","print(\"   results = run_complete_demo()\")\n","\n","print(\"\\n🛡️ OUTPUT GUARDRAIL FEATURES:\")\n","print(\"   • Converts 'the candidate' → 'I'\")\n","print(\"   • Converts 'candidate's' → 'my'\")\n","print(\"   • Removes '[recruiter name]' placeholders\")\n","print(\"   • Iterates up to k=5 times for improvement\")\n","print(\"   • Provides fallback correction if needed\")\n","\n","print(\"\\n💡 EXAMPLE USAGE:\")\n","print(\"=\"*50)\n","print(\"# Step 1: Load models\")\n","print(\"ai_assistant.load_models()\")\n","print(\"\")\n","print(\"# Step 2: Test message\")\n","print(\"test_message = '''\")\n","print(\"Hi! We have a Data Engineer position using Python and RAG.\")\n","print(\"€65k salary, 100% remote. Interested?\")\n","print(\"'''\")\n","print(\"result = test_custom_message(test_message, 'Quick Test')\")\n","\n","print(\"\\n\" + \"=\"*90)\n","print(\"🎯 SYSTEM READY - NO MORE SETUP NEEDED!\")\n","print(\"=\"*90)\n"]},{"cell_type":"markdown","metadata":{"id":"mQJ4MRbJtlyl"},"source":["# **Stage 4: Application Integration**"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1753663034106,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"0LKUf1XjP6VB"},"outputs":[],"source":["import gradio as gr\n","\n","def process_chat_message(message, history):\n","    \"\"\"\n","    Wrapper function for Gradio ChatInterface that processes recruiter messages\n","    using the existing ai_assistant object.\n","\n","    Args:\n","        message (str): User's input message\n","        history (list): Chat history (standard Gradio ChatInterface parameter)\n","\n","    Returns:\n","        str: Assistant's response to be displayed in the chat\n","    \"\"\"\n","    try:\n","        # Call the core logic using the existing ai_assistant object\n","        result = ai_assistant.process_recruiter_message(message)\n","\n","        # Extract the final_response from the dictionary returned by the method\n","        final_response = result.get('final_response', 'Sorry, we could not process your message. Try again.')\n","\n","        return final_response\n","\n","    except Exception as e:\n","        # Gracefully handle any potential errors during inference\n","        error_message = f\"\"\"There is an error processing your message.\n","\n","Please try again or contact support if the issue persists.\n","\n","Error details: {str(e)}\"\"\"\n","\n","        return error_message\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289,"referenced_widgets":["10efb148bed8460fbf5ffd5abb3a5142","4b57ef03ed0c427eb01bb49071263c69","2b3365b8679b4619ae4a9477bf2f6f2e","f37948fbce6649faac8b099491b1a2ae","9de9cddb0df74d1b9d52c8bd3af1b4cb","f8794c44aa174131aee7a022dfb44e7d","25aa5822d6fb40bfb0a7ded3b81f7df0","ceb22e3e71cc4e4284a64716517ab949","eff767605648400ba87ae88f4b1de108","9dbcf2b1fe7d46e8bbc294790e58cca5","e9df75d23ba94164b890714ff4d5f0c1","87268b3cd83b4a14be6df8e2bbc430fc","ae106afdc7974c79931be4711ed76608","754698c6b49c4d31a762ad0cd090657a","769c0a2031c742eba7bd2dc3c4b3e2e8","0579e7102db2437782c911e1124fd224","9d396692b94041baa2368bb5070d7fa3","2cf31dd839404c70bc3d422e9a727caa","2995fefd3ec642ad81d9c4f878bac9e5","7c5b722537674208856d6f2461909653","0cabbf792fbd443781fedc52d24958a2","60c868afd23d42b5b8ba60b82de92328"]},"id":"xak27Ogz7SHP","outputId":"a3d9fdb9-a684-4c8f-e000-5f96389399bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 Setting up AI Assistant for Gradio interface...\n","📚 Setting up RAG Knowledge Base...\n","   ✅ RAG vectorstore already exists!\n","🤖 Loading AI Assistant models...\n","🔧 Loading models...\n","\n","🛡️ Loading guardrail model: microsoft/Phi-3-mini-4k-instruct\n","⚡ Loading from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10efb148bed8460fbf5ffd5abb3a5142"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Guardrail model loaded successfully!\n","\n","🛡️ Loading output guardrail model: microsoft/Phi-3-mini-4k-instruct\n","⚡ Loading from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87268b3cd83b4a14be6df8e2bbc430fc"}},"metadata":{}}],"source":["# First, ensure RAG knowledge base is properly set up\n","print(\"🚀 Setting up AI Assistant for Gradio interface...\")\n","try:\n","    # Step 1: Setup RAG Knowledge Base if not already done\n","    print(\"📚 Setting up RAG Knowledge Base...\")\n","    if not rag_kb.vectorstore:\n","        print(\"   🔧 RAG vectorstore not found, creating it...\")\n","        rag_kb.setup_embeddings()\n","        if cv_content and expectations_content:\n","            rag_kb.load_and_process_documents(cv_content, expectations_content)\n","            rag_kb.create_vectorstore()\n","            print(\"   ✅ RAG vectorstore created successfully!\")\n","        else:\n","            raise ValueError(\"CV or expectations content not found. Please run earlier cells first.\")\n","    else:\n","        print(\"   ✅ RAG vectorstore already exists!\")\n","\n","    # Step 2: Load all models\n","    print(\"🤖 Loading AI Assistant models...\")\n","    ai_assistant.load_models()\n","    print(\"✅ All models loaded successfully!\")\n","\n","    # Create and launch the Gradio ChatInterface\n","    print(\"\\n🎯 Creating Gradio Chat Interface...\")\n","\n","    # Create the ChatInterface with the wrapper function\n","    chat_interface = gr.ChatInterface(\n","        fn=process_chat_message,\n","        title=\"🤖 AI Recruiter Assistant\",\n","        description=\"\"\"\n","        **Welcome Recruiter!**\n","\n","        **How it works:**\n","        - Send me a message and I'll analyze if it's a good fit\n","\n","        **Try the examples below or send your message!**\n","        \"\"\",\n","        examples=[\n","            \"Hi, are you open to new opportunities?\" #,\n","        #     \"We have a Senior Data Engineer role with Python and Azure, fully remote, €60-65k salary. Interested?\",\n","        #     \"Looking for a Java developer, 5 years experience, on-site in London. Competitive salary.\",\n","        #     \"Exciting GenAI opportunity with GCP, Dialogflow, remote work, B2B contract. Spanish required.\"\n","        ],\n","        theme=gr.themes.Soft(),\n","        chatbot=gr.Chatbot(\n","            height=800,\n","            show_label=False,\n","            avatar_images=(None, \"🤖\")\n","        )\n","    )\n","\n","    # Launch the interface with public URL and debugging enabled\n","    print(\"🌐 Launching Gradio interface...\")\n","    chat_interface.launch(\n","        share=True,  # Create a public URL\n","        debug=True,  # Enable debugging output\n","        server_name=\"0.0.0.0\",  # Allow connections from any IP (important for Colab)\n","        server_port=7860,  # Default Gradio port\n","        show_error=True  # Show detailed error messages\n","    )\n","\n","except Exception as e:\n","    print(f\"❌ Error setting up Gradio interface: {str(e)}\")\n","    print(\"\\nTroubleshooting tips:\")\n","    print(\"1. Make sure all previous cells have been executed successfully\")\n","    print(\"2. Verify that the ai_assistant object exists\")\n","    print(\"3. Check if there are any memory issues\")\n","    print(\"4. Try restarting the runtime if problems persist\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["U8cWQ1hcpXZt"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"10efb148bed8460fbf5ffd5abb3a5142":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b57ef03ed0c427eb01bb49071263c69","IPY_MODEL_2b3365b8679b4619ae4a9477bf2f6f2e","IPY_MODEL_f37948fbce6649faac8b099491b1a2ae"],"layout":"IPY_MODEL_9de9cddb0df74d1b9d52c8bd3af1b4cb"}},"4b57ef03ed0c427eb01bb49071263c69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8794c44aa174131aee7a022dfb44e7d","placeholder":"​","style":"IPY_MODEL_25aa5822d6fb40bfb0a7ded3b81f7df0","value":"Loading checkpoint shards: 100%"}},"2b3365b8679b4619ae4a9477bf2f6f2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceb22e3e71cc4e4284a64716517ab949","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eff767605648400ba87ae88f4b1de108","value":2}},"f37948fbce6649faac8b099491b1a2ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dbcf2b1fe7d46e8bbc294790e58cca5","placeholder":"​","style":"IPY_MODEL_e9df75d23ba94164b890714ff4d5f0c1","value":" 2/2 [00:11&lt;00:00,  5.64s/it]"}},"9de9cddb0df74d1b9d52c8bd3af1b4cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8794c44aa174131aee7a022dfb44e7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25aa5822d6fb40bfb0a7ded3b81f7df0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceb22e3e71cc4e4284a64716517ab949":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eff767605648400ba87ae88f4b1de108":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9dbcf2b1fe7d46e8bbc294790e58cca5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9df75d23ba94164b890714ff4d5f0c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87268b3cd83b4a14be6df8e2bbc430fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae106afdc7974c79931be4711ed76608","IPY_MODEL_754698c6b49c4d31a762ad0cd090657a","IPY_MODEL_769c0a2031c742eba7bd2dc3c4b3e2e8"],"layout":"IPY_MODEL_0579e7102db2437782c911e1124fd224"}},"ae106afdc7974c79931be4711ed76608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d396692b94041baa2368bb5070d7fa3","placeholder":"​","style":"IPY_MODEL_2cf31dd839404c70bc3d422e9a727caa","value":"Loading checkpoint shards:   0%"}},"754698c6b49c4d31a762ad0cd090657a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2995fefd3ec642ad81d9c4f878bac9e5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c5b722537674208856d6f2461909653","value":0}},"769c0a2031c742eba7bd2dc3c4b3e2e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cabbf792fbd443781fedc52d24958a2","placeholder":"​","style":"IPY_MODEL_60c868afd23d42b5b8ba60b82de92328","value":" 0/2 [00:00&lt;?, ?it/s]"}},"0579e7102db2437782c911e1124fd224":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d396692b94041baa2368bb5070d7fa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cf31dd839404c70bc3d422e9a727caa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2995fefd3ec642ad81d9c4f878bac9e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c5b722537674208856d6f2461909653":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cabbf792fbd443781fedc52d24958a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c868afd23d42b5b8ba60b82de92328":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}