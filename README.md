# AI Recruiter Assistant ðŸ¤–

An intelligent conversational chatbot designed to pre-screen job offers from recruiters using Retrieval-Augmented Generation (RAG) with a powerful open-source Large Language Model.

## ðŸŽ¯ Project Overview

This AI assistant automates the initial screening of job offers by:
1.  **Intent Detection**: Analyzing if messages are job offers or generic inquiries.
2.  **RAG Analysis**: Augmenting the LLM with real-time context from your CV and job preferences.
3.  **Match Scoring**: Calculating a compatibility score based on the provided context.
4.  **Smart Responses**: Generating contextual, human-like responses based on the match score.

## ðŸ—ï¸ Architecture

The system follows a **RAG-first approach** to ensure contextual accuracy and optimal performance without the immediate need for fine-tuning. This strategy prioritizes context-aware prompt engineering over model specialization.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Interface  â”‚â—„â”€â”€â”€â–ºâ”‚  State Manager  â”‚â—„â”€â”€â”€â–ºâ”‚   RAG Pipeline  â”‚
â”‚    (Gradio)     â”‚      â”‚ (Conversation)  â”‚      â”‚ (FAISS VectorDB)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                                                 â”‚
         â”‚                                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User/Recruiter â”‚      â”‚ Prompt Engineer â”‚â—„â”€â”€â”€â–ºâ”‚  Retrieved Docs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â–²                       â–²
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Base LLM      â”‚
                          â”‚ (No Fine-tuning)â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quick Start

### 1. Setup Environment
```bash
pip install -r requirements.txt
```

### 2. Prepare Your Data
- Ensure `cv.md` contains your CV/resume
- Ensure `job_expectations.md` contains your job preferences
- Place LinkedIn messages in `data/linkedin_messages.csv`

### 3. Run the Notebook
```bash
# Open the Jupyter notebook
jupyter notebook ai_recruiter_assistant_colab.ipynb
```

## ðŸ“Š Conversation Flow

### State Management
- **`pending_details`**: Waiting for job details
- **`passed`**: High match (>80%) - ready to schedule call
- **`stand_by`**: Medium match (60-80%) - manual review needed
- **`finished`**: Low match (<60%) - politely decline

### Match Scoring Logic
- **>80%**: Positive response, request call scheduling
- **60-80%**: Cordial response, manual review needed
- **<60%**: Polite decline with explanation

## ðŸ› ï¸ Technical Stack

### Core Technologies
- **LLM**: Multiple model options (Mistral-7B, Llama-3-8B, Phi-3-mini, Gemma-2-9B)
- **Strategy**: RAG-first approach with advanced prompt engineering
- **RAG**: FAISS vector database + sentence transformers
- **Framework**: LangChain for orchestration
- **Interface**: Gradio web application

### Model Selection
We benchmark **4 open-source models** to select the best performer for our RAG-based approach:

| Model | Size | Context | Multimodal | Description |
|-------|------|---------|------------|-------------|
| **Mistral-7B-Instruct-v0.2** | 7B | 32K | ðŸ“ No | Efficient instruction-following |
| **Meta-Llama-3-8B-Instruct** | 8B | 8K | ðŸ“ No | Strong reasoning capabilities |
| **Microsoft/Phi-3-mini-4k** | 3.8B | 4K | ðŸ“ No | Lightweight, fast inference |
| **Google/Gemma-2-9B-IT** | 9B | 8K | ðŸ“ No | Google's efficient instruction-tuned |

### Key Libraries
- `transformers`: Hugging Face transformers
- `langchain`: LLM orchestration
- `faiss-cpu`: Vector similarity search
- `sentence-transformers`: Embedding generation
- `gradio`: Web interface

## ðŸ“ Project Structure

```
AI_Recruiter_Assistant/
â”œâ”€â”€ ai_recruiter_assistant_colab.ipynb # Main notebook
â”œâ”€â”€ requirements.txt                    # Dependencies
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ PROJECT_SUMMARY.md                 # Development status
â”œâ”€â”€ RAG/
â”‚   â”œâ”€â”€ cv.md                         # Your CV/resume
â”‚   â””â”€â”€ job_expectations.md           # Job preferences
â”œâ”€â”€ data/
â”‚   â””â”€â”€ linkedin_messages.csv         # Historical data
â””â”€â”€ notebooks/                        # Development iterations
    â””â”€â”€ *.ipynb                       # Version history
```

## ðŸ”§ Development Plan (RAG-First Approach)

### Day 1-2: Foundation âœ… COMPLETED
- [x] Environment setup
- [x] Data loading and processing
- [x] RAG pipeline implementation
- [x] Vector database creation

### Day 3-4: Model Selection & Optimization ðŸ”„ CURRENT
- [x] LLM model benchmarking
- [x] Performance comparison
- [x] Model selection and optimization
- [x] Cache management setup

### Day 5-6: Prompt Engineering & Logic ðŸ“‹ NEXT
- [x] Context-aware prompt design
- [ ] Intent detection enhancement
- [ ] Match scoring refinement
- [ ] Response generation optimization

### Day 7: Integration & Testing ðŸš€ FINAL
- [ ] Gradio interface
- [ ] End-to-end testing
- [ ] Performance optimization
- [ ] Deployment and documentation

## ðŸŽ¯ Features

### âœ… Implemented
- Document loading and processing
- RAG pipeline with FAISS vector database
- State management system
- Basic intent detection
- Match scoring algorithm
- Response generation templates
- Model benchmarking infrastructure

### ðŸš§ In Progress
- Model selection and optimization
- Advanced prompt engineering
- Context-aware retrieval

### ðŸ”® Future Enhancements (Post-RAG Optimization)
- Fine-tuning with QLoRA (when performance plateau is reached)
- Google Calendar integration
- Advanced conversation memory
- Multi-language support
- Analytics dashboard

## ðŸ¤ Usage

1. **Open the notebook** in Google Colab
2. **Run all cells** to set up the environment
3. **Load your data** (CV, expectations, LinkedIn messages)
4. **Select optimal model** based on benchmark results
5. **Test prompt engineering** with RAG retrieval
6. **Launch the Gradio interface** for testing
7. **Deploy** for production use

## ðŸ“ˆ Performance Metrics

- **Response Time**: < 5 seconds
- **Match Accuracy**: 85%+ (target)
- **Memory Usage**: Optimized for Colab free tier
- **Model Size**: ~4GB (quantized)
- **RAG Retrieval**: < 1 second

## ðŸ”’ Privacy & Security

- All data processed locally
- No external API calls required
- Open-source model ensures transparency
- No data sent to third parties

## ðŸŽ“ Development Methodology

This project follows a **4-stage Generative AI lifecycle**:

1. **Define the Scope** âœ… - Problem identification and requirements
2. **Select Models** ðŸ”„ - Benchmark and choose optimal LLM
3. **Adapt & Align** ðŸ“‹ - RAG optimization and prompt engineering
4. **Application Integration** ðŸš€ - Deploy and test complete system

**Current Strategy**: RAG-first approach with fine-tuning postponed to future iterations, focusing on maximizing base model potential through advanced context retrieval and prompt engineering.

## ðŸ“ž Support

For questions or issues:
1. Check the notebook comments
2. Review the conversation flow logic
3. Verify your data format
4. Test with sample messages

---

**Built with â¤ï¸ using open-source AI technologies** 