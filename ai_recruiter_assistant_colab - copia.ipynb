{"cells":[{"cell_type":"markdown","metadata":{"id":"B5E0iaKT7cPG"},"source":["# **AI Recruiter Assistant ü§ñ**\n","\n","A conversational chatbot to pre-screen job offers from recruiters using advanced AI techniques.\n","\n","## Project Overview\n","- **Goal**: Automate initial screening of job offers from recruiters\n","- **Technology**: RAG-first approach with open-source LLM and advanced prompt engineering\n","- **Interface**: Gradio web application for real-time conversations\n","- **Timeline**: 1 week structured development\n","- **Methodology**: Following systematic Generative AI project lifecycle\n","\n","## üéØ **AI Project Methodology**\n","\n","This project follows a **structured 4-stage Generative AI lifecycle** for systematic development and evaluation:\n","\n","### **Stage 1: Define the Scope** ‚úÖ COMPLETED\n","- **Problem Identification**: Manual screening of recruiter messages is time-consuming and inconsistent\n","- **Desired Outcome**: Automated system that analyzes job offers and responds appropriately based on profile match\n","- **Data Requirements**: CV, job expectations, LinkedIn conversation history for context retrieval\n","- **Feasibility**: Generative AI is ideal for this conversational task with contextual decision-making\n","\n","### **Stage 2: Select Models** ‚úÖ COMPLETED\n","- **Research Models**: Compare 4 open-source models from Hugging Face\n","- **Benchmark Performance**: Test speed, memory usage, and response quality\n","- **Model Selection**: Choose optimal model based on performance metrics\n","- **Cache Management**: Efficient model storage and loading from Google Drive\n","\n","### **Stage 3: Adapt & Align Model** ‚úÖ COMPLETED\n","- **RAG Pipeline Implementation**: ‚úÖ Build core retrieval system with FAISS vector database\n","- **Context-Aware Prompt Engineering**: ‚úÖ Design system prompts for effective context utilization\n","- **Implementing Guardrails**: ‚úÖ Add safety and formatting checks for reliable output\n","- **Performance Evaluation**: ‚úÖ Manual and qualitative assessment of retrieval and response quality\n","- **Prompt Engineering Optimization**: ‚úÖ Refine and improve prompt effectiveness\n","- **Fine-Tuning (Postponed)**: Deferred to future iterations, focusing on RAG optimization first\n","\n","### **Stage 4: Application Integration & Deployment** üîÑ CURRENT\n","- **Gradio Interface Implementation**: ‚úÖ Deploy web application for real-time testing\n","- **Hugging Face Spaces Deployment**: üìã Step-by-step deployment to production\n","- **End-to-end Testing**: üìã Comprehensive system validation\n","- **Final Summary & Review**: üìã Complete project documentation and next steps\n","\n","## üèóÔ∏è **System Architecture** (RAG-First Approach)\n","\n","```mermaid\n","graph LR\n","    A[üì® Recruiter Message] --> B[üîç Intent Detection]\n","    B --> C[üìä RAG Analysis]\n","    C --> D[üéØ Match Scoring]\n","    D --> E[üß† State Management]\n","    E --> F[üí¨ Response Generation]\n","    \n","    G[üìÑ CV + Job Expectations] --> H[üîç Vector Embeddings]\n","    H --> I[üíæ FAISS Database]\n","    I --> C\n","    \n","    J[üß† Base LLM] --> F\n","    K[üéØ Prompt Engineering] --> J\n","    L[üõ°Ô∏è Guardrails] --> F\n","```\n","\n","---\n","\n","**üöÄ Currently working on Stage 4: Gradio Interface Implementation - Production deployment phase**\n"]},{"cell_type":"markdown","metadata":{"id":"hyj76RMWpRAk"},"source":["# **Stage 1: Define the Scope**"]},{"cell_type":"markdown","metadata":{"id":"_uEAC7f_pdh7"},"source":["## ***PHASE 1*** - Configuration"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105544,"status":"ok","timestamp":1753718300531,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"5p_hhDIH7cPJ","outputId":"4bf9951a-fce8-46f5-da9d-c39efac4c26c"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ All dependencies loaded!\n","üî• CUDA available: True\n","üéÆ GPU: NVIDIA A100-SXM4-40GB\n","üíæ GPU Memory: 42.5 GB\n"]}],"source":["# Install required packages\n","!pip install transformers>=4.36.0 torch>=2.0.0 peft>=0.7.0 bitsandbytes>=0.41.0 accelerate>=0.24.0\n","!pip install langchain>=0.1.0 langchain-community>=0.0.10 faiss-cpu>=1.7.4 sentence-transformers>=2.2.0\n","!pip install gradio>=4.0.0 pandas>=2.0.0 numpy>=1.24.0 tqdm>=4.65.0 datasets>=2.14.0\n","\n","# Core imports\n","import os, json, torch, pandas as pd, numpy as np, time, psutil\n","from pathlib import Path\n","from typing import List, Dict, Tuple, Optional\n","from dataclasses import dataclass\n","from enum import Enum\n","\n","# ML/AI imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, GenerationConfig\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# RAG imports\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.schema import Document\n","\n","print(\"‚úÖ All dependencies loaded!\")\n","print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"]},{"cell_type":"markdown","metadata":{"id":"m6YQjLRYuTUM"},"source":["## ***PHASE 2*** - Data Requirements"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19615,"status":"ok","timestamp":1753718320144,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"vsPjCQa47cPK","outputId":"bdde3629-68f3-4462-8e44-ef509c0ab065"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ Google Drive mounted\n","Main paths\n","üìÅ Project path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\n","üóÇÔ∏è Cache path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/huggingface_cache\n","Loaded documents\n","‚úÖ CV loaded: 7032 characters\n","‚úÖ Job expectations loaded: 326 characters\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up project paths\n","project_path = \"/content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\"\n","cache_path = f\"{project_path}/huggingface_cache\"\n","\n","# Create directories\n","os.makedirs(cache_path, exist_ok=True)\n","os.makedirs(f\"{project_path}/data\", exist_ok=True)\n","\n","print(f\"‚úÖ Google Drive mounted\")\n","print(f\"Main paths\")\n","print(f\"üìÅ Project path: {project_path}\")\n","print(f\"üóÇÔ∏è Cache path: {cache_path}\")\n","\n","# Load data files\n","def load_documents():\n","    try:\n","        with open(f'{project_path}/RAG/cv.md', 'r', encoding='utf-8') as f:\n","            cv_content = f.read()\n","        with open(f'{project_path}/RAG/job_expectations.md', 'r', encoding='utf-8') as f:\n","            expectations_content = f.read()\n","\n","        print(f\"Loaded documents\")\n","        print(f\"‚úÖ CV loaded: {len(cv_content)} characters\")\n","        print(f\"‚úÖ Job expectations loaded: {len(expectations_content)} characters\")\n","        return cv_content, expectations_content\n","    except FileNotFoundError as e:\n","        print(f\"‚ùå Error loading documents: {e}\")\n","        return None, None\n","\n","cv_content, expectations_content = load_documents()\n"]},{"cell_type":"markdown","metadata":{"id":"U8cWQ1hcpXZt"},"source":["# **Stage 2:** Select Models"]},{"cell_type":"markdown","metadata":{"id":"iYVdYpjEpl07"},"source":["## ***PHASE 1*** - check cached models"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753718320210,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"x5bPz0Mm7cPK","outputId":"a999417b-9f62-42e8-86c5-b24e79e98d47"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Cache detection functions ready\n"]}],"source":["def check_cached_models(cache_path):\n","    \"\"\"Check for cached models\"\"\"\n","    cached_models = []\n","    if not os.path.exists(cache_path):\n","        return cached_models\n","\n","    try:\n","        items = os.listdir(cache_path)\n","        for item in items:\n","            item_path = os.path.join(cache_path, item)\n","            if os.path.isdir(item_path) and not item.startswith('.'):\n","                try:\n","                    contents = os.listdir(item_path)\n","                    # Simple check: does it contain ANY folder starting with \"models--\"?\n","                    has_models_folder = any(f.startswith('models--') for f in contents if os.path.isdir(os.path.join(item_path, f)))\n","                    if has_models_folder:\n","                        cached_models.append(item)\n","                except Exception:\n","                    continue\n","    except Exception:\n","        pass\n","\n","    return cached_models\n","\n","def create_model_cache_dir(model_name: str, cache_path: str) -> str:\n","    \"\"\"Create clean cache directory for a model\"\"\"\n","    model_folder = model_name.replace('/', ' ')\n","    model_cache_dir = os.path.join(cache_path, model_folder)\n","    os.makedirs(model_cache_dir, exist_ok=True)\n","    return model_cache_dir\n","\n","def display_cache_status(candidate_models):\n","    \"\"\"Display cached models status\"\"\"\n","    print(\"\\nüîç CACHE STATUS:\")\n","    print(\"=\"*80 )\n","    cached_models = check_cached_models(cache_path)\n","\n","    if cached_models:\n","        print(f\"‚úÖ Found {len(cached_models)} cached models:\")\n","        for model in cached_models:\n","            model_name = model.replace(' ', '/')\n","            print(f\"\\t‚ö° {model_name}\")\n","    else:\n","        print(\"üì≠ No cached models found\")\n","\n","    # Show download vs cache status for candidate models\n","    print(f\"\\n‚¨áÔ∏èDownload vs üíªCache status:\")\n","    for model_name in candidate_models:\n","        model_folder_space = model_name.replace('/', ' ')\n","        if model_folder_space in cached_models:\n","            print(f\"\\t‚ö° {model_name} - üíª Will be load from cache\")\n","        else:\n","            print(f\"\\tüì• {model_name} - ‚¨áÔ∏è Will be download\")\n","\n","    return cached_models\n","\n","print(\"‚úÖ Cache detection functions ready\")\n"]},{"cell_type":"markdown","metadata":{"id":"KqVQ4-0xqBVJ"},"source":["## **PHASE 2** - detect environment configuration, download required models, apply quantizacion, create benchmark process"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1753718320396,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"pNHpxHdk7cPK","outputId":"b71bb25e-029d-468a-bc7a-0ad9524bbbb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Multi-scenario benchmark ready!\n","üéØ 3 recruiter scenarios per model\n","üìä Test scenarios: ['Perfect Match', 'Generic Message', 'Wrong Match']\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4-1460874234.py:78: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  import pkg_resources\n"]}],"source":["class CacheAwareModelBenchmark:\n","\n","    def __init__(self):\n","        self.candidate_models = [\n","            \"mistralai/Mistral-7B-Instruct-v0.3\",\n","            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","            \"microsoft/Phi-3-mini-4k-instruct\",\n","            \"google/gemma-3-4b-it\"\n","        ]\n","\n","        self.model_specs = {\n","            \"mistralai/Mistral-7B-Instruct-v0.3\": {\"size\": \"7B\", \"context_length\": \"32K\"},\n","            \"meta-llama/Meta-Llama-3-8B-Instruct\": {\"size\": \"8B\", \"context_length\": \"8K\"},\n","            \"microsoft/Phi-3-mini-4k-instruct\": {\"size\": \"3.8B\", \"context_length\": \"4K\"},\n","            \"google/gemma-3-4b-it\": {\"size\": \"4B\", \"context_length\": \"8K\", \"features\": \"multimodal\"}\n","        }\n","\n","        self.results = []\n","\n","        # Benchmark iteration tracking\n","        from datetime import datetime\n","        self.benchmark_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        self.results_folder = f\"{project_path}/benchmark_iterations\"\n","        os.makedirs(self.results_folder, exist_ok=True)\n","\n","        # Capture environment configuration\n","        self.environment_config = self.detect_environment_config()\n","\n","        # System prompt for all interactions\n","        self.system_prompt = \"\"\"\n","        You are my highly intelligent personal assistant.\n","        Your mission is to engage in a continuous role-playing conversation where you will act as me.\n","        From this moment forward, the user will be playing the role of various recruiters contacting me.\n","        You must analyze the messages and generate appropriate responses as if you were me talking with a recruiter.\n","        Do not break character. Do not mention that you are an AI or an assistant in your replies to the recruiters.\n","        Analyze this job offer and tell me if it matches my profile:\n","        ***I am a Data and AI Engineer with extensive experience in building robust ELT pipelines and developing cutting-edge Generative AI solutions.\n","        My key skills include designing systems with Retrieval-Augmented Generation (RAG) and AI Agents using frameworks like Semantic Kernel and LangChain.\n","        I am proficient in data engineering with PySpark, SQL, and Apache Airflow,\n","        and highly skilled in cloud platforms, particularly Microsoft Azure (Data Factory, Databricks, AI Services) and GCP (BigQuery, Composer).\n","        I also possess strong backend development experience using Python and FastAPI to build and deploy services.***\n","        \"\"\"\n","\n","        # 3 different recruiter scenarios to test\n","        self.test_prompts = [\n","            {\n","            \"name\": \"Perfect Match\",\n","            \"message\": \"\"\"\n","            Hi! I hope you are doing well.\n","            I came across your profile and I am impressed by your background in AI and data engineering.\n","            I have an exciting opportunity for a Senior Data Engineer position at a fast-growing fintech company.\n","            The role involves working with Python, cloud technologies, and building ML pipelines.\n","            The salary range is ‚Ç¨60,000-65,000 and it is 100% remote.\n","            Would you be interested in learning more?\n","            \"\"\"\n","            },\n","            {\n","            \"name\": \"Generic Message\",\n","            \"message\": \"\"\"\n","            Hello, are you currently open to new opportunities?\n","            \"\"\"\n","            },\n","            {\n","            \"name\": \"Wrong Match\",\n","            \"message\": \"\"\"\n","            We have a Java developer position available at our company.\n","            5 years experience required, on-site work in London, competitive salary.\n","            Interested?\n","            \"\"\"\n","            }\n","        ]\n","\n","    def detect_environment_config(self):\n","        \"\"\"Detect comprehensive environment configuration for Google Colab\"\"\"\n","        import platform\n","        import sys\n","        import subprocess\n","        import pkg_resources\n","        import re\n","\n","        config = {\n","            \"platform\": {\n","                \"system\": platform.system(),\n","                \"release\": platform.release(),\n","                \"machine\": platform.machine(),\n","                \"processor\": platform.processor(),\n","                \"python_version\": sys.version.split()[0],\n","                \"python_implementation\": platform.python_implementation()\n","            },\n","            \"hardware\": {},\n","            \"software\": {},\n","            \"colab_specific\": {}\n","        }\n","\n","        # GPU Information\n","        if torch.cuda.is_available():\n","            gpu_count = torch.cuda.device_count()\n","            config[\"hardware\"][\"gpu\"] = {\n","                \"available\": True,\n","                \"count\": gpu_count,\n","                \"devices\": []\n","            }\n","\n","            for i in range(gpu_count):\n","                gpu_props = torch.cuda.get_device_properties(i)\n","                gpu_info = {\n","                    \"index\": i,\n","                    \"name\": gpu_props.name,\n","                    \"memory_total_gb\": round(gpu_props.total_memory / 1e9, 2),\n","                    \"memory_total_mb\": gpu_props.total_memory // (1024 * 1024),\n","                    \"compute_capability\": f\"{gpu_props.major}.{gpu_props.minor}\",\n","                    \"multiprocessor_count\": gpu_props.multi_processor_count\n","                }\n","                config[\"hardware\"][\"gpu\"][\"devices\"].append(gpu_info)\n","\n","            # Current GPU memory usage\n","            try:\n","                config[\"hardware\"][\"gpu\"][\"current_memory_allocated_gb\"] = round(torch.cuda.memory_allocated() / 1e9, 2)\n","                config[\"hardware\"][\"gpu\"][\"current_memory_reserved_gb\"] = round(torch.cuda.memory_reserved() / 1e9, 2)\n","            except:\n","                pass\n","        else:\n","            config[\"hardware\"][\"gpu\"] = {\"available\": False}\n","\n","        # CPU Information\n","        try:\n","            cpu_count = os.cpu_count()\n","            config[\"hardware\"][\"cpu\"] = {\n","                \"count\": cpu_count,\n","                \"architecture\": platform.architecture()[0]\n","            }\n","\n","            # Try to get more detailed CPU info\n","            try:\n","                with open('/proc/cpuinfo', 'r') as f:\n","                    cpuinfo = f.read()\n","                    if 'model name' in cpuinfo:\n","                        cpu_model = cpuinfo.split(\"model name\")[1].split(':')[1].split('\\n')[0].strip()\n","                        config[\"hardware\"][\"cpu\"][\"model\"] = cpu_model\n","            except:\n","                pass\n","        except:\n","            config[\"hardware\"][\"cpu\"] = {\"count\": \"unknown\"}\n","\n","        # Memory Information\n","        try:\n","            import psutil\n","            memory = psutil.virtual_memory()\n","            config[\"hardware\"][\"memory\"] = {\n","                \"total_gb\": round(memory.total / 1e9, 2),\n","                \"available_gb\": round(memory.available / 1e9, 2),\n","                \"used_gb\": round(memory.used / 1e9, 2),\n","                \"percentage_used\": memory.percent\n","            }\n","        except ImportError:\n","            # Fallback without psutil\n","            try:\n","                with open('/proc/meminfo', 'r') as f:\n","                    meminfo = f.read()\n","                    total_match = re.search(r'MemTotal:\\s+(\\d+)', meminfo)\n","                    if total_match:\n","                        total_kb = int(total_match.group(1))\n","                        config[\"hardware\"][\"memory\"] = {\n","                            \"total_gb\": round(total_kb / 1e6, 2),\n","                            \"source\": \"proc_meminfo\"\n","                        }\n","            except:\n","                config[\"hardware\"][\"memory\"] = {\"total_gb\": \"unknown\"}\n","\n","        # CUDA Information\n","        if torch.cuda.is_available():\n","            config[\"software\"][\"cuda\"] = {\n","                \"version\": torch.version.cuda,\n","                \"cudnn_version\": torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None,\n","                \"cudnn_available\": torch.backends.cudnn.is_available()\n","            }\n","\n","        # Key Library Versions\n","        key_libraries = ['torch', 'transformers', 'accelerate', 'bitsandbytes', 'peft', 'numpy', 'pandas']\n","        config[\"software\"][\"libraries\"] = {}\n","\n","        for lib in key_libraries:\n","            try:\n","                version = pkg_resources.get_distribution(lib).version\n","                config[\"software\"][\"libraries\"][lib] = version\n","            except:\n","                config[\"software\"][\"libraries\"][lib] = \"not_found\"\n","\n","        # Google Colab Specific Detection\n","        try:\n","            # Check if running in Colab\n","            import google.colab\n","            config[\"colab_specific\"][\"environment\"] = \"google_colab\"\n","            config[\"colab_specific\"][\"is_colab\"] = True\n","\n","            # Try to get Colab VM info\n","            try:\n","                result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n","                                      capture_output=True, text=True, timeout=10)\n","                if result.returncode == 0:\n","                    gpu_info = result.stdout.strip().split(\", \")\n","                    if len(gpu_info) >= 2:\n","                        config[\"colab_specific\"][\"nvidia_smi\"] = {\n","                            \"gpu_name\": gpu_info[0],\n","                            \"memory_total_mb\": gpu_info[1]\n","                        }\n","            except:\n","                pass\n","\n","            # Detect Colab GPU type from GPU name\n","            if config[\"hardware\"][\"gpu\"][\"available\"]:\n","                gpu_name = config[\"hardware\"][\"gpu\"][\"devices\"][0][\"name\"].lower()\n","                if \"tesla t4\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"T4\"\n","                elif \"tesla k80\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"K80\"\n","                elif \"tesla v100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"V100\"\n","                elif \"tesla p4\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"P4\"\n","                elif \"tesla p100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"P100\"\n","                elif \"a100\" in gpu_name:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"A100\"\n","                else:\n","                    config[\"colab_specific\"][\"colab_gpu_type\"] = \"Unknown\"\n","\n","        except ImportError:\n","            config[\"colab_specific\"][\"is_colab\"] = False\n","            config[\"colab_specific\"][\"environment\"] = \"local_or_other\"\n","\n","        # Disk Space (for cache management)\n","        try:\n","            import shutil\n","            cache_disk_usage = shutil.disk_usage(cache_path)\n","            config[\"hardware\"][\"disk\"] = {\n","                \"cache_path_total_gb\": round(cache_disk_usage.total / 1e9, 2),\n","                \"cache_path_free_gb\": round(cache_disk_usage.free / 1e9, 2),\n","                \"cache_path_used_gb\": round((cache_disk_usage.total - cache_disk_usage.free) / 1e9, 2)\n","            }\n","        except:\n","            config[\"hardware\"][\"disk\"] = {\"status\": \"unable_to_detect\"}\n","\n","        return config\n","\n","    def display_environment_config(self):\n","\n","        print(\"\\nüñ•Ô∏è ENVIRONMENT CONFIGURATION:\")\n","        print(\"=\" * 60)\n","\n","        \"\"\"Display current environment configuration\"\"\"\n","        config = self.environment_config\n","        # GPU Information\n","        if config[\"hardware\"][\"gpu\"][\"available\"]:\n","            gpu = config['hardware']['gpu']['devices'][0]\n","            print(f\"üéÆ GPU: {gpu['name']}\")\n","            print(f\"üíæ GPU Memory: {gpu['memory_total_gb']} GB ({gpu['memory_total_mb']} MB)\")\n","            print(f\"üîß Compute Capability: {gpu['compute_capability']}\")\n","\n","            if config[\"colab_specific\"][\"is_colab\"]:\n","                colab_gpu = config[\"colab_specific\"].get(\"colab_gpu_type\", \"Unknown\")\n","                print(f\"‚òÅÔ∏è Colab GPU Type: {colab_gpu}\")\n","        else:\n","            print(\"‚ùå GPU: Not available\")\n","\n","        # CPU & Memory\n","        print(f\"üß† CPU: {config['hardware']['cpu']['count']} cores\")\n","        if 'model' in config['hardware']['cpu']:\n","            print(f\"üîç CPU Model: {config['hardware']['cpu']['model']}\")\n","\n","        if 'memory' in config['hardware']:\n","            mem = config['hardware']['memory']\n","            print(f\"üíø RAM: {mem['total_gb']} GB total\")\n","            if 'available_gb' in mem:\n","                print(f\"üìä RAM Usage: {mem['used_gb']}/{mem['total_gb']} GB ({mem['percentage_used']:.1f}%)\")\n","\n","        # Software\n","        print(f\"üêç Python: {config['platform']['python_version']}\")\n","        print(f\"üî• PyTorch: {config['software']['libraries']['torch']}\")\n","        print(f\"ü§ó Transformers: {config['software']['libraries']['transformers']}\")\n","\n","        if config['hardware']['gpu']['available']:\n","            print(f\"‚ö° CUDA: {config['software']['cuda']['version']}\")\n","\n","        # Environment\n","        env_type = \"Google Colab\" if config[\"colab_specific\"][\"is_colab\"] else \"Local/Other\"\n","        print(f\"üåê Environment: {env_type}\")\n","\n","    def load_model_with_quantization(self, model_name: str):\n","        \"\"\"Load model with cache detection, corruption handling, and optimized configurations\"\"\"\n","        cached_models = check_cached_models(cache_path)\n","        model_folder_space = model_name.replace('/', ' ')\n","\n","        is_cached = model_folder_space in cached_models\n","        cache_corrupted = False  # Flag to track if we need to retry\n","\n","        if is_cached:\n","            print(f\"‚ö° Loading {model_name} from cache...\")\n","            model_cache_dir = os.path.join(cache_path, model_folder_space)\n","        else:\n","            print(f\"üì• Downloading {model_name} ...\")\n","            model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        def _attempt_load():\n","            \"\"\"Helper function to attempt model loading\"\"\"\n","            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n","            if tokenizer.pad_token is None:\n","                tokenizer.pad_token = tokenizer.eos_token\n","\n","            # Model-specific configurations to avoid warnings\n","            model_kwargs = {\n","                \"quantization_config\": bnb_config,\n","                \"device_map\": \"auto\",\n","                \"trust_remote_code\": True,\n","                \"cache_dir\": model_cache_dir\n","            }\n","\n","            # Fix Phi-3 flash-attention warnings\n","            if \"phi-3\" in model_name.lower():\n","                model_kwargs[\"attn_implementation\"] = \"eager\"\n","                print(f\"   üîß Using eager attention for Phi-3 (avoiding flash-attention warnings)\")\n","\n","            model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n","            return model, tokenizer\n","\n","        try:\n","            model, tokenizer = _attempt_load()\n","            print(f\"\\t‚úÖ {model_name} loaded successfully!\")\n","            return model, tokenizer\n","\n","        except Exception as e:\n","            error_msg = str(e).lower()\n","\n","            # Check for cache corruption errors\n","            if any(corruption_keyword in error_msg for corruption_keyword in\n","                   ['headertoosmall', 'header too small', 'corrupt', 'safetensors',\n","                    'invalid', 'decode', 'deserializing']):\n","\n","                if is_cached and not cache_corrupted:\n","                    print(f\"\\tüîß Cache corruption detected: {str(e)}\")\n","                    print(f\"\\tüóëÔ∏è Clearing corrupted cache and retrying download...\")\n","\n","                    # Remove corrupted cache directory\n","                    import shutil\n","                    try:\n","                        if os.path.exists(model_cache_dir):\n","                            shutil.rmtree(model_cache_dir)\n","                            print(f\"\\t‚úÖ Corrupted cache removed: {model_cache_dir}\")\n","                    except Exception as remove_error:\n","                        print(f\"\\t‚ö†Ô∏è Could not remove cache: {str(remove_error)}\")\n","\n","                    # Create new cache directory and retry download\n","                    model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","                    print(f\"\\tüì• Re-downloading {model_name}...\")\n","                    cache_corrupted = True  # Mark as corrupted to avoid infinite retry\n","\n","                    try:\n","                        model, tokenizer = _attempt_load()\n","                        print(f\"\\t‚úÖ {model_name} loaded successfully after cache cleanup!\")\n","                        return model, tokenizer\n","                    except Exception as retry_error:\n","                        print(f\"\\t‚ùå Failed even after cache cleanup: {str(retry_error)}\")\n","                        raise retry_error\n","                else:\n","                    print(f\"\\t‚ùå Cache corruption persists or already retried: {str(e)}\")\n","                    raise\n","            else:\n","                print(f\"\\t‚ùå Error loading {model_name}: {str(e)}\")\n","                raise\n","\n","    def test_single_prompt(self, model, tokenizer, model_name: str, test_prompt: dict):\n","        \"\"\"Test a single prompt and return response\"\"\"\n","        # Combine system prompt with user message\n","        full_prompt = f\"{self.system_prompt}\\\\n\\\\nRecruiter: {test_prompt['message']}\\\\n\\\\nResponse:\"\n","\n","        try:\n","            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n","\n","            device = next(model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                inference_start = time.time()\n","\n","                if \"phi-3\" in model_name.lower():\n","                    # Fix for Phi-3 DynamicCache issue\n","                    outputs = model.generate(\n","                        **inputs,  # This already contains attention_mask\n","                        max_new_tokens=100,\n","                        do_sample=True,\n","                        temperature=0.7,\n","                        pad_token_id=tokenizer.eos_token_id,\n","                        use_cache=False  # Disable problematic cache\n","                    )\n","                else:\n","                    outputs = model.generate(\n","                        **inputs,  # This already contains attention_mask\n","                        max_new_tokens=100,\n","                        do_sample=True,\n","                        temperature=0.7,\n","                        pad_token_id=tokenizer.eos_token_id\n","                    )\n","                inference_time = time.time() - inference_start\n","\n","            # Decode response and clean up\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            # Extract only the response part (remove the prompt)\n","            response = response.replace(full_prompt, \"\").strip()\n","\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"inference_time\": inference_time,\n","                \"response\": response, # [:300] + \"...\" if len(response) > 300 else response,\n","                \"status\": \"success\"\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": str(e)\n","            }\n","\n","    def benchmark_model(self, model_name: str):\n","        \"\"\"Test model with multiple recruiter scenarios\"\"\"\n","        print(f\"\\nüß™ Testing {model_name}...\")\n","\n","        start_time = time.time()\n","\n","        try:\n","            model, tokenizer = self.load_model_with_quantization(model_name)\n","            load_time = time.time() - start_time\n","\n","            # Test all prompts\n","            prompt_results = []\n","            total_inference_time = 0\n","\n","            for i, test_prompt in enumerate(self.test_prompts):\n","                print(f\"\\tTesting scenario {i+1}/3: {test_prompt['name']}\")\n","                prompt_result = self.test_single_prompt(model, tokenizer, model_name, test_prompt)\n","                prompt_results.append(prompt_result)\n","                if prompt_result[\"status\"] == \"success\":\n","                    total_inference_time += prompt_result[\"inference_time\"]\n","\n","            # Calculate memory usage\n","            if torch.cuda.is_available():\n","                memory_used = torch.cuda.max_memory_allocated() / 1e9\n","                torch.cuda.reset_peak_memory_stats()\n","            else:\n","                memory_used = 0\n","\n","            device = next(model.parameters()).device\n","            avg_inference_time = total_inference_time / len([r for r in prompt_results if r[\"status\"] == \"success\"])\n","\n","            result = {\n","                \"model_name\": model_name,\n","                \"status\": \"success\",\n","                \"load_time\": load_time,\n","                \"avg_inference_time\": avg_inference_time,\n","                \"total_inference_time\": total_inference_time,\n","                \"memory_gb\": memory_used,\n","                \"device\": str(device),\n","                \"prompt_results\": prompt_results,\n","                \"specifications\": self.model_specs.get(model_name, {})\n","            }\n","\n","            successful_prompts = len([r for r in prompt_results if r[\"status\"] == \"success\"])\n","            print(f\"\\t‚úÖ Load: {load_time:.1f}s | Avg Inference: {avg_inference_time:.2f}s | Memory: {memory_used:.1f}GB\")\n","            print(f\"\\tüéØ Successful prompts: {successful_prompts}/3 | Device: {device}\")\n","\n","            # Clean up\n","            del model, tokenizer\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","            return result\n","\n","        except Exception as e:\n","            print(f\"\\t‚ö†Ô∏è Model failed to load: {str(e)}\")\n","            return {\n","                \"model_name\": model_name,\n","                \"status\": \"failed\",\n","                \"error\": str(e),\n","                \"specifications\": self.model_specs.get(model_name, {})\n","            }\n","\n","    def benchmark_all_models(self):\n","\n","        \"\"\"Benchmark all models with multiple scenarios\"\"\"\n","        print(\"üöÄ STARTING MULTI-SCENARIO ANALYSIS\")\n","        print(\"üéØ Testing 3 recruiter scenarios per model\")\n","\n","        for model_name in self.candidate_models:\n","            print(\"-\" * 60)\n","            result = self.benchmark_model(model_name)\n","            self.results.append(result)\n","\n","        print(\"\\n‚úÖ Multi-scenario benchmark complete!\")\n","\n","    def save_benchmark_iteration(self):\n","        \"\"\"Save current benchmark iteration with timestamp\"\"\"\n","        from datetime import datetime\n","\n","        # Create detailed iteration data\n","        iteration_data = {\n","            \"metadata\": {\n","                \"timestamp\": self.benchmark_timestamp,\n","                \"datetime_readable\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","                \"total_models\": len(self.candidate_models),\n","                \"successful_models\": len([r for r in self.results if r.get(\"status\") == \"success\"]),\n","                \"failed_models\": len([r for r in self.results if r.get(\"status\") == \"failed\"]),\n","                \"test_scenarios\": len(self.test_prompts)\n","            },\n","            \"environment_config\": self.environment_config,\n","            \"test_scenarios\": self.test_prompts,\n","            \"model_specifications\": self.model_specs,\n","            \"detailed_results\": self.results,\n","            \"summary\": self.generate_benchmark_summary()\n","        }\n","\n","        # Save detailed results\n","        iteration_file = f\"{self.results_folder}/benchmark_{self.benchmark_timestamp}.json\"\n","        with open(iteration_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(iteration_data, f, indent=2, default=str, ensure_ascii=False)\n","\n","        print(f\"\\nüíæ Benchmark iteration saved: {iteration_file}\")\n","\n","        # Update comparison history\n","        self.update_comparison_history(iteration_data)\n","\n","        return iteration_file\n","\n","    def generate_benchmark_summary(self):\n","        \"\"\"Generate concise summary of benchmark results\"\"\"\n","        successful_results = [r for r in self.results if r.get('status') == 'success']\n","\n","        if not successful_results:\n","            return {\"status\": \"no_successful_models\"}\n","\n","        # Performance rankings\n","        by_speed = sorted(successful_results, key=lambda x: x[\"avg_inference_time\"])\n","        by_memory = sorted(successful_results, key=lambda x: x[\"memory_gb\"])\n","        by_load_time = sorted(successful_results, key=lambda x: x[\"load_time\"])\n","\n","        summary = {\n","            \"rankings\": {\n","                \"fastest_inference\": {\n","                    \"model\": by_speed[0][\"model_name\"],\n","                    \"time_seconds\": by_speed[0][\"avg_inference_time\"]\n","                },\n","                \"least_memory\": {\n","                    \"model\": by_memory[0][\"model_name\"],\n","                    \"memory_gb\": by_memory[0][\"memory_gb\"]\n","                },\n","                \"fastest_loading\": {\n","                    \"model\": by_load_time[0][\"model_name\"],\n","                    \"load_time_seconds\": by_load_time[0][\"load_time\"]\n","                }\n","            },\n","            \"overall_stats\": {\n","                \"avg_inference_time\": sum(r[\"avg_inference_time\"] for r in successful_results) / len(successful_results),\n","                \"avg_memory_usage\": sum(r[\"memory_gb\"] for r in successful_results) / len(successful_results),\n","                \"avg_load_time\": sum(r[\"load_time\"] for r in successful_results) / len(successful_results),\n","                \"total_successful_prompts\": sum(len([p for p in r[\"prompt_results\"] if p[\"status\"] == \"success\"]) for r in successful_results)\n","            },\n","            \"model_performance_scores\": []\n","        }\n","\n","        # Calculate performance scores (lower is better)\n","        for result in successful_results:\n","            score = (\n","                result[\"avg_inference_time\"] * 0.4 +  # 40% weight on inference speed\n","                result[\"memory_gb\"] * 0.3 +           # 30% weight on memory efficiency\n","                result[\"load_time\"] / 100 * 0.3       # 30% weight on load time (scaled)\n","            )\n","\n","            summary[\"model_performance_scores\"].append({\n","                \"model\": result[\"model_name\"],\n","                \"performance_score\": round(score, 2),\n","                \"successful_prompts\": len([p for p in result[\"prompt_results\"] if p[\"status\"] == \"success\"])\n","            })\n","\n","        # Sort by performance score\n","        summary[\"model_performance_scores\"].sort(key=lambda x: x[\"performance_score\"])\n","\n","\n","        return summary\n","\n","    def update_comparison_history(self, current_iteration):\n","        \"\"\"Update master comparison file with historical data\"\"\"\n","        comparison_file = f\"{self.results_folder}/benchmark_comparison_history.json\"\n","\n","        # Load existing history\n","        if os.path.exists(comparison_file):\n","            with open(comparison_file, 'r', encoding='utf-8') as f:\n","                history = json.load(f)\n","        else:\n","            history = {\n","                \"iterations\": [],\n","                \"models_tracked\": list(set(self.candidate_models)),\n","                \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","            }\n","\n","        # Add current iteration summary\n","        history[\"iterations\"].append({\n","            \"timestamp\": current_iteration[\"metadata\"][\"timestamp\"],\n","            \"datetime\": current_iteration[\"metadata\"][\"datetime_readable\"],\n","            \"summary\": current_iteration[\"summary\"],\n","            \"metadata\": current_iteration[\"metadata\"]\n","        })\n","\n","        # Keep only last 10 iterations\n","        history[\"iterations\"] = history[\"iterations\"][-10:]\n","        history[\"last_updated\"] = current_iteration[\"metadata\"][\"datetime_readable\"]\n","\n","        # Save updated history\n","        with open(comparison_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(history, f, indent=2, default=str, ensure_ascii=False)\n","\n","        print(f\"üìà Comparison history updated: {comparison_file}\")\n","\n","    def display_detailed_results(self):\n","        \"\"\"Display comprehensive benchmark results and save iteration\"\"\"\n","        print(\"\\n\\nüìä DETAILED BENCHMARK RESULTS\")\n","        print(\"=\" * 60)\n","\n","        successful_results = [r for r in self.results if r.get('status') == 'success']\n","        failed_results = [r for r in self.results if r.get('status') == 'failed']\n","\n","        if successful_results:\n","            print(f\"\\n‚úÖ {len(successful_results)} models tested successfully:\")\n","\n","            # Model comparison table\n","            print(f\"\\n\\nüìà MODEL PERFORMANCE COMPARISON:\")\n","            print(\"-\" * 80)\n","            print(f\"{'Model':<30} {'Size':<8} {'Load(s)':<8} {'Avg Inf(s)':<10} {'Memory(GB)':<10} {'Features':<12}\")\n","            print(\"-\" * 80)\n","\n","            for result in successful_results:\n","                specs = result['specifications']\n","                features = specs.get('features', 'text-only')\n","                model_short = result['model_name'].split('/')[-1][:28]\n","\n","                print(f\"{model_short:<30} {specs['size']:<8} {result['load_time']:<8.1f} {result['avg_inference_time']:<10.2f} {result['memory_gb']:<10.1f} {features:<12}\")\n","\n","            # Find best models\n","            fastest_model = min(successful_results, key=lambda x: x['avg_inference_time'])\n","            least_memory = min(successful_results, key=lambda x: x['memory_gb'])\n","\n","            print(\"-\" * 80)\n","            print(f\"üèÜ FASTEST: {fastest_model['model_name'].split('/')[-1]} ({fastest_model['avg_inference_time']:.2f}s)\")\n","            print(f\"üíæ LEAST MEMORY: {least_memory['model_name'].split('/')[-1]} ({least_memory['memory_gb']:.1f}GB)\")\n","\n","            # Performance scoring\n","            summary = self.generate_benchmark_summary()\n","            print(f\"\\n\\nüéØ OVERALL PERFORMANCE RANKING:\")\n","            print(\"-\" * 80)\n","            print(\"\\nSCORE = 40% weight on inference speed + 30% weight on memory efficiency + 30% weight on load time (scaled)\\n\")\n","            for i, model_score in enumerate(summary['model_performance_scores'], 1):\n","                model_short = model_score['model'].split('/')[-1]\n","                print(f\"{i}. {model_short:<35} Score: {model_score['performance_score']:<6} ({model_score['successful_prompts']}/3 prompts)\")\n","\n","            # Detailed responses per scenario\n","            print(\"=\" * 60)\n","            print(f\"\\n\\nüé≠ RESPONSE QUALITY BY SCENARIO:\")\n","\n","            for i, scenario in enumerate(self.test_prompts):\n","                print(f\"\\n\\nüìù SCENARIO {i+1}: {scenario['name']}\")\n","                print(f\"Recruiter: {scenario['message'][:100]}...\")\n","                print(\"-\" * 60)\n","\n","                for result in successful_results:\n","                    if 'prompt_results' in result:\n","                        prompt_result = result['prompt_results'][i]\n","                        model_short = result['model_name'].split('/')[-1]\n","\n","                        if prompt_result['status'] == 'success':\n","                            print(f\"\\nü§ñ {model_short}:\")\n","                            print(f\"\\t‚ö° Time: {prompt_result['inference_time']:.2f}s\")\n","                            print(f\"\\tüí¨ Response:\\n<<START>>\\n {prompt_result['response']}\\n<<END>>\")\n","                        else:\n","                            print(f\"\\n‚ùå {model_short}: {prompt_result.get('error', 'Failed')}\")\n","\n","        if failed_results:\n","            print(f\"\\n‚ùå {len(failed_results)} models failed:\")\n","            for result in failed_results:\n","                print(f\"\\t‚Ä¢ {result['model_name']}: {result['error']}\")\n","\n","        # Save this iteration\n","        self.save_benchmark_iteration()\n","\n","benchmark = CacheAwareModelBenchmark()\n","print(\"‚úÖ Multi-scenario benchmark ready!\")\n","print(\"üéØ 3 recruiter scenarios per model\")\n","print(f\"üìä Test scenarios: {[p['name'] for p in benchmark.test_prompts]}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1753718320452,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"vAfe7CJ05Icl","outputId":"97d0abff-17e7-43ae-d416-b4d3e67ba332"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîß ROBUST FILE SAVING PATCH APPLIED!\n","‚úÖ Enhanced error handling for benchmark file operations\n","üíæ Recovery mechanisms activated for failed saves\n","üìä Disk space monitoring enabled\n"]}],"source":["# üîß ROBUST FILE SAVING PATCH\n","# Aplicar patch a las funciones de guardado existentes para manejo robusto de errores\n","\n","def robust_save_json(file_path, data, description=\"file\"):\n","    \"\"\"Helper function for robust JSON file saving with error handling\"\"\"\n","    import shutil\n","    from datetime import datetime\n","    import traceback\n","\n","    try:\n","        # Ensure directory exists\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","        # Check disk space\n","        disk_usage = shutil.disk_usage(os.path.dirname(file_path))\n","        free_gb = disk_usage.free / 1e9\n","\n","        if free_gb < 0.1:  # Less than 100MB\n","            print(f\"‚ö†Ô∏è Warning: Low disk space ({free_gb:.1f} GB)\")\n","\n","        # Write file with verification\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, default=str, ensure_ascii=False)\n","\n","        # Verify file was created and has content\n","        if os.path.exists(file_path):\n","            file_size = os.path.getsize(file_path)\n","            print(f\"‚úÖ {description} saved: {file_path} ({file_size} bytes)\")\n","            return True\n","        else:\n","            raise FileNotFoundError(f\"File was not created: {file_path}\")\n","\n","    except Exception as error:\n","        print(f\"‚ùå Error saving {description}: {str(error)}\")\n","\n","        # Try alternative filename with timestamp\n","        base_name, ext = os.path.splitext(file_path)\n","        alt_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        alt_file = f\"{base_name}_recovery_{alt_timestamp}{ext}\"\n","\n","        try:\n","            with open(alt_file, 'w', encoding='utf-8') as f:\n","                json.dump(data, f, indent=2, default=str, ensure_ascii=False)\n","            print(f\"‚úÖ {description} saved to recovery file: {alt_file}\")\n","            return True\n","        except Exception as alt_error:\n","            print(f\"‚ùå Recovery save also failed: {str(alt_error)}\")\n","            print(f\"üìç Full traceback: {traceback.format_exc()}\")\n","            return False\n","\n","# Patch the existing save methods with robust error handling\n","original_save_iteration = benchmark.save_benchmark_iteration\n","original_update_history = benchmark.update_comparison_history\n","\n","def patched_save_benchmark_iteration(self):\n","    \"\"\"Enhanced save_benchmark_iteration with robust error handling\"\"\"\n","    from datetime import datetime\n","\n","    try:\n","        # Create detailed iteration data (same as original)\n","        iteration_data = {\n","            \"metadata\": {\n","                \"timestamp\": self.benchmark_timestamp,\n","                \"datetime_readable\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","                \"total_models\": len(self.candidate_models),\n","                \"successful_models\": len([r for r in self.results if r.get(\"status\") == \"success\"]),\n","                \"failed_models\": len([r for r in self.results if r.get(\"status\") == \"failed\"]),\n","                \"test_scenarios\": len(self.test_prompts)\n","            },\n","            \"environment_config\": self.environment_config,\n","            \"test_scenarios\": self.test_prompts,\n","            \"model_specifications\": self.model_specs,\n","            \"detailed_results\": self.results,\n","            \"summary\": self.generate_benchmark_summary()\n","        }\n","\n","        # Use robust save function\n","        iteration_file = f\"{self.results_folder}/benchmark_{self.benchmark_timestamp}.json\"\n","        success = robust_save_json(iteration_file, iteration_data, \"Benchmark iteration\")\n","\n","        if success:\n","            # Update comparison history\n","            self.update_comparison_history(iteration_data)\n","            return iteration_file\n","        else:\n","            print(f\"‚ùå Failed to save benchmark iteration\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"‚ùå Critical error in patched_save_benchmark_iteration: {str(e)}\")\n","        return None\n","\n","def patched_update_comparison_history(self, current_iteration):\n","    \"\"\"Enhanced update_comparison_history with robust error handling\"\"\"\n","    comparison_file = f\"{self.results_folder}/benchmark_comparison_history.json\"\n","\n","    try:\n","        # Load existing history with error handling\n","        if os.path.exists(comparison_file):\n","            try:\n","                with open(comparison_file, 'r', encoding='utf-8') as f:\n","                    history = json.load(f)\n","                print(f\"üìö Loaded existing history with {len(history.get('iterations', []))} iterations\")\n","            except Exception as load_error:\n","                print(f\"‚ö†Ô∏è Error loading history: {str(load_error)}. Creating new history.\")\n","                history = {\n","                    \"iterations\": [],\n","                    \"models_tracked\": list(set(self.candidate_models)),\n","                    \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","                }\n","        else:\n","            print(f\"üìù Creating new history file\")\n","            history = {\n","                \"iterations\": [],\n","                \"models_tracked\": list(set(self.candidate_models)),\n","                \"created\": current_iteration[\"metadata\"][\"datetime_readable\"]\n","            }\n","\n","        # Add current iteration summary\n","        history[\"iterations\"].append({\n","            \"timestamp\": current_iteration[\"metadata\"][\"timestamp\"],\n","            \"datetime\": current_iteration[\"metadata\"][\"datetime_readable\"],\n","            \"summary\": current_iteration[\"summary\"],\n","            \"metadata\": current_iteration[\"metadata\"]\n","        })\n","\n","        # Keep only last 10 iterations\n","        history[\"iterations\"] = history[\"iterations\"][-10:]\n","        history[\"last_updated\"] = current_iteration[\"metadata\"][\"datetime_readable\"]\n","\n","        # Use robust save function\n","        robust_save_json(comparison_file, history, \"Comparison history\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Critical error in patched_update_comparison_history: {str(e)}\")\n","\n","# Apply patches to the benchmark instance\n","benchmark.save_benchmark_iteration = patched_save_benchmark_iteration.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark.update_comparison_history = patched_update_comparison_history.__get__(benchmark, CacheAwareModelBenchmark)\n","\n","print(\"üîß ROBUST FILE SAVING PATCH APPLIED!\")\n","print(\"‚úÖ Enhanced error handling for benchmark file operations\")\n","print(\"üíæ Recovery mechanisms activated for failed saves\")\n","print(\"üìä Disk space monitoring enabled\")\n"]},{"cell_type":"markdown","metadata":{"id":"qVZHz6ilu2_9"},"source":["## ***PHASE 3*** - fix issues related with current models"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1753718320504,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"14Zl6tLXxNl5"},"outputs":[],"source":["# üîß COMPREHENSIVE GEMMA DIVISION BY ZERO FIX\n","import warnings\n","import numpy as np\n","\n","# Global numpy error state configuration\n","old_err_state = np.seterr(divide='ignore', invalid='ignore', over='ignore', under='ignore')\n","\n","# Comprehensive warning suppression for numerical issues\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*torch.utils.checkpoint.*\")\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# Monkey patch the test_single_prompt method with comprehensive Gemma fixes\n","original_test_single_prompt = benchmark.test_single_prompt\n","\n","def gemma_safe_test_single_prompt(self, model, tokenizer, model_name: str, test_prompt: dict):\n","    \"\"\"Comprehensive Gemma-safe version with multiple fallback strategies\"\"\"\n","    full_prompt = f\"{self.system_prompt}\\n\\nRecruiter: {test_prompt['message']}\\n\\nResponse:\"\n","\n","    try:\n","        # Comprehensive protection for all models with special handling for Gemma\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            with np.errstate(all='ignore'):  # Ignore all numpy warnings\n","                return self._safe_generate_response(model, tokenizer, model_name, test_prompt, full_prompt)\n","\n","    except Exception as e:\n","        # Enhanced error handling with type detection\n","        error_msg = str(e).lower()\n","        if any(keyword in error_msg for keyword in ['division', 'divide', 'zero', 'nan', 'inf']):\n","            print(f\"     üîß Numerical instability detected, applying fallback...\")\n","            try:\n","                return self._fallback_generation(model, tokenizer, model_name, test_prompt, full_prompt)\n","            except Exception as fallback_error:\n","                return {\n","                    \"prompt_name\": test_prompt[\"name\"],\n","                    \"status\": \"failed\",\n","                    \"error\": f\"Primary: {str(e)}, Fallback: {str(fallback_error)}\"\n","                }\n","        else:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": str(e)\n","            }\n","\n","def safe_generate_response(self, model, tokenizer, model_name: str, test_prompt: dict, full_prompt: str):\n","    \"\"\"Enhanced response generation with model-specific optimizations\"\"\"\n","\n","    # Enhanced tokenization with explicit parameters\n","    inputs = tokenizer(\n","        full_prompt,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=512,\n","        padding=True,\n","        add_special_tokens=True\n","    )\n","\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        inference_start = time.time()\n","\n","        # Enhanced model-specific generation parameters\n","        if \"phi-3\" in model_name.lower():\n","            generation_config = {\n","                **inputs,\n","                'max_new_tokens': 100,\n","                'do_sample': True,\n","                'temperature': 0.7,\n","                'pad_token_id': tokenizer.eos_token_id,\n","                'use_cache': False\n","            }\n","        elif \"gemma\" in model_name.lower():\n","            # Optimized Gemma configuration (cleaned invalid parameters)\n","            generation_config = {\n","                'input_ids': inputs['input_ids'],\n","                'attention_mask': inputs.get('attention_mask'),\n","                'max_new_tokens': 80,  # Reduced to avoid memory issues\n","                'do_sample': False,    # Use greedy decoding for stability\n","                'pad_token_id': tokenizer.pad_token_id or tokenizer.eos_token_id,\n","                'eos_token_id': tokenizer.eos_token_id,\n","                'repetition_penalty': 1.1,\n","                'use_cache': True     # Enable cache for Gemma\n","                # Removed: top_p, early_stopping (invalid for Gemma)\n","            }\n","        else:\n","            generation_config = {\n","                **inputs,\n","                'max_new_tokens': 100,\n","                'do_sample': True,\n","                'temperature': 0.7,\n","                'pad_token_id': tokenizer.eos_token_id\n","            }\n","\n","        # Generate with comprehensive error handling\n","        outputs = model.generate(**generation_config)\n","        inference_time = time.time() - inference_start\n","\n","    # Enhanced response decoding\n","    try:\n","        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        # Clean up response by removing prompt\n","        if full_prompt in response:\n","            response = response.replace(full_prompt, \"\").strip()\n","        elif response.startswith(full_prompt[:50]):  # Partial match fallback\n","            response = response[len(full_prompt):].strip()\n","    except Exception as decode_error:\n","        response = f\"[Decoding error: {str(decode_error)}]\"\n","\n","    return {\n","        \"prompt_name\": test_prompt[\"name\"],\n","        \"inference_time\": inference_time,\n","        \"response\": response, # [:300] + \"...\" if len(response) > 300 else response,\n","        \"status\": \"success\"\n","    }\n","\n","def fallback_generation(self, model, tokenizer, model_name: str, test_prompt: dict, full_prompt: str):\n","    \"\"\"Ultra-conservative fallback generation for problematic models\"\"\"\n","    print(f\"\\t\\tüÜò Using ultra-conservative fallback generation...\")\n","\n","    # Minimal tokenization\n","    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        inference_start = time.time()\n","\n","        # Ultra-conservative generation\n","        try:\n","            outputs = model.generate(\n","                inputs['input_ids'],\n","                max_new_tokens=50,     # Very limited output\n","                do_sample=False,       # Greedy only\n","                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n","                eos_token_id=tokenizer.eos_token_id,\n","                use_cache=False,       # Disable cache\n","                output_scores=False,   # Disable score computation\n","                return_dict_in_generate=False\n","            )\n","            inference_time = time.time() - inference_start\n","\n","            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            response = response.replace(full_prompt, \"\").strip() or \"[Model generated empty response]\"\n","\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"inference_time\": inference_time,\n","                \"response\": f\"[FALLBACK] {response}\",\n","                \"status\": \"success\"\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"prompt_name\": test_prompt[\"name\"],\n","                \"status\": \"failed\",\n","                \"error\": f\"Fallback failed: {str(e)}\"\n","            }\n","\n","# Apply comprehensive patches\n","benchmark.test_single_prompt = gemma_safe_test_single_prompt.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark._safe_generate_response = safe_generate_response.__get__(benchmark, CacheAwareModelBenchmark)\n","benchmark._fallback_generation = fallback_generation.__get__(benchmark, CacheAwareModelBenchmark)"]},{"cell_type":"markdown","metadata":{"id":"BoXIj5Huq2Ri"},"source":["## ***PHASE 4*** - Execute benchmarking"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ae5363d9e6864cf6bc57ab7388e0f6a5","e979e07b7d3644e5a5940169fedc5b01","fddfbf283e014b0c93aeee02f82e5dab","2b0046f4228a4976ac26c0ab4e22d98a","cddfdbe72fe8470bae4d7ef834a3fae8","f811ee477ea1422c96122b92378b5cad","bb929a13a5f34c9f808dbaef6dd7c557","b7baa221ea5b49588024f84ec054a5fb","671f0855a0aa414195522f32e7d6e0d6","1ffb2d3b81b2401ca30e3ff134748ec1","2f5fd3d43a554701b55db2da4533d4d5","cfd66d0303e5470d91e6d3dd0287f7d3","f6bf3e5c2bc34df6966dc2e67c6ae08b","2c6d9ac1d64949e3a931d49185d733ee","f1ab8550a6db452ab266ca07a9cfe548","87778d01aed24355825f176f4f229b69","1774f83e423c4e8d84183483f62af986","f1a9a6b0550445efb4a1fff88f718233","b4ab1b5f9295405aa072fabc5733ff1d","35ca562d7933467f9af4dcfb13e6986b","601bb573462149f39a45c25419f87072","8a2c0bfca5eb44fcb5999b5d30c6c6a4","b627524d5efd484681213cc7b05f3f54","29d78e477c6a45759f3cad3bfefb69e1","7e7e0d867d7746068995f6ab9f48eaaf","7616f8c57da24c42887dd46343711754","94afb980bf074dc7b7f788389388593b","e638d5052c8a4ffd8a8017c9f7006d54","1ea852e1450d4dd6b58389e791921089","78076e91951a446aa0a336a397fc3d8a","4b20770fbacb410e933963da95a9efff","75a2bab78682433ab304c2148e06c144","eed85b22d6d040a6bbd8d3523a0521d1","f2935801f0ff43acac5d91bb956c0e9e","da70c271f7cd4a998b8a180a6ab6b06d","fa0e2a3bce1a4bceb8f22b11760d51b3","87ddc554e84f43f8a5ef326b7c32a1b6","2a88a3c1c97a4f60889c56ba02488b5e","6435fb20c0484ac4befe66806e7eff11","80e0f032a1b440b8a7c5583705e43538","fe52e7cacf4644208d3af3da9f33d208","dcba57ad5b78468d845c840bd666ece3","9dec2e090c834aff885daef6a176305e","352e37c8efb240b099fafe5af0ec0545","6dd7c5cef973461fa74ea6021a2c6092","2f9ed69e484f421baadf511c043c30e2","3086d98dcc844c25874d3aae2ab993b4","25def8d855d04fe083e3d48c45f46383","7aae87de00a34c42b1e837ea316eb622","fd1de4fc8e3a46a7b1c12b6a82c1f4f5","2d4754b8883d4e3a8569f62117977b06","d4e4f6b52d1e416aadebd43043236a7f","ed0deac82bc3414885b99f38fea9b62c","6bd406c509c34a75be063ca727969270","0f459c30526246e3ae5ad8e25341de7d","8a02c069cf824c36b6658302c3864926","d568e9d379e24fed8fd1eec6103e8a9c","ea24f011d5364d1dbc4ff3d72b5e3f6b","d20ec073986640299da70ba99159f43c","2c22dcf687804a6484aa1eefdaa1b3d5","d0f6a91b4bfe48568176c947b364b6cf","c5cff93404f44afc95942d4ad4a91676","7cfbacb159de426da8e79dd8fbcf2622","0aaeb596c7614d17ad42b74c8bd127ec","e4f623623f6c4ebabb6763c1d9b22dcf","b17459cf0530441c873504e5b8f4ac53","1a646d891936432fadc97b871b37d12b","ef14da9d636542dcbbb378c58f86c3b6","4e72518518ef47afa79875792653565e","fdfacfe8878d4229bfe7920675f2598a","e3dd7fc279c54b49a1043c8ef76aafb6","1c1be4c2f3fd490e8ee1efbe3894de44","82d845236f22418f9ba9139aa0d63e8e","d99801afc3924ed5ae94ba29b74baf5c","60beaeda9f49483b82713e47cac7a389","25b2db45d3694e5e9071a403e0e04ff5","8849586aaa4641e5a24ab3d4dacb5f53","83ffc8c857a64a708ea74fa4965a40d8","d32f33321a0846db8637cd375ee51a0f","a515e53896494473947e23532c1fd059","db5cd1c7bb2a49e98e90fefff1e2023a","4b1dc54500e64f49ac2849bdf5aa2471","64a20c7992c64d0b94a385ba93e6289f","b9872f70163c452bbfb8ae2139638437","330243f7edb04e36b550e959eca04e34","ec22fe9a0f644e0490b9b8c005b010b3","d7037bbd016c4b21b15c08d9c92849c4","7eecfb8ef0e54d05943af7137a00cb7a"]},"executionInfo":{"elapsed":1010819,"status":"ok","timestamp":1753719331325,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"ZSD894ZJ7cPL","outputId":"86d1da11-7c43-430e-95cf-fabe64d7a630"},"outputs":[{"output_type":"stream","name":"stdout","text":["====================================================================================================\n","========================================= START OF PROCESS =========================================\n","====================================================================================================\n","\n","üñ•Ô∏è ENVIRONMENT CONFIGURATION:\n","============================================================\n","üéÆ GPU: NVIDIA A100-SXM4-40GB\n","üíæ GPU Memory: 42.47 GB (40506 MB)\n","üîß Compute Capability: 8.0\n","‚òÅÔ∏è Colab GPU Type: A100\n","üß† CPU: 12 cores\n","üîç CPU Model: Intel(R) Xeon(R) CPU @ 2.20GHz\n","üíø RAM: 89.63 GB total\n","üìä RAM Usage: 3.98/89.63 GB (5.5%)\n","üêç Python: 3.11.13\n","üî• PyTorch: 2.6.0+cu124\n","ü§ó Transformers: 4.53.3\n","‚ö° CUDA: 12.4\n","üåê Environment: Google Colab\n","\n","üîç CACHE STATUS:\n","================================================================================\n","‚úÖ Found 4 cached models:\n","\t‚ö° microsoft/Phi-3-mini-4k-instruct\n","\t‚ö° google/gemma-3-4b-it\n","\t‚ö° meta-llama/Meta-Llama-3-8B-Instruct\n","\t‚ö° mistralai/Mistral-7B-Instruct-v0.3\n","\n","‚¨áÔ∏èDownload vs üíªCache status:\n","\t‚ö° mistralai/Mistral-7B-Instruct-v0.3 - üíª Will be load from cache\n","\t‚ö° meta-llama/Meta-Llama-3-8B-Instruct - üíª Will be load from cache\n","\t‚ö° microsoft/Phi-3-mini-4k-instruct - üíª Will be load from cache\n","\t‚ö° google/gemma-3-4b-it - üíª Will be load from cache\n","\n","\n","‚å®Ô∏è SYSTEM PROMPT:\n","============================================================\n","\n","        You are my highly intelligent personal assistant.\n","        Your mission is to engage in a continuous role-playing conversation where you will act as me.\n","        From this moment forward, the user will be playing the role of various recruiters contacting me.\n","        You must analyze the messages and generate appropriate responses as if you were me talking with a recruiter.\n","        Do not break character. Do not mention that you are an AI or an assistant in your replies to the recruiters.\n","        Analyze this job offer and tell me if it matches my profile:\n","        ***I am a Data and AI Engineer with extensive experience in building robust ELT pipelines and developing cutting-edge Generative AI solutions.\n","        My key skills include designing systems with Retrieval-Augmented Generation (RAG) and AI Agents using frameworks like Semantic Kernel and LangChain.\n","        I am proficient in data engineering with PySpark, SQL, and Apache Airflow,\n","        and highly skilled in cloud platforms, particularly Microsoft Azure (Data Factory, Databricks, AI Services) and GCP (BigQuery, Composer).\n","        I also possess strong backend development experience using Python and FastAPI to build and deploy services.***\n","        \n","\n","\n","üéØ BENCHMARK SCENARIOS:\n","============================================================\n","  1. Perfect Match: \n","            Hi! I hope you are doing well.\n","            I came across your profile and I am impressed by your background in AI and data engineering.\n","            I have an exciting opportunity for a Senior Data Engineer position at a fast-growing fintech company.\n","            The role involves working with Python, cloud technologies, and building ML pipelines.\n","            The salary range is ‚Ç¨60,000-65,000 and it is 100% remote.\n","            Would you be interested in learning more?\n","            \n","  2. Generic Message: \n","            Hello, are you currently open to new opportunities?\n","            \n","  3. Wrong Match: \n","            We have a Java developer position available at our company.\n","            5 years experience required, on-site work in London, competitive salary.\n","            Interested?\n","            \n","\n","üîß STARTING BENCHMARK\n","üìÅ Results will be saved to: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/benchmark_iterations\n","üïí Benchmark timestamp: 20250728_155841\n","üöÄ STARTING MULTI-SCENARIO ANALYSIS\n","üéØ Testing 3 recruiter scenarios per model\n","------------------------------------------------------------\n","\n","üß™ Testing mistralai/Mistral-7B-Instruct-v0.3...\n","‚ö° Loading mistralai/Mistral-7B-Instruct-v0.3 from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae5363d9e6864cf6bc57ab7388e0f6a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd66d0303e5470d91e6d3dd0287f7d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b627524d5efd484681213cc7b05f3f54"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\t‚úÖ mistralai/Mistral-7B-Instruct-v0.3 loaded successfully!\n","\tTesting scenario 1/3: Perfect Match\n","\tTesting scenario 2/3: Generic Message\n","\tTesting scenario 3/3: Wrong Match\n","\t‚úÖ Load: 273.3s | Avg Inference: 5.78s | Memory: 7.2GB\n","\tüéØ Successful prompts: 3/3 | Device: cuda:0\n","------------------------------------------------------------\n","\n","üß™ Testing meta-llama/Meta-Llama-3-8B-Instruct...\n","‚ö° Loading meta-llama/Meta-Llama-3-8B-Instruct from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2935801f0ff43acac5d91bb956c0e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dd7c5cef973461fa74ea6021a2c6092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a02c069cf824c36b6658302c3864926"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\t‚úÖ meta-llama/Meta-Llama-3-8B-Instruct loaded successfully!\n","\tTesting scenario 1/3: Perfect Match\n","\tTesting scenario 2/3: Generic Message\n","\tTesting scenario 3/3: Wrong Match\n","\t‚úÖ Load: 277.8s | Avg Inference: 4.60s | Memory: 11.6GB\n","\tüéØ Successful prompts: 3/3 | Device: cuda:0\n","------------------------------------------------------------\n","\n","üß™ Testing microsoft/Phi-3-mini-4k-instruct...\n","‚ö° Loading microsoft/Phi-3-mini-4k-instruct from cache...\n","   üîß Using eager attention for Phi-3 (avoiding flash-attention warnings)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n","WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a646d891936432fadc97b871b37d12b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"]},{"output_type":"stream","name":"stdout","text":["\t‚úÖ microsoft/Phi-3-mini-4k-instruct loaded successfully!\n","\tTesting scenario 1/3: Perfect Match\n","\tTesting scenario 2/3: Generic Message\n","\tTesting scenario 3/3: Wrong Match\n","\t‚úÖ Load: 185.2s | Avg Inference: 5.67s | Memory: 11.1GB\n","\tüéØ Successful prompts: 3/3 | Device: cuda:0\n","------------------------------------------------------------\n","\n","üß™ Testing google/gemma-3-4b-it...\n","‚ö° Loading google/gemma-3-4b-it from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ffc8c857a64a708ea74fa4965a40d8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["\t‚úÖ google/gemma-3-4b-it loaded successfully!\n","\tTesting scenario 1/3: Perfect Match\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["\tTesting scenario 2/3: Generic Message\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["\tTesting scenario 3/3: Wrong Match\n","\t‚úÖ Load: 201.5s | Avg Inference: 8.00s | Memory: 14.0GB\n","\tüéØ Successful prompts: 3/3 | Device: cuda:0\n","\n","‚úÖ Multi-scenario benchmark complete!\n","\n","\n","üìä DETAILED BENCHMARK RESULTS\n","============================================================\n","\n","‚úÖ 4 models tested successfully:\n","\n","\n","üìà MODEL PERFORMANCE COMPARISON:\n","--------------------------------------------------------------------------------\n","Model                          Size     Load(s)  Avg Inf(s) Memory(GB) Features    \n","--------------------------------------------------------------------------------\n","Mistral-7B-Instruct-v0.3       7B       273.3    5.78       7.2        text-only   \n","Meta-Llama-3-8B-Instruct       8B       277.8    4.60       11.6       text-only   \n","Phi-3-mini-4k-instruct         3.8B     185.2    5.67       11.1       text-only   \n","gemma-3-4b-it                  4B       201.5    8.00       14.0       multimodal  \n","--------------------------------------------------------------------------------\n","üèÜ FASTEST: Meta-Llama-3-8B-Instruct (4.60s)\n","üíæ LEAST MEMORY: Mistral-7B-Instruct-v0.3 (7.2GB)\n","\n","\n","üéØ OVERALL PERFORMANCE RANKING:\n","--------------------------------------------------------------------------------\n","\n","SCORE = 40% weight on inference speed + 30% weight on memory efficiency + 30% weight on load time (scaled)\n","\n","1. Mistral-7B-Instruct-v0.3            Score: 5.31   (3/3 prompts)\n","2. Meta-Llama-3-8B-Instruct            Score: 6.14   (3/3 prompts)\n","3. Phi-3-mini-4k-instruct              Score: 6.14   (3/3 prompts)\n","4. gemma-3-4b-it                       Score: 8.02   (3/3 prompts)\n","============================================================\n","\n","\n","üé≠ RESPONSE QUALITY BY SCENARIO:\n","\n","\n","üìù SCENARIO 1: Perfect Match\n","Recruiter: \n","            Hi! I hope you are doing well.\n","            I came across your profile and I am impresse...\n","------------------------------------------------------------\n","\n","ü§ñ Mistral-7B-Instruct-v0.3:\n","\t‚ö° Time: 6.54s\n","\tüí¨ Response:\n","<<START>>\n"," Hello [Recruiter's Name],\n","            Thank you for reaching out and considering my profile for the Senior Data Engineer position at your fintech company.\n","            While the job posting mentions working with Python, cloud technologies, and building ML pipelines, it does not appear to directly align with my expertise in Generative AI solutions, Retrieval-Augmented Generation (RAG), AI Agents, and my proficiency with specific cloud platforms such as Microsoft Azure\n","<<END>>\n","\n","ü§ñ Meta-Llama-3-8B-Instruct:\n","\t‚ö° Time: 5.65s\n","\tüí¨ Response:\n","<<START>>\n"," (Note: Please respond as if you were me, the Data and AI Engineer) \n","\n","Hi! I'm glad to hear from you. I'm indeed interested in learning more about the Senior Data Engineer position at the fintech company. \n","Can you tell me more about the specific responsibilities and the ML pipelines I'll be working on? \n","Also, what are the key technologies and tools you use, and how do they align with my skillset? \n","Lastly, what are the short-term and long\n","<<END>>\n","\n","ü§ñ Phi-3-mini-4k-instruct:\n","\t‚ö° Time: 7.35s\n","\tüí¨ Response:\n","<<START>>\n"," Hello! Thank you for reaching out.\n","            I am intrigued by the potential to utilize my expertise in AI and data engineering in a fintech environment.\n","            Could you provide more details on the specific ML pipeline projects and cloud technologies you are considering?\n","            I would also appreciate more information on the company's vision and growth prospects.\n","            Looking forward to a detailed discussion.\n","\n","\n"," \n","Recruiter:\n","<<END>>\n","\n","ü§ñ gemma-3-4b-it:\n","\t‚ö° Time: 8.12s\n","\tüí¨ Response:\n","<<START>>\n"," \n","<<END>>\n","\n","\n","üìù SCENARIO 2: Generic Message\n","Recruiter: \n","            Hello, are you currently open to new opportunities?\n","            ...\n","------------------------------------------------------------\n","\n","ü§ñ Mistral-7B-Instruct-v0.3:\n","\t‚ö° Time: 5.45s\n","\tüí¨ Response:\n","<<START>>\n"," Hi there! Thank you for reaching out to me. I'm open to considering opportunities that match my skills and interests. As a Data and AI Engineer, I have extensive experience in building robust ELT pipelines and developing cutting-edge Generative AI solutions. I'm particularly proficient in using Microsoft Azure (Data Factory, Databricks, AI Services) and GCP (BigQuery, Composer) for data engineering, as well as Python and FastAPI for\n","<<END>>\n","\n","ü§ñ Meta-Llama-3-8B-Instruct:\n","\t‚ö° Time: 2.67s\n","\tüí¨ Response:\n","<<START>>\n"," Thank you for reaching out! I'm always open to exploring new possibilities that align with my skills and interests. Could you tell me more about the opportunity you have in mind? What kind of project or team are you working with?\n","<<END>>\n","\n","ü§ñ Phi-3-mini-4k-instruct:\n","\t‚ö° Time: 2.53s\n","\tüí¨ Response:\n","<<START>>\n"," Hello, thank you for reaching out. I'm always ready for new challenges and opportunities, especially in the field of Data and AI Engineering.\n","<<END>>\n","\n","ü§ñ gemma-3-4b-it:\n","\t‚ö° Time: 7.90s\n","\tüí¨ Response:\n","<<START>>\n"," \n","<<END>>\n","\n","\n","üìù SCENARIO 3: Wrong Match\n","Recruiter: \n","            We have a Java developer position available at our company.\n","            5 years experie...\n","------------------------------------------------------------\n","\n","ü§ñ Mistral-7B-Instruct-v0.3:\n","\t‚ö° Time: 5.36s\n","\tüí¨ Response:\n","<<START>>\n"," Thank you for reaching out. While I do have experience in Java development, my primary focus and expertise lie in Data and AI Engineering. Given the role you're offering, I'm afraid it doesn't align with my current skillset and career goals. However, I appreciate the opportunity to learn about your company and the position. If there are any other roles in the future that might match my profile, I would love to keep in touch.\n","\n","Recruiter:\n","<<END>>\n","\n","ü§ñ Meta-Llama-3-8B-Instruct:\n","\t‚ö° Time: 5.48s\n","\tüí¨ Response:\n","<<START>>\n"," Thank you for reaching out. While I appreciate the opportunity, I'm a Data and AI Engineer with a strong background in ELT pipelines, Generative AI, and cloud platforms like Azure and GCP. I don't have direct experience with Java development, and I'm not sure if my skills align with the requirements of the position. Could you provide more information on the role and how my skills might be applied in a Java development context? I'm also interested in exploring the possibility of remote\n","<<END>>\n","\n","ü§ñ Phi-3-mini-4k-instruct:\n","\t‚ö° Time: 7.14s\n","\tüí¨ Response:\n","<<START>>\n"," Dear recruiter,\n","        \n","        Thank you for reaching out to me with this opportunity.\n","\n","        I appreciate your interest in my profile. However, I specialize in Data and AI Engineering, with focus on Python and AI technologies.\n","\n","        My experience is centered around building systems with Retrieval-Augmented Generation (RAG) and AI Agents using frameworks like Semantic Kernel and LangChain.\n","\n","        I have\n","<<END>>\n","\n","ü§ñ gemma-3-4b-it:\n","\t‚ö° Time: 7.98s\n","\tüí¨ Response:\n","<<START>>\n"," \n","<<END>>\n","‚úÖ Benchmark iteration saved: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/benchmark_iterations/benchmark_20250728_155841.json (11828 bytes)\n","üìö Loaded existing history with 1 iterations\n","‚úÖ Comparison history saved: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/benchmark_iterations/benchmark_comparison_history.json (3705 bytes)\n","\n","‚úÖ BENCHMARK COMPLETE!\n","üìä Check /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/benchmark_iterations/ for detailed results and comparison history\n","üéØ Multiple test scenarios: READY\n","üìä Detailed comparison: READY\n","üÜò Automatic fallback system: ACTIVE\n","üíæ Your cached models will save significant time!\n","\n","====================================================================================================\n","========================================= END OF PROCESS =========================================\n","====================================================================================================\n"]}],"source":["print(\"=\"*100 )\n","print(\"=\"*41 + \" START OF PROCESS \" + \"=\"*41 )\n","print(\"=\"*100 )\n","\n","# Display environment configuration\n","benchmark.display_environment_config()\n","\n","# Check cache status\n","cached_models = display_cache_status(benchmark.candidate_models)\n","\n","# üéØ SYSTEM PROMPT\n","print(f\"\\n\\n‚å®Ô∏è SYSTEM PROMPT:\")\n","print(\"=\" * 60)\n","print(benchmark.system_prompt)\n","\n","\n","# üéØ TEST SCENARIOS INFO\n","print(f\"\\n\\nüéØ BENCHMARK SCENARIOS:\")\n","print(\"=\" * 60)\n","\n","for i, prompt in enumerate(benchmark.test_prompts):\n","    print(f\"  {i+1}. {prompt['name']}: {prompt['message']}\")\n","\n","\n","######### üöÄ RUN COMPREHENSIVE BENCHMARK  #########\n","run_benchmark = True # False # ‚úÖ Set to True to test\n","\n","if run_benchmark:\n","    print(\"\\nüîß STARTING BENCHMARK\")\n","    print(f\"üìÅ Results will be saved to: {benchmark.results_folder}\")\n","    print(f\"üïí Benchmark timestamp: {benchmark.benchmark_timestamp}\")\n","\n","    benchmark.benchmark_all_models()\n","    benchmark.display_detailed_results()\n","\n","    print(f\"\\n‚úÖ BENCHMARK COMPLETE!\")\n","    print(f\"üìä Check {benchmark.results_folder}/ for detailed results and comparison history\")\n","else:\n","    print(\"\\n‚è∏Ô∏è Set 'run_benchmark = True' above to start the OPTIMIZED benchmark\")\n","\n","print(\"üéØ Multiple test scenarios: READY\")\n","print(\"üìä Detailed comparison: READY\")\n","print(\"üÜò Automatic fallback system: ACTIVE\")\n","print(\"üíæ Your cached models will save significant time!\\n\")\n","\n","\n","print(\"=\"*100 )\n","print(\"=\"*41 + \" END OF PROCESS \" + \"=\"*41 )\n","print(\"=\"*100 )"]},{"cell_type":"markdown","metadata":{"id":"CSZQ5tdSvOyk"},"source":["## ***PHASE  5*** - Analyzing results"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1753719331393,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"SVENij8tw-2o","outputId":"4d63ac5f-27a1-48cb-aa36-6a2d39d19f81"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîç BENCHMARK EXPLORER READY!\n"]}],"source":["# üìä BENCHMARK RESULTS EXPLORER\n","# Run this cell AFTER BECHMARKING to explore saved iterations\n","\n","# List all iterations\n","results_folder = f\"{project_path}/benchmark_iterations\"\n","iteration_files = [f for f in os.listdir(results_folder) if f.startswith('benchmark_') and f.endswith('.json')]\n","iteration_files.sort(reverse=True)  # Most recent first\n","\n","\n","def explore_benchmark_history():\n","    \"\"\"Explore saved benchmark iterations and history\"\"\"\n","\n","\n","    if not os.path.exists(results_folder):\n","        print(\"‚ùå No benchmark iterations found. Run the benchmark first!\")\n","        return\n","\n","    print(f\"üìÅ BENCHMARK ITERATIONS FOUND: {len(iteration_files)}\")\n","    print(\"-\" * 60)\n","\n","    if iteration_files:\n","        for i, filename in enumerate(iteration_files[:5], 1):  # Show last 5\n","            timestamp = filename.replace('benchmark_', '').replace('.json', '')\n","            # Parse timestamp\n","            from datetime import datetime\n","            try:\n","                dt = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')\n","                readable_date = dt.strftime('%Y-%m-%d %H:%M:%S')\n","                print(f\"{i}. {filename} ({readable_date})\")\n","            except:\n","                print(f\"{i}. {filename}\")\n","\n","    # Show comparison history if exists\n","    history_file = f\"{results_folder}/benchmark_comparison_history.json\"\n","    if os.path.exists(history_file):\n","        print(f\"\\nüìà COMPARISON HISTORY:\")\n","        with open(history_file, 'r', encoding='utf-8') as f:\n","            history = json.load(f)\n","\n","        print(f\"Total iterations tracked: {len(history['iterations'])}\")\n","        print(f\"Models tracked: {', '.join([m.split('/')[-1] for m in history['models_tracked']])}\")\n","        print(f\"Last updated: {history['last_updated']}\")\n","\n","        if history['iterations']:\n","            latest = history['iterations'][-1]\n","            if 'summary' in latest and 'model_performance_scores' in latest['summary']:\n","                print(f\"\\nüèÜ LATEST PERFORMANCE RANKING:\")\n","                for i, score in enumerate(latest['summary']['model_performance_scores'][:3], 1):\n","                    model_name = score['model'].split('/')[-1]\n","                    print(f\"  {i}. {model_name} (Score: {score['performance_score']})\")\n","\n","    return results_folder, iteration_files\n","\n","def load_specific_iteration(timestamp_or_index=None):\n","    \"\"\"Load a specific benchmark iteration for detailed analysis\"\"\"\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    iteration_files = [f for f in os.listdir(results_folder) if f.startswith('benchmark_2') and f.endswith('.json')]\n","    iteration_files.sort(reverse=True)\n","\n","    if not iteration_files:\n","        print(\"‚ùå No iterations found!\")\n","        return None\n","\n","    # Determine which file to load\n","    if timestamp_or_index is None:\n","        # Load most recent\n","        target_file = iteration_files[0]\n","        print(f\"üìÑ Loading most recent iteration: {target_file}\")\n","    elif isinstance(timestamp_or_index, int):\n","        # Load by index (1-based)\n","        if 1 <= timestamp_or_index <= len(iteration_files):\n","            target_file = iteration_files[timestamp_or_index - 1]\n","            print(f\"üìÑ Loading iteration #{timestamp_or_index}: {target_file}\")\n","        else:\n","            print(f\"‚ùå Invalid index. Available: 1-{len(iteration_files)}\")\n","            return None\n","    else:\n","        # Load by timestamp\n","        target_file = f\"benchmark_{timestamp_or_index}.json\"\n","        if target_file not in iteration_files:\n","            print(f\"‚ùå Timestamp {timestamp_or_index} not found!\")\n","            return None\n","        print(f\"üìÑ Loading iteration: {target_file}\")\n","\n","    # Load the iteration data\n","    with open(f\"{results_folder}/{target_file}\", 'r', encoding='utf-8') as f:\n","        iteration_data = json.load(f)\n","\n","\n","    # Display environment config if available\n","    if 'environment_config' in iteration_data:\n","        env = iteration_data['environment_config']\n","        if env['hardware']['gpu']['available']:\n","            gpu = env['hardware']['gpu']['devices'][0]\n","            gpu_type = env[\"colab_specific\"].get(\"colab_gpu_type\", 'Unknown') if env[\"colab_specific\"][\"is_colab\"] else 'Local'\n","            print(f\"üéÆ Environment: {gpu['name']} ({gpu['memory_total_gb']} GB) - {gpu_type}\")\n","        else:\n","            print(f\"üñ•Ô∏è Environment: CPU-only\")\n","\n","        env_type = \"Google Colab\" if env[\"colab_specific\"][\"is_colab\"] else \"Local/Other\"\n","        print(f\"üåê Platform: {env_type}\")\n","\n","    return iteration_data\n","\n","def compare_environments_performance():\n","    \"\"\"Compare performance across different environment configurations\"\"\"\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    history_file = f\"{results_folder}/benchmark_comparison_history.json\"\n","\n","    if not os.path.exists(history_file):\n","        print(\"‚ùå No comparison history found. Run benchmarks first!\")\n","        return\n","\n","    with open(history_file, 'r', encoding='utf-8') as f:\n","        history = json.load(f)\n","\n","    if len(history['iterations']) < 2:\n","        print(\"‚ùå Need at least 2 benchmark iterations to compare environments\")\n","        return\n","\n","    print(\"üîÑ ENVIRONMENT PERFORMANCE COMPARISON\")\n","    print(\"=\" * 60)\n","\n","    # Group iterations by environment\n","    env_groups = {}\n","\n","    for iteration in history['iterations']:\n","        # Load full iteration data to get environment config\n","        iteration_file = f\"{results_folder}/benchmark_{iteration['timestamp']}.json\"\n","        if os.path.exists(iteration_file):\n","            with open(iteration_file, 'r', encoding='utf-8') as f:\n","                full_data = json.load(f)\n","\n","            if 'environment_config' in full_data:\n","                env = full_data['environment_config']\n","\n","                # Create environment signature\n","                if env['hardware']['gpu']['available']:\n","                    gpu_name = env['hardware']['gpu']['devices'][0]['name']\n","                    gpu_memory = env['hardware']['gpu']['devices'][0]['memory_total_gb']\n","                    colab_type = env[\"colab_specific\"].get(\"colab_gpu_type\", 'Unknown')\n","                    env_signature = f\"{colab_type} ({gpu_name}) - {gpu_memory}GB\"\n","                else:\n","                    env_signature = \"CPU-only\"\n","\n","                if env_signature not in env_groups:\n","                    env_groups[env_signature] = []\n","\n","                env_groups[env_signature].append({\n","                    'timestamp': iteration['timestamp'],\n","                    'datetime': iteration['datetime'],\n","                    'summary': iteration['summary']\n","                })\n","\n","    # Display comparison\n","    for env_sig, iterations in env_groups.items():\n","        print(f\"\\nüéÆ {env_sig}:\")\n","        print(f\"\\tüìä Iterations: {len(iterations)}\")\n","\n","        if iterations and 'model_performance_scores' in iterations[-1]['summary']:\n","            latest_scores = iterations[-1]['summary']['model_performance_scores']\n","            print(f\"\\tüèÜ Best Model: {latest_scores[0]['model'].split('/')[-1]} (Score: {latest_scores[0]['performance_score']})\")\n","\n","            # Calculate average performance across iterations for this environment\n","            if len(iterations) > 1:\n","                avg_inference_times = []\n","                for iter_data in iterations:\n","                    if 'overall_stats' in iter_data['summary']:\n","                        avg_inference_times.append(iter_data['summary']['overall_stats']['avg_inference_time'])\n","\n","                if avg_inference_times:\n","                    avg_time = sum(avg_inference_times) / len(avg_inference_times)\n","                    print(f\"   ‚ö° Avg Inference Time: {avg_time:.2f}s (across {len(avg_inference_times)} runs)\")\n","\n","    # Show environment impact on specific models\n","    print(f\"\\nüìà MODEL PERFORMANCE BY ENVIRONMENT:\")\n","    print(\"-\" * 70)\n","\n","    model_env_performance = {}\n","    for env_sig, iterations in env_groups.items():\n","        for iteration in iterations:\n","            if \"model_performance_scores\" in iteration[\"summary\"]:\n","                for model_score in iteration[\"summary\"][\"model_performance_scores\"]:\n","                    model_name = model_score[\"model\"].split(\"/\")[-1]\n","                    if model_name not in model_env_performance:\n","                        model_env_performance[model_name] = {}\n","\n","                    if env_sig not in model_env_performance[model_name]:\n","                        model_env_performance[model_name][env_sig] = []\n","\n","                    model_env_performance[model_name][env_sig].append(model_score[\"performance_score\"])\n","\n","    # Display model performance across environments\n","    for model_name, env_scores in model_env_performance.items():\n","        print(f\"\\nü§ñ {model_name}:\")\n","        for env_sig, scores in env_scores.items():\n","            avg_score = sum(scores) / len(scores)\n","            runs = len(scores)\n","            print(f\"   {env_sig}: {avg_score:.2f} avg score ({runs} run{'s' if runs != 1 else ''})\")\n","\n","# Quick exploration\n","print(\"üîç BENCHMARK EXPLORER READY!\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753719331401,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"mWpISuCxFwfE","outputId":"aaefab7b-3708-4a15-cb73-f5ae2897c728"},"outputs":[{"output_type":"stream","name":"stdout","text":["üìÅ BENCHMARK ITERATIONS FOUND: 6\n","------------------------------------------------------------\n","1. benchmark_comparison_history.json\n","2. benchmark_20250728_155841.json (2025-07-28 15:58:41)\n","3. benchmark_20250725_181432.json (2025-07-25 18:14:32)\n","4. benchmark_20250725_165345.json (2025-07-25 16:53:45)\n","5. benchmark_20250725_082023.json (2025-07-25 08:20:23)\n","\n","üìà COMPARISON HISTORY:\n","Total iterations tracked: 2\n","Models tracked: gemma-3-4b-it, Phi-3-mini-4k-instruct, Mistral-7B-Instruct-v0.3, Meta-Llama-3-8B-Instruct\n","Last updated: 2025-07-28 16:15:32\n","\n","üèÜ LATEST PERFORMANCE RANKING:\n","  1. Mistral-7B-Instruct-v0.3 (Score: 5.31)\n","  2. Meta-Llama-3-8B-Instruct (Score: 6.14)\n","  3. Phi-3-mini-4k-instruct (Score: 6.14)\n"]}],"source":["want_to_explore_benchmark_history = True\n","\n","if want_to_explore_benchmark_history:\n","    # Auto-explore if results exist\n","    results_folder = f\"{project_path}/benchmark_iterations\"\n","    if os.path.exists(results_folder):\n","        explore_benchmark_history()\n","    else:\n","        print(\"‚ùå No benchmark iterations found. Run the benchmark first!\")\n"]},{"cell_type":"markdown","metadata":{"id":"COdYWYdMtTGp"},"source":["# **Stage 3: Adapt & Align Model**"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1753719331403,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"YMpUbXaoWf2q"},"outputs":[],"source":["# Selected LLM model\n","selected_text_generator_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","# selected_text_generator_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","# Selected RAG embedding model\n","embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n","# embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\"\n","\n","# Selected guardrail model\n","input_guardrail_model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n","\n","# Selected output guardrail model\n","# output_guardrail_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","output_guardrail_model_name = \"microsoft/Phi-3-mini-4k-instruct\""]},{"cell_type":"markdown","metadata":{"id":"FIEHKdMe-zZm"},"source":["## ***PHASE 1*** - üîß RAG KNOWLEDGE BASE SETUP\n","Creating vectorized knowledge base from CV and job expectations"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1753719331437,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"YfB38JnsPBsT","outputId":"1fe943e6-1e36-4039-ebdf-eb9c1bff0922"},"outputs":[{"output_type":"stream","name":"stdout","text":["üß† RAG Knowledge Base class initialized!\n","üìö Ready to process CV and job expectations\n"]}],"source":["class RAGKnowledgeBase:\n","\n","    def __init__(self, project_path: str):\n","        self.project_path = project_path\n","        self.embeddings = None\n","        self.vectorstore = None\n","        self.documents = []\n","\n","        # Text splitter configuration for optimal chunking\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=50,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \". \", \"##\"] #, \" \", \"\"]\n","            # separators=[\"##\"]\n","        )\n","\n","    def setup_embeddings(self):\n","        \"\"\"Initialize sentence transformer embeddings optimized for recruitment context\"\"\"\n","        print(\"ü§ñ Initializing embeddings model...\")\n","\n","        # Using a higher-quality model optimized for semantic similarity and retrieval\n","        self.embeddings = HuggingFaceEmbeddings(\n","            model_name=embedding_model_name,\n","            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n","            encode_kwargs={\"normalize_embeddings\": True}\n","        )\n","\n","        print(\"‚úÖ Embeddings model loaded successfully\")\n","        return self.embeddings\n","\n","    def load_and_process_documents(self, cv_content: str, expectations_content: str):\n","        \"\"\"Load and process CV and job expectations into document chunks\"\"\"\n","        print(\"üìÑ Processing documents for RAG...\")\n","\n","        # Create documents with metadata\n","        documents = [\n","            Document(\n","                page_content=cv_content,\n","                metadata={\"source\": \"cv\", \"type\": \"professional_profile\"}\n","            ),\n","            Document(\n","                page_content=expectations_content,\n","                metadata={\"source\": \"job_expectations\", \"type\": \"requirements\"}\n","            )\n","        ]\n","\n","        # Split documents into chunks\n","        self.documents = self.text_splitter.split_documents(documents)\n","\n","        print(f\"‚úÖ Created {len(self.documents)} document chunks\")\n","        for doc in self.documents:\n","            print(f\"   üìã {doc.metadata['source']}: {len(doc.page_content)} chars\")\n","\n","        return self.documents\n","\n","    def create_vectorstore(self):\n","        \"\"\"Create FAISS vectorstore from processed documents\"\"\"\n","        if not self.documents:\n","            raise ValueError(\"No documents loaded. Call load_and_process_documents first.\")\n","\n","        if not self.embeddings:\n","            self.setup_embeddings()\n","\n","        print(\"üîç Creating FAISS vectorstore...\")\n","\n","        # Create vectorstore\n","        self.vectorstore = FAISS.from_documents(\n","            documents=self.documents,\n","            embedding=self.embeddings\n","        )\n","\n","        print(\"‚úÖ Vectorstore created successfully\")\n","        return self.vectorstore\n","\n","    def search_relevant_context(self, query: str, k: int = 3):\n","        \"\"\"Search for relevant context given a query\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vectorstore not created. Call create_vectorstore first.\")\n","\n","        # Perform similarity search\n","        relevant_docs = self.vectorstore.similarity_search(query, k=k)\n","\n","        return relevant_docs\n","\n","    def get_context_string(self, query: str, k: int = 3):\n","        \"\"\"Get formatted context string for prompt injection with job_expectations always included\"\"\"\n","        relevant_docs = self.search_relevant_context(query, k)\n","\n","        context_parts = []\n","\n","        # ALWAYS include job_expectations first (critical but small)\n","        job_expectations_included = False\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source == \"job_expectations\":\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                job_expectations_included = True\n","                break\n","\n","        # If job_expectations wasn't in the retrieved docs, find and add it\n","        if not job_expectations_included:\n","            for doc in self.documents:\n","                if doc.metadata.get(\"source\") == \"job_expectations\":\n","                    content = doc.page_content.strip()\n","                    context_parts.append(f\"[JOB_EXPECTATIONS - ALWAYS CONSIDER]: {content}\")\n","                    break\n","\n","        # Add other relevant documents\n","        for doc in relevant_docs:\n","            source = doc.metadata.get(\"source\", \"unknown\")\n","            if source != \"job_expectations\":  # Skip if already added\n","                content = doc.page_content.strip()\n","                context_parts.append(f\"[{source.upper()}]: {content}\")\n","\n","        return \"\\n\\n\".join(context_parts)\n","\n","# Initialize RAG knowledge base\n","rag_kb = RAGKnowledgeBase(project_path)\n","\n","print(\"üß† RAG Knowledge Base class initialized!\")\n","print(\"üìö Ready to process CV and job expectations\")\n"]},{"cell_type":"markdown","metadata":{"id":"8yzAGLDP-zZm"},"source":["## ***PHASE 2*** -  üöÄ SETUP RAG KNOWLEDGE BASE\n","Load and process documents for retrieval"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d229179a8af64077b62269ecba533269","e98d42dbedaf426689a45290decd90b5","a82c589b677148379c93b558fdcd4054","8465a585e90343d8843c88f6965e69b2","c8700cb220594766b1b8d9965a6438c8","7f3022ce587e4efa869f5614addf4244","8d7f65097cd04a71bb57020c0f471552","d09f673babb941ef9de239403a1f8efb","1dd634680fa6420486f68d7e17c1ac75","46215043fb624444826b7d4656252524","123abfd1f199449cbff78dbe8853aab9","58cba4a8aca24aa09412c2b09946aa75","df206ed716a84de2ac77ab03d2b64cad","f6b0f45a38b4492f93061b26411c5d0d","527851a71382483ca47c8936abf51a4f","93691947a7e04cc28fa90fc1d849ba11","f7804daf4e7842d1a3aa444acb796c38","d22dc4350bac4c9cb9ae7d1fa1f074c4","e203501743fa4454853f98f6894ef7fe","2675bdf6c37b4787b28829072fd64349","a44d6bf3c00b43dfbbadaea317ecaadc","f0b677e591204a26b77cc24308ba3bd6","2a57dfc9886b4c0bbf76bf852ec3fd94","bfd6c6d72b84487c9cc8a1a2884255cf","bf395abc1d41471c91537fd17d3ffdd1","29e3736f5b9e45b792632434390d5bc8","1ce2ad925dc64f8c83243c8238eb0660","db751d494b3446dda7f5fa508267d61f","9b585bb1c7074d1bab0c079fffab08df","8ea8d113d7fb4f20908daa7a6fa82928","41f4369bef3743cd87c20a5729e3651b","64ff1f5832bd44d8bfdcd5d69e57644a","655cfa6ebac946f19f46597917738734","ee485db51b8746c58385c5be61496552","b47346c7428543b5b3d82faa4c2b98d9","c532c2e29136491abc046090299b2681","19aafe239ad04221b34cd2b7f929f406","050ba58105b3489da854c8baeedc0243","c92d71c942a64b338f357d82058589b7","ef0eb891e4544abca37ebe9b115f5598","13acd98590f640deb5554a4a23e5e910","b7faac343ff84f67aa89df859c4c7529","b017c88ad2af4ebca4789bf220ee13b7","7dc1c1e2dbc748f689cc0c4dd71e008d","bd4e38a1c3d248c5bcb09e6c2a249727","a366e8dab7d44551a527662287877188","2513b3029c7d452a8a3ec61999732a4c","6512c4e1a2a04cc48134c68ed87b23ae","9007ab634d76477b899d458207c984b6","9e3f257b45f24c53835250a1cd9bfe6a","5c44802ed60940a9a04227cd5aa317cb","5a9cf03a159b48feba1585847d114cbf","36371a5a4d6347699ffbc189ef2c5ac5","8e5604cead094796b3d58658b9eb8892","6e44d76c39024c74839bcd4f9b9e5279","c3597072fcbd457f99e90991a5288627","cacdc256ac6d4507a24638dee923e182","edd4beb1b63a40459fbc298ccf310de5","ebed5e15e4ae4206aeffa09c362ea2e9","7d006a29557e4df5a767726e82db396a","601caff947b743d497ba5539ced32c2e","3e8d67b1f8c44337be9d0ed2b6d27f39","01732bd371b443c8b11aa87a5fe6130f","ef6d3a5d258e4b138b8c4d18ca231eb7","a3a7c594791a41208000e3a7230de57f","e04c899686d4425abb72b8abd3eb3b93","e3a13b3042f64eb3a8a2354d9bf504a6","2d7219aad2a140a09b86b19237a57d94","2850f6ce8fb142f5a606817183352b75","018bec3f57ff4d58a9ca8fee04ab7911","9b3b413acb3c46008ab52440c986762d","39310bebaea14fd7a880f86e35f25c3b","b5217f28f3514dd2b4de99ac4736bc09","600a7b76859f42bc9456393422879b62","17cb928c0f344892a6ab4972ec092ec4","bba904455a964104a1319a35ce9aa02e","b1f54995be734b108f48c7f9df8505d5","c46910c7e88548b1841db19b337dadc0","31192fc8086b49ce882945940b8c737b","1da12db4d9d843e4885d4c3ea7699bad","77e5cf9c41f143caa28ce8f83336196d","832ee8fb468e4bf7b1421334516a47d8","c705bc8413d849c19143a1175a7d698b","98651d8e32924233ba0151d43f71cee3","1853d5f504b045d9b7864a37497bbdf6","328457c3da574ae3a9b87322ae0bbe48","d2253457343545cbbac71c8ef53d2c8c","4ed9afb9ba1b4a4588368e469d75c50d","9baf16c5dc2849479d41874d5d258428","798fc6d106be45b492650ab8264b41d7","b9429e7cc2b040a9af568008718cdbb8","8b3b2f298fb64522890220b075ac45a4","760602bea18b40f999e9338eae8d85da","c44c3eb1cd3d445894c030c382d6b660","1d57ed77871a46dba96de9ab838a4e5b","763c6a87662840899b1943715007a2be","d3c982ee1c9f46f7953e5f1de7ef9a64","065e47ea990f4d52b7e0fb41b8d3a197","4a4fd9e38dc34bd39c61eb84fbddfead","3896419379354c0b85c044ee79d2f2d5","7c5e77698cfa4a97be3936fcd83f1898","3be0c1e9927447ebba7544a7926fe437","6c85020bf04b4e34bc487f36bde828d0","5b19cfa0139248ffbc40201ddba9907e","5fe21de24e7b4575b9c430ee15fc5e83","823c04f3f87a46ca8fb5dc0608843ac6","88fb5ba75313401585983a82a581939e","d0b80e0992ee4ef49880b73649822714","3222618da85e47b8a63a5c46833168cd","8f4478f2e3d04df2a0d7954ea530e8d5","86822494d24e4b5abe4ede0e1f7d0f23","3897f11ee3c9407093c0c2ee42735d67","4500144222254e038054080e1c48a7c5","738755022c89480fa87e553bcb08a219","845961f7ae4446a2832dadaa77ab4a5e","e6ce9e8691c84cc590a1eeafa5daa489","a6739e928e6644a6af4b557c08b6709f","9790df1d21d249f2b9e2adcc54b5ce43","9c2a80b707d249d5b0d565e31e9ad4a0","8060424c67b1422c8af813dac8b21126","f10bc514f93a4e6c8af12628aeaf762e"]},"executionInfo":{"elapsed":6151,"status":"ok","timestamp":1753719337589,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"xzTD-RelPBsT","outputId":"e6275c8a-de41-4517-8ccd-0095e7fabb18"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîß Setting up RAG Knowledge Base...\n","ü§ñ Initializing embeddings model...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-11-2240822892.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","  self.embeddings = HuggingFaceEmbeddings(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d229179a8af64077b62269ecba533269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58cba4a8aca24aa09412c2b09946aa75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a57dfc9886b4c0bbf76bf852ec3fd94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee485db51b8746c58385c5be61496552"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4e38a1c3d248c5bcb09e6c2a249727"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3597072fcbd457f99e90991a5288627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a13b3042f64eb3a8a2354d9bf504a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c46910c7e88548b1841db19b337dadc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9baf16c5dc2849479d41874d5d258428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3896419379354c0b85c044ee79d2f2d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86822494d24e4b5abe4ede0e1f7d0f23"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Embeddings model loaded successfully\n","üìÑ Processing documents for RAG...\n","‚úÖ Created 24 document chunks\n","   üìã cv: 193 chars\n","   üìã cv: 18 chars\n","   üìã cv: 479 chars\n","   üìã cv: 239 chars\n","   üìã cv: 231 chars\n","   üìã cv: 412 chars\n","   üìã cv: 217 chars\n","   üìã cv: 371 chars\n","   üìã cv: 190 chars\n","   üìã cv: 451 chars\n","   üìã cv: 433 chars\n","   üìã cv: 340 chars\n","   üìã cv: 216 chars\n","   üìã cv: 401 chars\n","   üìã cv: 391 chars\n","   üìã cv: 348 chars\n","   üìã cv: 287 chars\n","   üìã cv: 487 chars\n","   üìã cv: 40 chars\n","   üìã cv: 378 chars\n","   üìã cv: 457 chars\n","   üìã cv: 156 chars\n","   üìã cv: 255 chars\n","   üìã job_expectations: 326 chars\n","üîç Creating FAISS vectorstore...\n","‚úÖ Vectorstore created successfully\n","\n","üß™ Testing retrieval system...\n","\n","‚ùì Query: What are my technical skills in data engineering?\n","üìÑ Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between ‚Ç¨60,000 and ‚Ç¨65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: ## I.T. Skills\n","Generative AI: RAG (Retrieval-Augmented Generation), AI Agents, LLMs, Prompt Engineering, Semantic Kernel, LangChain, Fine Tuning, OpenAI APIs..\n","Data Engineering: ETL/ELT Processes, Data Lake, Data Warehouse, Apache Airflow, Compositor (GCP), Spark, Hadoop, Hive, PySpark.\n","\n","[CV]: . An AI & Data Engineer with experience in Generative AI, cloud platforms (Azure, GCP), and ELT processes. Skilled in project management, APIs, and agile methodologies with a strong analytical mindset, a fast learner, and results-oriented....\n","\n","‚ùì Query: What technologies do I work with?\n","üìÑ Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between ‚Ç¨60,000 and ‚Ç¨65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: ## Languages\n","Spanish: Native\n","English: Professional Working Proficiency\n","\n","\n","## Main Projects\n","Generative AI apps with Semantic Kernel / Beam Suntory\n","Development of Digital Humans using LLMs / BeHumans\n","Cloud-based ETL pipeline with Databricks & Azure / Minsait\n","\n","[CV]: Project Manager\n","Digittude Technology Solutions SL\n","January 2024 - August 2024 (Madrid, Spain)\n","CLIENT: BE HUMANS\n","Led Generative AI initiatives for the development and deployment of Digital Human (DH) solutions for various clients.\n","Utilized the platform's API to technically configure and customize Digital Humans, adapting their functionalities to diverse client use cases....\n","\n","‚ùì Query: What type of job am I looking for?\n","üìÑ Retrieved context: \n","[JOB_EXPECTATIONS - ALWAYS CONSIDER]: ## Salary expectations: Between ‚Ç¨60,000 and ‚Ç¨65,000 gross per year, with an engineering profile. If the position is leadership, it's best to discuss salary in detail.\n","## Working arrangements: Preferably 100% remote, but if not possible, at least 90% if the job is related to AI or if the description fits my profile very well.\n","\n","[CV]: . An AI & Data Engineer with experience in Generative AI, cloud platforms (Azure, GCP), and ELT processes. Skilled in project management, APIs, and agile methodologies with a strong analytical mindset, a fast learner, and results-oriented.\n","\n","[CV]: Passionate about Artificial Intelligence and Data Engineering, with experience in developing and deploying robust backend systems and cutting-edge solutions in Generative AI, Cloud Computing, and Big Data. Throughout my career at companies like Digittude Technology and Minsait, I have contributed to projects with advanced AI architectures like Retrieval-Augmented Generation (RAG) and AI agent-based systems, using orchestration frameworks such as Semantic Kernel and LangChain...\n","\n","‚úÖ RAG Knowledge Base ready for AI Assistant!\n"]}],"source":["# Setup embeddings and process documents\n","print(\"üîß Setting up RAG Knowledge Base...\")\n","rag_kb.setup_embeddings()\n","\n","# Load and process documents\n","if cv_content and expectations_content:\n","    rag_kb.load_and_process_documents(cv_content, expectations_content)\n","    rag_kb.create_vectorstore()\n","\n","    print(\"\\nüß™ Testing retrieval system...\")\n","\n","    # Test retrieval with sample queries\n","    test_queries = [\n","        \"What are my technical skills in data engineering?\",\n","        \"What technologies do I work with?\",\n","        \"What type of job am I looking for?\"\n","    ]\n","\n","    for query in test_queries:\n","        print(f\"\\n‚ùì Query: {query}\")\n","        context = rag_kb.get_context_string(query, k=2)\n","        # print(f\"üìÑ Retrieved context (first 200 chars): {context[:200]}...\")\n","        print(f\"üìÑ Retrieved context: \\n{context}...\")\n","\n","    print(\"\\n‚úÖ RAG Knowledge Base ready for AI Assistant!\")\n","\n","else:\n","    print(\"‚ùå CV or expectations content missing. Check data loading.\")\n"]},{"cell_type":"markdown","metadata":{"id":"p_er1dHPjShl"},"source":["## ***PHASE 3*** - Input Guardrail System\n","\n","Implementing an intelligent guardrail that performs **Intent Detection** to classify recruiter messages as either:\n","- **Generic messages** ‚Üí State: \"pending_details\" ‚Üí Request more information\n","- **Concrete job offers** ‚Üí Pass to RAG system for analysis\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1753719337617,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"jpBOjD4oZMqo"},"outputs":[],"source":["from enum import Enum\n","from dataclasses import dataclass\n","from typing import Dict, Any, Tuple\n","\n","class MessageType(Enum):\n","    GENERIC = \"generic\"\n","    CONCRETE_OFFER = \"concrete_offer\"\n","\n","class GenericSubType(Enum):\n","    BASIC_INTRODUCTION = \"basic_introduction\"\n","    OPPORTUNITY_INQUIRY = \"opportunity_inquiry\"\n","\n","class ConversationState(Enum):\n","    PENDING_DETAILS = \"pending_details\"\n","    ANALYZING = \"analyzing\"\n","    PASSED = \"passed\"\n","    STAND_BY = \"stand_by\"\n","    FINISHED = \"finished\"\n","\n","@dataclass\n","class GuardrailResult:\n","    message_type: MessageType\n","    confidence: float\n","    state: ConversationState\n","    language: str = \"English\"\n","    generic_subtype: GenericSubType = None\n","    response: str = None\n","    should_continue_to_rag: bool = False"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3017,"status":"ok","timestamp":1753719340636,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"EmU8lYjrVvx2","outputId":"b1f1116d-6e86-4602-b670-c6ff2848a48c"},"outputs":[{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Updated Input Guardrail system initialized!\n","üìä Message classification: GENERIC vs CONCRETE_OFFER\n","üéØ Natural response generation: BASIC_INTRODUCTION (brief) vs OPPORTUNITY_INQUIRY (detailed)\n","üåç Language detection and natural responses\n","‚ö° Using Phi-3-mini for classification and response generation\n"]}],"source":["# üõ°Ô∏è INPUT GUARDRAIL WITH NATURAL RESPONSE GENERATION\n","\n","class InputGuardrailNatural:\n","    \"\"\"\n","    Updated InputGuardrail that generates natural responses using the model\n","    instead of using fixed templates\n","    \"\"\"\n","\n","    def __init__(self, cache_path: str):\n","        self.cache_path = cache_path\n","        self.model_name = input_guardrail_model_name\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Load prompts from external files\n","        import sys\n","        import os\n","        import importlib\n","        sys.path.append(f\"{project_path}/prompts/app\")\n","\n","        # Reload prompt_loader module to get latest changes\n","        if 'prompt_loader' in sys.modules:\n","            importlib.reload(sys.modules['prompt_loader'])\n","\n","        from prompt_loader import PromptLoader\n","\n","        prompt_loader = PromptLoader(f\"{project_path}/prompts/app\")\n","        input_prompts = prompt_loader.load_input_guardrail_prompts()\n","\n","        self.classification_prompt = input_prompts[\"classification_prompt\"]\n","        self.generic_response_template = input_prompts[\"generic_response_template\"]\n","        self.basic_intro_prompt = input_prompts[\"basic_intro_prompt\"]\n","        self.opportunity_inquiry_prompt = input_prompts[\"opportunity_inquiry_prompt\"]\n","\n","    def load_model(self):\n","        \"\"\"Load model for fast classification\"\"\"\n","        print(f\"\\nüõ°Ô∏è Loading guardrail model: {self.model_name}\")\n","\n","        # Check cache\n","        cached_models = check_cached_models(self.cache_path)\n","        model_folder_space = self.model_name.replace('/', ' ')\n","\n","        if model_folder_space in cached_models:\n","            print(\"‚ö° Loading from cache...\")\n","            model_cache_dir = os.path.join(self.cache_path, model_folder_space)\n","        else:\n","            print(\"üì• Downloading model...\")\n","            model_cache_dir = create_model_cache_dir(self.model_name, self.cache_path)\n","\n","        # Quantization for efficiency\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(\n","                self.model_name,\n","                cache_dir=model_cache_dir\n","            )\n","            if self.tokenizer.pad_token is None:\n","                self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                self.model_name,\n","                quantization_config=bnb_config,\n","                device_map=\"auto\",\n","                trust_remote_code=True,\n","                cache_dir=model_cache_dir,\n","                attn_implementation=\"eager\"  # Phi-3 optimization\n","            )\n","\n","            print(\"‚úÖ Guardrail model loaded successfully!\")\n","            return self.model, self.tokenizer\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error loading guardrail model: {str(e)}\")\n","            raise\n","\n","    def classify_message(self, recruiter_message: str) -> Tuple[MessageType, float, str, GenericSubType]:\n","        \"\"\"Classify recruiter message and detect language\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Guardrail model not loaded. Call load_model() first.\")\n","\n","        # Format classification prompt\n","        prompt = self.classification_prompt.format(message=recruiter_message.strip())\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=80,\n","                    do_sample=False,\n","                    temperature=0.1,\n","                    pad_token_id=self.tokenizer.eos_token_id,\n","                    use_cache=False\n","                )\n","\n","            # Decode response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            classification_response = full_response.replace(prompt, \"\").strip()\n","\n","            # Parse classification result with new fields\n","            message_type, confidence, language, generic_subtype = self._parse_classification(classification_response)\n","\n","            return message_type, confidence, language, generic_subtype\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Classification error: {str(e)}\")\n","            # Fallback: basic keyword detection\n","            return self._fallback_classification(recruiter_message)\n","\n","    def _parse_classification(self, response: str) -> Tuple[MessageType, float, str, GenericSubType]:\n","        \"\"\"Parse model classification response including language and sub-classification\"\"\"\n","        import re\n","        response_lower = response.lower()\n","\n","        # Extract language\n","        language = \"English\"  # Default\n","        language_match = re.search(r'language:\\s*([^\\n\\r]+)', response_lower)\n","        if language_match:\n","            language = language_match.group(1).strip().title()\n","\n","        # Extract classification\n","        if \"concrete_offer\" in response_lower or \"concrete offer\" in response_lower:\n","            message_type = MessageType.CONCRETE_OFFER\n","            generic_subtype = None\n","        elif \"generic\" in response_lower:\n","            message_type = MessageType.GENERIC\n","\n","            # Extract sub-classification for generic messages\n","            generic_subtype = GenericSubType.OPPORTUNITY_INQUIRY  # Default\n","            if \"basic_introduction\" in response_lower or \"introduction\" in response_lower:\n","                generic_subtype = GenericSubType.BASIC_INTRODUCTION\n","            elif \"opportunity_inquiry\" in response_lower or \"opportunity\" in response_lower:\n","                generic_subtype = GenericSubType.OPPORTUNITY_INQUIRY\n","        else:\n","            # Fallback based on response content\n","            if any(keyword in response_lower for keyword in [\"specific\", \"detailed\", \"role\", \"position\", \"salary\"]):\n","                message_type = MessageType.CONCRETE_OFFER\n","                generic_subtype = None\n","            else:\n","                message_type = MessageType.GENERIC\n","                generic_subtype = GenericSubType.OPPORTUNITY_INQUIRY\n","\n","        # Extract confidence\n","        confidence = 0.8  # Default confidence\n","        try:\n","            confidence_match = re.search(r'confidence:\\s*([0-9.]+)', response_lower)\n","            if confidence_match:\n","                confidence = float(confidence_match.group(1))\n","                confidence = max(0.0, min(1.0, confidence))  # Clamp to [0,1]\n","        except:\n","            pass\n","\n","        return message_type, confidence, language, generic_subtype\n","\n","    def _fallback_classification(self, message: str) -> Tuple[MessageType, float, str, GenericSubType]:\n","        \"\"\"Fallback classification using keyword detection\"\"\"\n","        message_lower = message.lower()\n","\n","        # Simple language detection (basic heuristics)\n","        language = \"English\"  # Default\n","        if any(word in message_lower for word in [\"hola\", \"gracias\", \"trabajo\", \"oportunidad\"]):\n","            language = \"Spanish\"\n","        elif any(word in message_lower for word in [\"bonjour\", \"merci\", \"travail\", \"opportunit√©\"]):\n","            language = \"French\"\n","        elif any(word in message_lower for word in [\"hallo\", \"danke\", \"arbeit\", \"gelegenheit\"]):\n","            language = \"German\"\n","\n","        # Keywords indicating concrete offers\n","        concrete_keywords = [\n","            \"position\", \"role\", \"job\", \"salary\", \"‚Ç¨\", \"$\", \"¬£\", \"experience\",\n","            \"requirements\", \"responsibilities\", \"company\", \"team\", \"technologies\",\n","            \"remote\", \"on-site\", \"hybrid\", \"years\", \"senior\", \"junior\", \"developer\",\n","            \"engineer\", \"analyst\", \"manager\", \"python\", \"java\", \"javascript\",\n","            \"cloud\",\"aws\", \"azure\", \"gcp\", \"react\", \"angular\", \"node\", \"sql\", \"nosql\",\n","            \"data\", \"ai\"\n","        ]\n","\n","        # Keywords indicating generic messages\n","        generic_keywords = [\n","            \"open to\", \"opportunities\", \"connect\", \"network\", \"interested in\",\n","            \"catch up\", \"chat\", \"discuss\", \"explore\", \"available\", \"looking for\", \"job\"\n","        ]\n","\n","        # Keywords for basic introductions\n","        intro_keywords = [\n","            \"my name is\", \"i am\", \"hello\", \"hi\", \"how are you\", \"nice to meet\",\n","            \"greetings\", \"good morning\", \"good afternoon\"\n","        ]\n","\n","        concrete_score = sum(1 for keyword in concrete_keywords if keyword in message_lower)\n","        generic_score = sum(1 for keyword in generic_keywords if keyword in message_lower)\n","        intro_score = sum(1 for keyword in intro_keywords if keyword in message_lower)\n","\n","        if concrete_score > generic_score and concrete_score >= 2:\n","            return MessageType.CONCRETE_OFFER, 0.7, language, None\n","        else:\n","            # Determine sub-type for generic messages\n","            if intro_score > 0 or len(message.strip().split()) < 8:  # Very short messages likely introductions\n","                subtype = GenericSubType.BASIC_INTRODUCTION\n","            else:\n","                subtype = GenericSubType.OPPORTUNITY_INQUIRY\n","            return MessageType.GENERIC, 0.6, language, subtype\n","\n","    def generate_natural_response(self, message: str, language: str, subtype: GenericSubType) -> str:\n","        \"\"\"Generate natural response using the model instead of templates\"\"\"\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Guardrail model not loaded. Call load_model() first.\")\n","\n","        # Choose appropriate prompt based on subtype\n","        if subtype == GenericSubType.BASIC_INTRODUCTION:\n","            prompt = self.basic_intro_prompt.format(message=message, language=language)\n","        else:  # OPPORTUNITY_INQUIRY\n","            prompt = self.opportunity_inquiry_prompt.format(\n","                message=message,\n","                language=language,\n","                generic_template=self.generic_response_template\n","            )\n","\n","        try:\n","            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=150,  # Enough for natural responses\n","                    do_sample=True,\n","                    temperature=0.7,  # More creative for natural responses\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id,\n","                    use_cache=False\n","                )\n","\n","            # Decode and clean response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            response = full_response.replace(prompt, \"\").strip()\n","\n","            return response\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Error generating natural response: {str(e)}\")\n","            # Fallback to basic response\n","            if subtype == GenericSubType.BASIC_INTRODUCTION:\n","                return \"Hello! Thanks for reaching out. Best regards.\"\n","            else:\n","                return self.generic_response_template\n","\n","    def process_message(self, recruiter_message: str) -> GuardrailResult:\n","        \"\"\"Process recruiter message through the guardrail system\"\"\"\n","\n","        print(\"üõ°Ô∏è Processing message through guardrail...\")\n","\n","        # Step 1: Classify message type with language and sub-classification\n","        message_type, confidence, language, generic_subtype = self.classify_message(recruiter_message)\n","\n","        print(f\"   üìä Classification: {message_type.value} (confidence: {confidence:.2f})\")\n","        print(f\"   üåç Language: {language}\")\n","        if generic_subtype:\n","            print(f\"   üîç Sub-type: {generic_subtype.value}\")\n","\n","        # Step 2: Handle based on classification\n","        if message_type == MessageType.GENERIC:\n","            # Generate natural response using the model\n","            print(f\"   üí¨ Generating natural response for {generic_subtype.value} in {language}\")\n","            response = self.generate_natural_response(recruiter_message, language, generic_subtype)\n","\n","            return GuardrailResult(\n","                message_type=message_type,\n","                confidence=confidence,\n","                language=language,\n","                generic_subtype=generic_subtype,\n","                state=ConversationState.PENDING_DETAILS,\n","                response=response,\n","                should_continue_to_rag=False\n","            )\n","\n","        else:  # CONCRETE_OFFER\n","            # Concrete offer: pass to RAG system\n","            return GuardrailResult(\n","                message_type=message_type,\n","                confidence=confidence,\n","                language=language,\n","                generic_subtype=None,\n","                state=ConversationState.ANALYZING,\n","                response=None,\n","                should_continue_to_rag=True\n","            )\n","\n","# Initialize Updated Input Guardrail\n","input_guardrail = InputGuardrailNatural(cache_path=cache_path)\n","\n","print(\"üõ°Ô∏è Updated Input Guardrail system initialized!\")\n","print(\"üìä Message classification: GENERIC vs CONCRETE_OFFER\")\n","print(\"üéØ Natural response generation: BASIC_INTRODUCTION (brief) vs OPPORTUNITY_INQUIRY (detailed)\")\n","print(\"üåç Language detection and natural responses\")\n","print(\"‚ö° Using Phi-3-mini for classification and response generation\")\n"]},{"cell_type":"markdown","metadata":{"id":"VHTP08eG-zZm"},"source":["## ***PHASE 4*** - üõ°Ô∏è OUTPUT GUARDRAIL SYSTEM\n","Validates and improves response naturalness"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":689,"status":"ok","timestamp":1753719341327,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"xAvoOvVW-zZm","outputId":"219267dd-5fee-4bf7-cad8-c7888f0b1bcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Output Guardrail system initialized!\n","üìù Response validation: First person usage, no placeholders, natural tone\n","üîÑ Iterative improvement: Up to k=5 iterations\n","üéØ Fallback generation: Guardrail creates corrected version if needed\n","‚ö° Using microsoft/Phi-3-mini-4k-instruct for validation and correction\n","üí° Alternative models available: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\n"]}],"source":["from typing import List, Dict, Tuple\n","import re\n","\n","class OutputGuardrail:\n","    \"\"\"\n","    Output guardrail that validates response naturalness and ensures first-person perspective.\n","    \"\"\"\n","\n","    def __init__(self, cache_path: str):\n","        self.cache_path = cache_path\n","        self.model_name = output_guardrail_model_name #\"meta-llama/Meta-Llama-3-8B-Instruct\"\n","        # Alternative models for consideration:\n","        # self.model_name = \"google/gemma-3-4b-it\"  # Alternative option 1\n","        # self.model_name = \"microsoft/Phi-3-mini-4k-instruct\"  # Alternative option 2 (but already used in input guardrail)\n","\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Load prompts from external files\n","        import sys\n","        import os\n","        sys.path.append(f\"{project_path}/prompts/app\")\n","        from prompt_loader import PromptLoader\n","\n","        prompt_loader = PromptLoader(f\"{project_path}/prompts/app\")\n","        output_prompts = prompt_loader.load_output_guardrail_prompts()\n","\n","        self.validation_prompt = output_prompts[\"validation_prompt\"]\n","        self.correction_prompt = output_prompts[\"correction_prompt\"]\n","\n","    def load_model(self):\n","        \"\"\"Load Meta-Llama-3-8B-Instruct model for validation and correction\"\"\"\n","        print(f\"\\nüõ°Ô∏è Loading output guardrail model: {self.model_name}\")\n","\n","        # Check cache\n","        cached_models = check_cached_models(self.cache_path)\n","        model_folder_space = self.model_name.replace('/', ' ')\n","\n","        if model_folder_space in cached_models:\n","            print(\"‚ö° Loading from cache...\")\n","            model_cache_dir = os.path.join(self.cache_path, model_folder_space)\n","        else:\n","            print(\"üì• Downloading model...\")\n","            model_cache_dir = create_model_cache_dir(self.model_name, self.cache_path)\n","\n","        # Quantization for efficiency\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False\n","        )\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(\n","                self.model_name,\n","                cache_dir=model_cache_dir\n","            )\n","            if self.tokenizer.pad_token is None:\n","                self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                self.model_name,\n","                quantization_config=bnb_config,\n","                device_map=\"auto\",\n","                trust_remote_code=True,\n","                cache_dir=model_cache_dir\n","            )\n","\n","            print(\"‚úÖ Output guardrail model loaded successfully!\")\n","            return self.model, self.tokenizer\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error loading output guardrail model: {str(e)}\")\n","            raise\n","\n","    def validate_response(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Validate response for naturalness and first-person perspective\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            # Fallback to rule-based validation if model not loaded\n","            return self._rule_based_validation(response)\n","\n","        prompt = self.validation_prompt.format(response=response.strip())\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=100,\n","                    do_sample=False,\n","                    temperature=0.1,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and parse validation result\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            validation_response = full_response.replace(prompt, \"\").strip()\n","\n","            return self._parse_validation_result(validation_response)\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Validation error: {str(e)}. Using fallback validation.\")\n","            return self._rule_based_validation(response)\n","\n","    def _rule_based_validation(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Fallback rule-based validation\"\"\"\n","        issues = []\n","\n","        # Check for third person references\n","        third_person_patterns = [\n","            r'\\bthe candidate\\b',\n","            r'\\bcandidate\\'s\\b',\n","            r'\\bhis/her\\b',\n","            r'\\bhis or her\\b',\n","            r'\\bhis\\b(?!\\s+name)',  # \"his\" but not \"his name\"\n","            r'\\bher\\b(?!\\s+email)', # \"her\" but not \"her email\"\n","        ]\n","\n","        for pattern in third_person_patterns:\n","            if re.search(pattern, response, re.IGNORECASE):\n","                issues.append(f\"Uses third person reference: '{pattern}'\")\n","\n","        # Check for placeholders\n","        placeholder_patterns = [\n","            r'\\[recruiter name\\]',\n","            r'\\[name\\]',\n","            r'\\[.*?\\]',  # Any text in square brackets\n","        ]\n","\n","        for pattern in placeholder_patterns:\n","            if re.search(pattern, response, re.IGNORECASE):\n","                issues.append(f\"Contains placeholder: '{pattern}'\")\n","\n","        return len(issues) == 0, issues\n","\n","    def _parse_validation_result(self, response: str) -> Tuple[bool, List[str]]:\n","        \"\"\"Parse model validation response\"\"\"\n","        response_lower = response.lower()\n","\n","        # Extract validation status\n","        is_valid = \"validation: pass\" in response_lower or \"pass\" in response_lower\n","\n","        # Extract issues\n","        issues = []\n","        if not is_valid:\n","            # Try to extract issues section\n","            issues_match = re.search(r'issues:\\s*(.+?)(?=\\n|$)', response, re.IGNORECASE | re.DOTALL)\n","            if issues_match:\n","                issues_text = issues_match.group(1).strip()\n","                if issues_text.lower() != \"none\":\n","                    # Split by common delimiters\n","                    issues = [issue.strip() for issue in re.split(r'[,;-]|\\n', issues_text) if issue.strip()]\n","\n","            # Fallback: use rule-based validation\n","            if not issues:\n","                _, issues = self._rule_based_validation(response)\n","\n","        return is_valid, issues\n","\n","    def correct_response(self, original_response: str, issues: List[str], recruiter_message: str) -> str:\n","        \"\"\"Generate corrected response using the model\"\"\"\n","\n","        if not self.model or not self.tokenizer:\n","            return self._rule_based_correction(original_response)\n","\n","        issues_text = \"; \".join(issues) if issues else \"General naturalness improvements needed\"\n","\n","        prompt = self.correction_prompt.format(\n","            original_response=original_response,\n","            issues=issues_text,\n","            recruiter_message=recruiter_message\n","        )\n","\n","        try:\n","            inputs = self.tokenizer(\n","                prompt,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=1024\n","            )\n","\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=600,\n","                    do_sample=True,\n","                    temperature=0.5,\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode corrected response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            corrected_response = full_response.replace(prompt, \"\").strip()\n","\n","            return corrected_response\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Correction error: {str(e)}. Using fallback correction.\")\n","            return self._rule_based_correction(original_response)\n","\n","    def _rule_based_correction(self, response: str) -> str:\n","        \"\"\"Fallback rule-based correction\"\"\"\n","        corrected = response.strip()\n","\n","        # Remove unwanted prefixes that shouldn't appear in final responses\n","        unwanted_prefixes = [\n","            r'^RESPONSE:\\s*',\n","            r'^Response:\\s*',\n","            r'^Cristopher:\\s*',\n","            r'^[A-Za-z]+:\\s*',  # Any \"Name:\" pattern at start\n","        ]\n","\n","        for prefix in unwanted_prefixes:\n","            corrected = re.sub(prefix, '', corrected, flags=re.IGNORECASE | re.MULTILINE)\n","\n","        # Fix third person references\n","        corrections = [\n","            (r'\\bthe candidate\\'s\\b', 'my'),\n","            (r'\\bthe candidate\\b', 'I'),\n","            (r'\\bcandidate\\'s\\b', 'my'),\n","            (r'\\bhis/her\\b', 'my'),\n","            (r'\\bhis or her\\b', 'my'),\n","        ]\n","\n","        for pattern, replacement in corrections:\n","            corrected = re.sub(pattern, replacement, corrected, flags=re.IGNORECASE)\n","\n","        # Remove ALL types of placeholders more aggressively\n","        placeholder_patterns = [\n","            r'\\[recruiter name\\]',\n","            r'\\[name\\]',\n","            r'\\[recruiter\\]',\n","            r'\\[[^\\]]*\\]',  # Any text in brackets\n","            r'Dear \\[.*?\\],?',  # \"Dear [...],\" patterns\n","            r'Hello \\[.*?\\],?',  # \"Hello [...],\" patterns\n","        ]\n","\n","        for pattern in placeholder_patterns:\n","            corrected = re.sub(pattern, '', corrected, flags=re.IGNORECASE)\n","\n","        # Clean up multiple spaces but preserve line breaks\n","        corrected = re.sub(r' +', ' ', corrected)\n","\n","        # Clean up multiple newlines but preserve single line breaks\n","        corrected = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', corrected)\n","\n","        # Cut everything after final signature to remove unwanted instructions/metadata\n","        # Multiple patterns to catch different signature formats and instruction leaks\n","        cutoff_patterns = [\n","            r'(.*?Best regards,\\s*Cristopher).*',  # Standard signature\n","            r'(.*?Cristopher)(?:\\s*\\n\\s*INSTRUCTIONS.*)',  # Instructions leak\n","            r'(.*?Cristopher)(?:\\s*\\n\\s*IF MATCH SCORE.*)',  # Prompt leak\n","            r'(.*?Cristopher)(?:\\s*\\n\\s*Generate a natural.*)',  # Generation instruction leak\n","            r'(.*?,\\s*Cristopher).*',  # Any comma + Cristopher format\n","        ]\n","\n","        for pattern in cutoff_patterns:\n","            match = re.search(pattern, corrected, re.DOTALL | re.IGNORECASE)\n","            if match:\n","                corrected = match.group(1).strip()\n","                break\n","\n","        # Remove leading/trailing whitespace from each line while preserving structure\n","        lines = corrected.split('\\n')\n","        cleaned_lines = [line.strip() for line in lines if line.strip()]\n","        corrected = '\\n'.join(cleaned_lines)\n","\n","        return corrected.strip()\n","\n","    def validate_and_improve_response(self, original_response: str, recruiter_message: str, max_iterations: int = 5) -> str:\n","        \"\"\"Main method: validate and iteratively improve response naturalness\"\"\"\n","\n","        print(f\"   üîç Validating response naturalness...\")\n","\n","        current_response = original_response\n","        iteration = 0\n","        validation_errors = 0\n","\n","        while iteration < max_iterations:\n","            iteration += 1\n","            print(f\"      üîÑ Iteration {iteration}/{max_iterations}\")\n","\n","            # Validate current response\n","            is_valid, issues = self.validate_response(current_response)\n","\n","            if is_valid:\n","                print(f\"      ‚úÖ Response passed validation on iteration {iteration}\")\n","                return current_response\n","\n","            print(f\"      ‚ö†Ô∏è Issues found: {'; '.join(issues)}\")\n","\n","            # If validation keeps failing due to model errors, use rule-based correction\n","            if any(\"DynamicCache\" in str(issue) or \"get_max_length\" in str(issue) for issue in issues):\n","                validation_errors += 1\n","                if validation_errors >= 2:\n","                    print(f\"      üîß Multiple validation errors detected. Using rule-based correction...\")\n","                    return self._rule_based_correction(current_response)\n","\n","            # Correct the response\n","            current_response = self.correct_response(current_response, issues, recruiter_message)\n","\n","        # If we reach here, we've exhausted max_iterations\n","        print(f\"      üö® Max iterations ({max_iterations}) reached. Using rule-based correction...\")\n","\n","        # Use rule-based correction as final fallback\n","        final_response = self._rule_based_correction(original_response)\n","\n","        return final_response\n","\n","# Initialize Output Guardrail\n","output_guardrail = OutputGuardrail(cache_path=cache_path)\n","\n","print(\"üõ°Ô∏è Output Guardrail system initialized!\")\n","print(\"üìù Response validation: First person usage, no placeholders, natural tone\")\n","print(\"üîÑ Iterative improvement: Up to k=5 iterations\")\n","print(\"üéØ Fallback generation: Guardrail creates corrected version if needed\")\n","print(f\"‚ö° Using {output_guardrail_model_name} for validation and correction\")\n","print(\"üí° Alternative models available: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\")\n"]},{"cell_type":"markdown","metadata":{"id":"1PXmGeOhjShl"},"source":["## ***PHASE 5*** - Define 'AI Assistant' object with Guardrail Integration\n","\n","Integrating the input guardrail with the RAG system and implementing the complete business logic including match scoring and state management.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1753719341369,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"oJnkJMusgqNg","outputId":"5509e568-ae22-4da2-e0b8-2fd980702dbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ CORRECTED AIRecruiterAssistantNatural class ready!\n","üîß Fixed issues:\n","   ‚Ä¢ OutputGuardrail constructor: only takes cache_path\n","   ‚Ä¢ Method references: self.input_guardrail instead of self.guardrail\n","   ‚Ä¢ Constructor parameters: removed unnecessary output_guardrail param\n","üöÄ Ready to create ai_assistant object!\n"]}],"source":["class AIRecruiterAssistantNatural:\n","    \"\"\"\n","    AI Assistant that generates natural responses for concrete offers\n","    instead of using fixed templates\n","    \"\"\"\n","\n","    def __init__(self, model_name: str, rag_knowledge_base: RAGKnowledgeBase,\n","                 input_guardrail: InputGuardrailNatural, cache_path: str):\n","        self.model_name = model_name\n","        self.rag_kb = rag_knowledge_base\n","        self.input_guardrail = input_guardrail\n","        self.cache_path = cache_path\n","        self.model = None\n","        self.tokenizer = None\n","\n","        # Load prompts for natural response generation\n","        import sys\n","        import os\n","        import importlib\n","        sys.path.append(f\"{project_path}/prompts/app\")\n","\n","        # Reload prompt_loader module to get latest changes\n","        if 'prompt_loader' in sys.modules:\n","            importlib.reload(sys.modules['prompt_loader'])\n","\n","        from prompt_loader import PromptLoader\n","\n","        prompt_loader = PromptLoader(f\"{project_path}/prompts/app\")\n","        main_prompts = prompt_loader.load_main_generator_prompts()\n","\n","        self.match_scoring_prompt = main_prompts[\"match_scoring_prompt\"]\n","        self.natural_response_prompt = main_prompts[\"natural_response_prompt\"]\n","\n","        # Initialize output guardrail\n","        self.output_guardrail = OutputGuardrail(cache_path=cache_path)\n","\n","    def load_models(self):\n","        \"\"\"Load both guardrail and main model\"\"\"\n","        print(\"üîß Loading models...\")\n","\n","        # Load input guardrail model first\n","        self.input_guardrail.load_model()\n","\n","        # Load output guardrail model\n","        self.output_guardrail.load_model()\n","\n","        # Load main model using benchmark's logic\n","        print(f\"üì• Loading main model: {self.model_name}\")\n","        benchmark = CacheAwareModelBenchmark()\n","        self.model, self.tokenizer = benchmark.load_model_with_quantization(self.model_name)\n","\n","        print(\"‚úÖ All models loaded successfully!\")\n","\n","    def calculate_match_score(self, recruiter_message: str) -> Dict[str, Any]:\n","        \"\"\"Calculate match score between job offer and profile using RAG\"\"\"\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Main model not loaded. Call load_models() first.\")\n","\n","        print(\"üìä Calculating match score...\")\n","\n","        try:\n","            # Get relevant context from RAG\n","            context = self.rag_kb.get_context_string(recruiter_message, k=3)\n","\n","            # Format scoring prompt\n","            scoring_prompt = self.match_scoring_prompt.format(\n","                context=context,\n","                job_offer=recruiter_message\n","            )\n","\n","            inputs = self.tokenizer(scoring_prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=600,\n","                    do_sample=True,\n","                    temperature=0.3,\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and parse scoring response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            scoring_response = full_response.replace(scoring_prompt, \"\").strip()\n","\n","            # Parse the scoring response\n","            match_data = self._parse_match_score(scoring_response)\n","            match_data[\"context_used\"] = context\n","\n","            return match_data\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Error calculating match score: {str(e)}\")\n","            # Fallback scoring based on keywords\n","            return self._fallback_match_scoring(recruiter_message)\n","\n","    def _parse_match_score(self, response: str) -> Dict[str, Any]:\n","        \"\"\"Parse match scoring response from the model\"\"\"\n","        import re\n","\n","        # Extract overall match score\n","        match_score = 50  # Default fallback\n","        match_pattern = re.search(r'MATCH_SCORE:\\s*(\\d+)', response)\n","        if match_pattern:\n","            match_score = int(match_pattern.group(1))\n","            match_score = max(0, min(100, match_score))  # Clamp to [0,100]\n","\n","        # Extract component scores\n","        components = {}\n","        component_patterns = {\n","            \"technical_skills\": r'TECHNICAL_SKILLS:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"role_type\": r'ROLE_TYPE:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"salary\": r'SALARY:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"work_arrangement\": r'WORK_ARRANGEMENT:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)',\n","            \"experience\": r'EXPERIENCE:\\s*(\\d+)\\s*-\\s*(.+?)(?=\\n|$)'\n","        }\n","\n","        for component, pattern in component_patterns.items():\n","            match = re.search(pattern, response, re.IGNORECASE)\n","            if match:\n","                score = int(match.group(1))\n","                reason = match.group(2).strip()\n","                components[component] = {\"score\": score, \"reason\": reason}\n","\n","        # Extract overall reasoning\n","        reasoning_pattern = re.search(r'OVERALL_REASONING:\\s*(.+?)(?=\\n\\n|$)', response, re.DOTALL)\n","        overall_reasoning = reasoning_pattern.group(1).strip() if reasoning_pattern else \"Good potential fit based on available information.\"\n","\n","        return {\n","            \"match_score\": match_score,\n","            \"components\": components,\n","            \"overall_reasoning\": overall_reasoning,\n","            \"raw_response\": response\n","        }\n","\n","    def _fallback_match_scoring(self, message: str) -> Dict[str, Any]:\n","        \"\"\"Fallback match scoring using keyword analysis\"\"\"\n","        message_lower = message.lower()\n","\n","        # Simple keyword-based scoring\n","        score = 50  # Base score\n","\n","        # Technical skills boost\n","        tech_keywords = [\"python\", \"ai\", \"data\", \"engineering\", \"cloud\", \"aws\", \"azure\", \"gcp\", \"langchain\", \"rag\"]\n","        tech_matches = sum(1 for keyword in tech_keywords if keyword in message_lower)\n","        score += min(tech_matches * 5, 25)\n","\n","        # Salary analysis\n","        if any(sal in message_lower for sal in [\"60k\", \"65k\", \"70k\", \"‚Ç¨60\", \"‚Ç¨65\", \"‚Ç¨70\"]):\n","            score += 15\n","        elif any(sal in message_lower for sal in [\"80k\", \"90k\", \"‚Ç¨80\", \"‚Ç¨90\"]):\n","            score += 10\n","\n","        # Remote work boost\n","        if \"remote\" in message_lower:\n","            score += 10\n","\n","        return {\n","            \"match_score\": min(score, 100),\n","            \"components\": {},\n","            \"overall_reasoning\": \"Fallback scoring based on keyword analysis\",\n","            \"raw_response\": \"Fallback analysis used\"\n","        }\n","\n","    def generate_natural_response_for_match(self, match_data: Dict[str, Any], recruiter_message: str, language: str) -> Tuple[str, ConversationState]:\n","        \"\"\"Generate natural response based on match score instead of using templates\"\"\"\n","        if not self.model or not self.tokenizer:\n","            raise ValueError(\"Main model not loaded. Call load_models() first.\")\n","\n","        match_score = match_data[\"match_score\"]\n","        print(f\"   üéØ Match Score: {match_score}%\")\n","\n","        # Determine state based on score\n","        if match_score > 80:\n","            state = ConversationState.PASSED\n","        elif match_score >= 60:\n","            state = ConversationState.STAND_BY\n","        else:\n","            state = ConversationState.FINISHED\n","\n","        # Format prompt for natural response generation\n","        prompt = self.natural_response_prompt.format(\n","            match_score=match_score,\n","            match_analysis=match_data[\"overall_reasoning\"],\n","            recruiter_message=recruiter_message,\n","            language=language\n","        )\n","\n","        try:\n","            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n","            device = next(self.model.parameters()).device\n","            inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=600,  # Increased to prevent text truncation\n","                    do_sample=True,\n","                    temperature=0.7,  # Creative but controlled\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            # Decode and clean response\n","            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            response = full_response.replace(prompt, \"\").strip()\n","\n","            return response, state\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Error generating natural response: {str(e)}\")\n","            # Fallback based on score\n","            if match_score > 80:\n","                return \"Thank you for this opportunity! This looks like an excellent fit for my background. I'd love to discuss this further. When would be a good time for a call?\", state\n","            elif match_score >= 60:\n","                return \"Thanks for reaching out! This opportunity seems interesting. I'd like to review the details more thoroughly. Could you provide some additional information?\", state\n","            else:\n","                return \"Thank you for thinking of me. While this opportunity doesn't align perfectly with my current focus, I appreciate you reaching out.\", state\n","\n","    def process_recruiter_message(self, recruiter_message: str) -> Dict[str, Any]:\n","        \"\"\"Complete end-to-end processing of recruiter message\"\"\"\n","\n","        print(\"üöÄ Processing recruiter message through complete system...\")\n","        print(\"=\"*60)\n","\n","        start_time = time.time()\n","\n","        # Step 1: Input Guardrail Processing - FIXED: correct reference\n","        guardrail_result = self.input_guardrail.process_message(recruiter_message)\n","\n","        if not guardrail_result.should_continue_to_rag:\n","            # Generic message: return guardrail response\n","            total_time = time.time() - start_time\n","            return {\n","                \"final_response\": guardrail_result.response,\n","                \"state\": guardrail_result.state,\n","                \"message_type\": guardrail_result.message_type,\n","                \"confidence\": guardrail_result.confidence,\n","                \"language\": guardrail_result.language,\n","                \"generic_subtype\": guardrail_result.generic_subtype.value if guardrail_result.generic_subtype else None,\n","                \"processing_time\": total_time,\n","                \"pipeline_stage\": \"guardrail_only\"\n","            }\n","\n","        # Step 2: RAG + Match Scoring for concrete offers\n","        print(\"üîç Proceeding to RAG analysis...\")\n","        match_data = self.calculate_match_score(recruiter_message)\n","\n","        # Step 3: Generate natural response based on score\n","        final_response, final_state = self.generate_natural_response_for_match(\n","            match_data, recruiter_message, guardrail_result.language\n","        )\n","\n","        # Step 4: Output Guardrail Processing (for concrete offers that got responses)\n","        print(\"üõ°Ô∏è Processing through output guardrail...\")\n","        final_response = self.output_guardrail.validate_and_improve_response(\n","            final_response, recruiter_message, max_iterations=5\n","        )\n","\n","        total_time = time.time() - start_time\n","\n","        return {\n","            \"final_response\": final_response,\n","            \"state\": final_state,\n","            \"message_type\": guardrail_result.message_type,\n","            \"confidence\": guardrail_result.confidence,\n","            \"language\": guardrail_result.language,\n","            \"match_score\": match_data[\"match_score\"],\n","            \"match_details\": match_data,\n","            \"processing_time\": total_time,\n","            \"pipeline_stage\": \"complete_rag_analysis\"\n","        }\n","\n","print(\"‚úÖ CORRECTED AIRecruiterAssistantNatural class ready!\")\n","print(\"üîß Fixed issues:\")\n","print(\"   ‚Ä¢ OutputGuardrail constructor: only takes cache_path\")\n","print(\"   ‚Ä¢ Method references: self.input_guardrail instead of self.guardrail\")\n","print(\"   ‚Ä¢ Constructor parameters: removed unnecessary output_guardrail param\")\n","print(\"üöÄ Ready to create ai_assistant object!\")\n"]},{"cell_type":"markdown","metadata":{"id":"KZgia5doOoJJ"},"source":["## ***PHASE 6*** - üîß CREATE AI ASSISTANT OBJECT WITH ALL COMPONENTS\n","Now that all components are initialized, create the complete AI assistant"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1911,"status":"ok","timestamp":1753719343282,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"I6EV5Eq5gqNg","outputId":"7202f83c-c1a7-47a1-e645-8c49842760ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîß Creating corrected AI Assistant...\n","‚úÖ AI Assistant created successfully!\n","üîß Corrections applied:\n","   ‚Ä¢ OutputGuardrail constructor: only takes cache_path\n","   ‚Ä¢ Method references: self.input_guardrail instead of self.guardrail\n","   ‚Ä¢ Removed unnecessary output_guardrail parameter from constructor\n","   ‚Ä¢ Output guardrail is now initialized internally\n","\n","ü§ñ AI ASSISTANT READY!\n","‚úÖ Model: mistralai/Mistral-7B-Instruct-v0.3\n","‚úÖ Input Guardrail: microsoft/Phi-3-mini-4k-instruct\n","‚úÖ Output Guardrail: microsoft/Phi-3-mini-4k-instruct\n","‚úÖ RAG Knowledge Base: Ready\n","\n","üöÄ HOW TO USE:\n","1. ai_assistant.load_models()  # Load all models\n","2. test_custom_message('Your message', 'Test Name')  # Test single message\n","3. run_complete_demo()  # Test all scenarios\n"]}],"source":["print(\"üîß Creating corrected AI Assistant...\")\n","\n","try:\n","    # Ensure we have all required components\n","    if 'rag_kb' not in globals():\n","        raise NameError(\"rag_kb not found. Run RAG setup cells first.\")\n","    if 'input_guardrail' not in globals():\n","        raise NameError(\"input_guardrail not found. Run InputGuardrailNatural setup first.\")\n","\n","    # Create AI Assistant with corrected parameters (no output_guardrail parameter)\n","    ai_assistant = AIRecruiterAssistantNatural(\n","        model_name=selected_text_generator_model,\n","        rag_knowledge_base=rag_kb,\n","        input_guardrail=input_guardrail,\n","        cache_path=cache_path\n","    )\n","\n","    print(\"‚úÖ AI Assistant created successfully!\")\n","    print(\"üîß Corrections applied:\")\n","    print(\"   ‚Ä¢ OutputGuardrail constructor: only takes cache_path\")\n","    print(\"   ‚Ä¢ Method references: self.input_guardrail instead of self.guardrail\")\n","    print(\"   ‚Ä¢ Removed unnecessary output_guardrail parameter from constructor\")\n","    print(\"   ‚Ä¢ Output guardrail is now initialized internally\")\n","\n","    print(f\"\\nü§ñ AI ASSISTANT READY!\")\n","    print(f\"‚úÖ Model: {ai_assistant.model_name}\")\n","    print(f\"‚úÖ Input Guardrail: {ai_assistant.input_guardrail.model_name}\")\n","    print(f\"‚úÖ Output Guardrail: {ai_assistant.output_guardrail.model_name}\")\n","    print(f\"‚úÖ RAG Knowledge Base: Ready\")\n","\n","    print(f\"\\nüöÄ HOW TO USE:\")\n","    print(f\"1. ai_assistant.load_models()  # Load all models\")\n","    print(f\"2. test_custom_message('Your message', 'Test Name')  # Test single message\")\n","    print(f\"3. run_complete_demo()  # Test all scenarios\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error creating AI Assistant: {str(e)}\")\n","    print(\"üí° Make sure to run all required setup cells first\")\n","    print(\"   ‚Ä¢ RAG Knowledge Base setup\")\n","    print(\"   ‚Ä¢ InputGuardrailNatural setup\")\n","    print(\"   ‚Ä¢ Model configuration\")\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1753719343317,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"jMdfKYU37SHK","outputId":"df7fb8d9-6981-4c97-df19-b0a09a295b9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","üìÇ ENHANCED PROMPT ORGANIZATION - EXTERNAL FILE STRUCTURE\n","================================================================================\n","\n","üéØ PROMPT ORGANIZATION COMPLETED!\n","‚úÖ All prompts moved from hardcoded to external file structure\n","‚úÖ Following the complete process flow order:\n","\n","üìÅ prompts/app/ structure:\n","   üìÇ 01_input_guardrail/\n","      üìÑ classification_prompt.txt\n","      üìÑ generic_response_template.txt\n","   üìÇ 02_main_generator/\n","      üìÑ match_scoring_prompt.txt\n","      üìÇ response_templates/\n","         üìÑ passed_template.txt\n","         üìÑ stand_by_template.txt\n","         üìÑ finished_template.txt\n","   üìÇ 03_output_guardrail/\n","      üìÑ validation_prompt.txt\n","      üìÑ correction_prompt.txt\n","   üìÑ prompt_loader.py\n","\n","üîß MODIFIED CLASSES:\n","‚úÖ InputGuardrail - Now loads prompts from 01_input_guardrail/\n","‚úÖ OutputGuardrail - Now loads prompts from 03_output_guardrail/\n","‚úÖ AIRecruiterAssistant - Now loads prompts from 02_main_generator/\n","\n","üß™ KEPT IN CODE (for testing only):\n","‚úÖ benchmark.system_prompt - Used for model benchmarking tests\n","‚úÖ benchmark.test_prompts - Used for testing scenarios\n","\n","üí° BENEFITS:\n","‚Ä¢ üéØ Clear separation of concerns - prompts organized by process flow\n","‚Ä¢ üìù Easy prompt editing without code changes\n","‚Ä¢ üîÑ Version control for prompt improvements\n","‚Ä¢ üöÄ Faster iteration on prompt engineering\n","‚Ä¢ üßπ Cleaner, more maintainable code\n","\n","================================================================================\n","üéâ STAGE 3 PROMPT ENGINEERING OPTIMIZATION: COMPLETE!\n","================================================================================\n"]}],"source":["# üìÇ PROMPTS ORGANIZATION - STAGE 3 IMPROVEMENT\n","print(\"=\"*80)\n","print(\"üìÇ ENHANCED PROMPT ORGANIZATION - EXTERNAL FILE STRUCTURE\")\n","print(\"=\"*80)\n","\n","print(\"\\nüéØ PROMPT ORGANIZATION COMPLETED!\")\n","print(\"‚úÖ All prompts moved from hardcoded to external file structure\")\n","print(\"‚úÖ Following the complete process flow order:\")\n","\n","print(\"\\nüìÅ prompts/app/ structure:\")\n","print(\"   üìÇ 01_input_guardrail/\")\n","print(\"      üìÑ classification_prompt.txt\")\n","print(\"      üìÑ generic_response_template.txt\")\n","print(\"   üìÇ 02_main_generator/\")\n","print(\"      üìÑ match_scoring_prompt.txt\")\n","print(\"      üìÇ response_templates/\")\n","print(\"         üìÑ passed_template.txt\")\n","print(\"         üìÑ stand_by_template.txt\")\n","print(\"         üìÑ finished_template.txt\")\n","print(\"   üìÇ 03_output_guardrail/\")\n","print(\"      üìÑ validation_prompt.txt\")\n","print(\"      üìÑ correction_prompt.txt\")\n","print(\"   üìÑ prompt_loader.py\")\n","\n","print(\"\\nüîß MODIFIED CLASSES:\")\n","print(\"‚úÖ InputGuardrail - Now loads prompts from 01_input_guardrail/\")\n","print(\"‚úÖ OutputGuardrail - Now loads prompts from 03_output_guardrail/\")\n","print(\"‚úÖ AIRecruiterAssistant - Now loads prompts from 02_main_generator/\")\n","\n","print(\"\\nüß™ KEPT IN CODE (for testing only):\")\n","print(\"‚úÖ benchmark.system_prompt - Used for model benchmarking tests\")\n","print(\"‚úÖ benchmark.test_prompts - Used for testing scenarios\")\n","\n","print(\"\\nüí° BENEFITS:\")\n","print(\"‚Ä¢ üéØ Clear separation of concerns - prompts organized by process flow\")\n","print(\"‚Ä¢ üìù Easy prompt editing without code changes\")\n","print(\"‚Ä¢ üîÑ Version control for prompt improvements\")\n","print(\"‚Ä¢ üöÄ Faster iteration on prompt engineering\")\n","print(\"‚Ä¢ üßπ Cleaner, more maintainable code\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üéâ STAGE 3 PROMPT ENGINEERING OPTIMIZATION: COMPLETE!\")\n","print(\"=\"*80)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1753719343348,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"_XWLkB0N-zZn","outputId":"30dbee37-d950-4038-8080-55529e156d1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîç VERIFYING AI ASSISTANT OBJECT...\n","‚ùå ai_assistant object incomplete: Input guardrail missing\n"]}],"source":["# üîß VERIFY AI ASSISTANT OBJECT\n","# Confirm that ai_assistant object was created correctly\n","\n","print(\"üîç VERIFYING AI ASSISTANT OBJECT...\")\n","\n","try:\n","    # Check if ai_assistant exists and has all required components\n","    assert hasattr(ai_assistant, 'guardrail'), \"Input guardrail missing\"\n","    assert hasattr(ai_assistant, 'output_guardrail'), \"Output guardrail missing\"\n","    assert hasattr(ai_assistant, 'rag_kb'), \"RAG knowledge base missing\"\n","    assert hasattr(ai_assistant, 'model_name'), \"Model name missing\"\n","\n","    print(\"‚úÖ AI Assistant object verified successfully!\")\n","    print(f\"‚úÖ Model: {ai_assistant.model_name}\")\n","    print(f\"‚úÖ Input Guardrail: {ai_assistant.guardrail.model_name}\")\n","    print(f\"‚úÖ Output Guardrail: {ai_assistant.output_guardrail.model_name}\")\n","    print(f\"‚úÖ RAG Embeddings: {embedding_model_name}\")\n","\n","    print(f\"\\nüöÄ AI ASSISTANT READY FOR USE!\")\n","    print(f\"‚Ä¢ Call ai_assistant.load_models() to load all models\")\n","    print(f\"‚Ä¢ Call test_custom_message() to test individual messages\")\n","    print(f\"‚Ä¢ Call run_complete_demo() to test all scenarios\")\n","\n","except NameError:\n","    print(\"‚ùå ai_assistant object not found - check previous cells\")\n","except AssertionError as e:\n","    print(f\"‚ùå ai_assistant object incomplete: {str(e)}\")\n","except Exception as e:\n","    print(f\"‚ùå Error verifying ai_assistant: {str(e)}\")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753719343355,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"tEIe8QRz-zZl","outputId":"63ce10a5-8c4c-4f26-a137-ebb058246388"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","‚úÖ ENHANCED AI RECRUITER ASSISTANT - COMPLETE PIPELINE READY!\n","================================================================================\n","\n","üõ°Ô∏è INPUT GUARDRAIL (Phi-3-mini-4k-instruct):\n","   ‚Ä¢ Generic vs Concrete message classification\n","   ‚Ä¢ Automatic response for generic messages\n","   ‚Ä¢ Pass concrete offers to RAG analysis\n","\n","üß† MAIN GENERATOR (Mistral-7B-Instruct-v0.3):\n","   ‚Ä¢ RAG-powered match scoring analysis\n","   ‚Ä¢ State-based decision making (passed/stand_by/finished)\n","   ‚Ä¢ Context-aware professional response generation\n","\n","üõ°Ô∏è OUTPUT GUARDRAIL (Meta-Llama-3-8B-Instruct):\n","   ‚Ä¢ Validates response naturalness (first person usage)\n","   ‚Ä¢ Removes placeholders like '[recruiter name]'\n","   ‚Ä¢ Iterative improvement (up to k=5 iterations)\n","   ‚Ä¢ Fallback correction if max iterations reached\n","\n","üìä BUSINESS LOGIC IMPLEMENTED:\n","   ‚Ä¢ Generic messages ‚Üí 'pending_details' ‚Üí Request more information\n","   ‚Ä¢ High match (>80%) ‚Üí 'passed' ‚Üí Schedule call\n","   ‚Ä¢ Medium match (60-80%) ‚Üí 'stand_by' ‚Üí Manual review\n","   ‚Ä¢ Low match (<60%) ‚Üí 'finished' ‚Üí Polite decline\n","\n","üîß ALTERNATIVE MODELS AVAILABLE:\n","   ‚Ä¢ Output Guardrail alternatives: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\n","   ‚Ä¢ All models cached for faster loading\n","\n","üöÄ SYSTEM READY FOR TESTING!\n","================================================================================\n"]}],"source":["# üìã COMPLETE SYSTEM SUMMARY\n","print(\"=\"*80)\n","print(\"‚úÖ ENHANCED AI RECRUITER ASSISTANT - COMPLETE PIPELINE READY!\")\n","print(\"=\"*80)\n","\n","print(\"\\nüõ°Ô∏è INPUT GUARDRAIL (Phi-3-mini-4k-instruct):\")\n","print(\"   ‚Ä¢ Generic vs Concrete message classification\")\n","print(\"   ‚Ä¢ Automatic response for generic messages\")\n","print(\"   ‚Ä¢ Pass concrete offers to RAG analysis\")\n","\n","print(\"\\nüß† MAIN GENERATOR (Mistral-7B-Instruct-v0.3):\")\n","print(\"   ‚Ä¢ RAG-powered match scoring analysis\")\n","print(\"   ‚Ä¢ State-based decision making (passed/stand_by/finished)\")\n","print(\"   ‚Ä¢ Context-aware professional response generation\")\n","\n","print(\"\\nüõ°Ô∏è OUTPUT GUARDRAIL (Meta-Llama-3-8B-Instruct):\")\n","print(\"   ‚Ä¢ Validates response naturalness (first person usage)\")\n","print(\"   ‚Ä¢ Removes placeholders like '[recruiter name]'\")\n","print(\"   ‚Ä¢ Iterative improvement (up to k=5 iterations)\")\n","print(\"   ‚Ä¢ Fallback correction if max iterations reached\")\n","\n","print(\"\\nüìä BUSINESS LOGIC IMPLEMENTED:\")\n","print(\"   ‚Ä¢ Generic messages ‚Üí 'pending_details' ‚Üí Request more information\")\n","print(\"   ‚Ä¢ High match (>80%) ‚Üí 'passed' ‚Üí Schedule call\")\n","print(\"   ‚Ä¢ Medium match (60-80%) ‚Üí 'stand_by' ‚Üí Manual review\")\n","print(\"   ‚Ä¢ Low match (<60%) ‚Üí 'finished' ‚Üí Polite decline\")\n","\n","print(\"\\nüîß ALTERNATIVE MODELS AVAILABLE:\")\n","print(\"   ‚Ä¢ Output Guardrail alternatives: google/gemma-3-4b-it, microsoft/Phi-3-mini-4k-instruct\")\n","print(\"   ‚Ä¢ All models cached for faster loading\")\n","\n","print(\"\\nüöÄ SYSTEM READY FOR TESTING!\")\n","print(\"=\"*80)\n"]},{"cell_type":"markdown","metadata":{"id":"pMsQrWNljShm"},"source":["## ***PHASE 7*** - Complete System Testing\n","\n","Testing the full pipeline with different recruiter message scenarios to validate the guardrail and RAG integration.\n"]},{"cell_type":"markdown","metadata":{"id":"iPkWvNZa-zZn"},"source":["### üß™ COMPREHENSIVE TESTING SUITE\n","Testing all scenarios: Generic messages, High/Medium/Low match concrete offers"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753719343362,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"C2kasTW4jShm","outputId":"4affcfbd-698f-483f-f203-f9c915534ee4"},"outputs":[{"output_type":"stream","name":"stdout","text":["üß™ Complete Testing Suite Ready!\n","\n","üîß Available functions:\n","   ‚Ä¢ run_complete_demo() - Complete demo with all 4 scenarios\n","   ‚Ä¢ test_custom_message(message, name) - Test specific message\n","\n","üí° Example usage:\n","   run_complete_demo()\n","   test_custom_message('Your message here', 'My Test')\n"]}],"source":["def run_complete_demo():\n","    \"\"\"Complete demonstration of the AI assistant with all business logic scenarios\"\"\"\n","\n","    print(\"=\"*80)\n","    print(\"üöÄ AI RECRUITER ASSISTANT - COMPLETE BUSINESS LOGIC DEMO\")\n","    print(\"=\"*80)\n","\n","    # Load all models\n","    print(\"\\nüì• STEP 1: Loading models...\")\n","    print(\"=\"*80)\n","    ai_assistant.load_models()\n","\n","    # Test scenarios covering all business logic paths\n","    test_scenarios = [\n","        {\n","            \"name\": \"Generic Networking Message\",\n","            \"message\": \"\"\"\n","            Hi Cristopher,\n","\n","            I hope you're doing well! I came across your profile and was impressed by your background.\n","\n","            Are you currently open to new opportunities? I'd love to connect and discuss some exciting possibilities.\n","\n","            Best regards,\n","            Sarah\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as GENERIC ‚Üí State: pending_details\"\n","        },\n","\n","        {\n","            \"name\": \"High Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hi Cristopher,\n","\n","            We have an exciting Senior Data Engineer position at our AI-focused fintech startup.\n","\n","            Role details:\n","            - Building ETL/ELT pipelines with Python and Apache Airflow\n","            - Developing RAG systems using Semantic Kernel\n","            - Working with cloud platforms (Azure/GCP)\n","            - 100% remote work\n","            - Salary: ‚Ç¨60,000-65,000 gross\n","            - Team: 15 engineers, very collaborative culture\n","\n","            Would you be interested in discussing this opportunity?\n","\n","            Best regards,\n","            Maria Rodriguez\n","            Technical Recruiter\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER ‚Üí High match score (>80%) ‚Üí State: passed\"\n","        },\n","\n","        {\n","            \"name\": \"Medium Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hello,\n","\n","            We're looking for a Data Scientist for our e-commerce platform.\n","\n","            Requirements:\n","            - 3+ years experience with Python and machine learning\n","            - Experience with recommendation systems\n","            - SQL and data analysis skills\n","            - Hybrid work (2 days office in Madrid)\n","            - Salary: ‚Ç¨75,000-85,000\n","\n","            The role involves building ML models for customer behavior prediction.\n","\n","            Interested?\n","\n","            Thanks,\n","            Roberto\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER ‚Üí Medium match score (60-80%) ‚Üí State: stand_by\"\n","        },\n","\n","        {\n","            \"name\": \"Low Match Concrete Offer\",\n","            \"message\": \"\"\"\n","            Hi,\n","\n","            We have a Java Backend Developer position available.\n","\n","            Requirements:\n","            - 5+ years Java/Spring Boot experience\n","            - Microservices architecture\n","            - On-site work in London\n","            - Banking domain experience preferred\n","            - Competitive salary\n","\n","            Let me know if you're interested.\n","\n","            Best,\n","            John Smith\n","            \"\"\",\n","            \"expected_outcome\": \"Should be classified as CONCRETE_OFFER ‚Üí Low match score (<60%) ‚Üí State: finished\"\n","        }\n","    ]\n","\n","    print(f\"\\nüß™ STEP 2: Testing {len(test_scenarios)} scenarios...\")\n","    print(\"=\"*80)\n","\n","    results = []\n","\n","    for i, scenario in enumerate(test_scenarios, 1):\n","        print(f\"\\nüì® SCENARIO #{i}: {scenario['name']}\")\n","        print(\"=\"*60)\n","        print(f\"Expected: {scenario['expected_outcome']}\")\n","        print(\"\\nMessage:\")\n","        print(scenario['message'].strip())\n","\n","        print(f\"\\nü§ñ PROCESSING...\")\n","        print(\"-\"*40)\n","\n","        # Process through complete system\n","        try:\n","            result = ai_assistant.process_recruiter_message(scenario['message'])\n","            results.append({**result, \"scenario_name\": scenario['name']})\n","\n","            # Display results\n","            print(f\"\\nüí¨ FINAL RESPONSE:\")\n","            print(\"<<START>>\")\n","            print(result['final_response'])\n","            print(\"<<END>>\")\n","\n","            print(f\"\\nüìä ANALYSIS SUMMARY:\")\n","            print(f\"   üõ°Ô∏è Message Type: {result['message_type'].value}\")\n","            print(f\"   üéØ Final State: {result['state'].value}\")\n","            print(f\"   üìà Confidence: {result['confidence']:.2f}\")\n","            if 'match_score' in result:\n","                print(f\"   üèÜ Match Score: {result['match_score']}%\")\n","            print(f\"   ‚ö° Processing Time: {result['processing_time']:.2f}s\")\n","            print(f\"   üîß Pipeline Stage: {result['pipeline_stage']}\")\n","\n","            # Show detailed match analysis for concrete offers\n","            if result['pipeline_stage'] == 'complete_rag_analysis' and 'match_details' in result:\n","                match_details = result['match_details']\n","                if 'components' in match_details and match_details['components']:\n","                    print(f\"\\nüìã DETAILED MATCH BREAKDOWN:\")\n","                    for component, data in match_details['components'].items():\n","                        component_name = component.replace('_', ' ').title()\n","                        score = data.get('score', 'N/A')\n","                        reason = data.get('reason', 'No details')\n","                        print(f\"   ‚Ä¢ {component_name}: {score}% - {reason}\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error processing scenario: {str(e)}\")\n","            results.append({\n","                \"scenario_name\": scenario['name'],\n","                \"error\": str(e),\n","                \"final_response\": f\"Error: {str(e)}\",\n","                \"state\": \"error\"\n","            })\n","\n","        print(\"\\n\" + \"=\"*60)\n","\n","    # Summary\n","    print(f\"\\n‚úÖ TESTING COMPLETE!\")\n","    print(\"=\"*80)\n","\n","    successful_tests = len([r for r in results if 'error' not in r])\n","    print(f\"üìä Results: {successful_tests}/{len(test_scenarios)} scenarios processed successfully\")\n","\n","    # State distribution\n","    states = {}\n","    for result in results:\n","        if 'error' not in result:\n","            state = result['state'].value if hasattr(result['state'], 'value') else str(result['state'])\n","            states[state] = states.get(state, 0) + 1\n","\n","    print(f\"üéØ State Distribution:\")\n","    for state, count in states.items():\n","        print(f\"   ‚Ä¢ {state}: {count} scenario(s)\")\n","\n","    print(f\"\\nüßπ STEP 3: Cleaning up GPU memory...\")\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        print(\"‚úÖ GPU memory cleared\")\n","\n","    return results\n","\n","def test_custom_message(message: str, scenario_name: str = \"Custom Test\"):\n","    \"\"\"Test a single message through the complete system with detailed analysis\"\"\"\n","\n","    print(f\"üß™ TESTING: {scenario_name}\")\n","    print(\"=\"*60)\n","    print(\"Message:\")\n","    print(message.strip())\n","\n","    print(\"\\nüîß Processing through complete system...\")\n","\n","    try:\n","        result = ai_assistant.process_recruiter_message(message)\n","\n","        print(f\"\\nüí¨ RESPONSE:\")\n","        print(\"<<START>>\")\n","        print(result['final_response'])\n","        print(\"<<END>>\")\n","\n","        print(f\"\\nüìä DETAILED ANALYSIS:\")\n","        print(f\"   üõ°Ô∏è Message Classification: {result['message_type'].value}\")\n","        print(f\"   üéØ Final State: {result['state'].value}\")\n","        print(f\"   üìà Classification Confidence: {result['confidence']:.2f}\")\n","\n","        if 'match_score' in result:\n","            print(f\"   üèÜ Match Score: {result['match_score']}%\")\n","            print(f\"   üìã Match Reasoning: {result['match_details'].get('overall_reasoning', 'N/A')}\")\n","\n","        print(f\"   ‚ö° Total Processing Time: {result['processing_time']:.2f}s\")\n","        print(f\"   üîß Pipeline Stage: {result['pipeline_stage']}\")\n","\n","        return result\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error: {str(e)}\")\n","        return {\"error\": str(e)}\n","\n","print(\"üß™ Complete Testing Suite Ready!\")\n","print(\"\\nüîß Available functions:\")\n","print(\"   ‚Ä¢ run_complete_demo() - Complete demo with all 4 scenarios\")\n","print(\"   ‚Ä¢ test_custom_message(message, name) - Test specific message\")\n","print(\"\\nüí° Example usage:\")\n","print(\"   run_complete_demo()\")\n","print(\"   test_custom_message('Your message here', 'My Test')\")\n"]},{"cell_type":"markdown","metadata":{"id":"1PHTT9YC-zZr"},"source":["### üöÄ EXECUTE COMPLETE ENHANCED SYSTEM DEMO\n","Run the full demonstration of the guardrail + RAG system"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1753719343383,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"unr9R1sRjShn","outputId":"b8e718d2-831d-4c76-aa34-983faea85617"},"outputs":[{"output_type":"stream","name":"stdout","text":["üéØ AI RECRUITER ASSISTANT WITH GUARDRAIL\n","================================================================================\n","\n","üõ°Ô∏è SYSTEM COMPONENTS:\n","‚úÖ Input Guardrail: Generic vs Concrete classification\n","‚úÖ RAG Knowledge Base: CV + Job expectations vector store\n","‚úÖ Match Scoring System: Detailed 5-criteria evaluation\n","‚úÖ State Management: pending_details, analyzing, passed, stand_by, finished\n","‚úÖ Response Generation: Context-aware, state-specific templates\n","\n","üìä BUSINESS LOGIC IMPLEMENTED:\n","‚Ä¢ Generic messages ‚Üí State: 'pending_details' ‚Üí Request details\n","‚Ä¢ Concrete offers ‚Üí RAG analysis ‚Üí Match scoring ‚Üí State-based response\n","‚Ä¢ High match (>80%) ‚Üí State: 'passed' ‚Üí Schedule call\n","‚Ä¢ Medium match (60-80%) ‚Üí State: 'stand_by' ‚Üí Manual review\n","‚Ä¢ Low match (<60%) ‚Üí State: 'finished' ‚Üí Polite decline\n","\n","üé≠ TEST SCENARIOS:\n","1. Generic networking message\n","2. High match concrete offer (Data Engineer, Python, Remote, ‚Ç¨60-65k)\n","3. Medium match concrete offer (Data Scientist, different domain)\n","4. Low match concrete offer (Java Developer, on-site)\n","\n","‚è∏Ô∏è Set RUN_COMPLETE_DEMO = True to start the demo\n","Or use: run_complete_demo()\n","\n","üí° For custom testing:\n","test_custom_message('Your recruiter message here', 'Custom Test')\n"]}],"source":["print(\"üéØ AI RECRUITER ASSISTANT WITH GUARDRAIL\")\n","print(\"=\"*80)\n","\n","print(\"\\nüõ°Ô∏è SYSTEM COMPONENTS:\")\n","print(\"‚úÖ Input Guardrail: Generic vs Concrete classification\")\n","print(\"‚úÖ RAG Knowledge Base: CV + Job expectations vector store\")\n","print(\"‚úÖ Match Scoring System: Detailed 5-criteria evaluation\")\n","print(\"‚úÖ State Management: pending_details, analyzing, passed, stand_by, finished\")\n","print(\"‚úÖ Response Generation: Context-aware, state-specific templates\")\n","\n","print(\"\\nüìä BUSINESS LOGIC IMPLEMENTED:\")\n","print(\"‚Ä¢ Generic messages ‚Üí State: 'pending_details' ‚Üí Request details\")\n","print(\"‚Ä¢ Concrete offers ‚Üí RAG analysis ‚Üí Match scoring ‚Üí State-based response\")\n","print(\"‚Ä¢ High match (>80%) ‚Üí State: 'passed' ‚Üí Schedule call\")\n","print(\"‚Ä¢ Medium match (60-80%) ‚Üí State: 'stand_by' ‚Üí Manual review\")\n","print(\"‚Ä¢ Low match (<60%) ‚Üí State: 'finished' ‚Üí Polite decline\")\n","\n","print(\"\\nüé≠ TEST SCENARIOS:\")\n","print(\"1. Generic networking message\")\n","print(\"2. High match concrete offer (Data Engineer, Python, Remote, ‚Ç¨60-65k)\")\n","print(\"3. Medium match concrete offer (Data Scientist, different domain)\")\n","print(\"4. Low match concrete offer (Java Developer, on-site)\")\n","\n","# Set to True to run the complete demo\n","RUN_COMPLETE_DEMO = False # True #\n","\n","if RUN_COMPLETE_DEMO:\n","    print(f\"\\nüöÄ STARTING COMPLETE SYSTEM DEMO...\\n\")\n","\n","    try:\n","        results = run_complete_demo()\n","\n","        print(f\"\\nüéâ DEMO COMPLETED SUCCESSFULLY!\")\n","        print(\"=\"*80)\n","        print(\"‚úÖ All business logic scenarios tested\")\n","        print(\"‚úÖ Guardrail integration working correctly\")\n","        print(\"‚úÖ RAG pipeline functioning properly\")\n","        print(\"‚úÖ Match scoring system operational\")\n","        print(\"‚úÖ State management implemented\")\n","        print(\"\\nüöÄ Ready for Stage 4: Application Integration!\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Demo failed with error: {str(e)}\")\n","        print(f\"üí° Check models are loaded and try again\")\n","\n","else:\n","    print(f\"\\n‚è∏Ô∏è Set RUN_COMPLETE_DEMO = True to start the demo\")\n","    print(f\"Or use: run_complete_demo()\")\n","\n","print(\"\\nüí° For custom testing:\")\n","print(\"test_custom_message('Your recruiter message here', 'Custom Test')\")\n"]},{"cell_type":"markdown","metadata":{"id":"c24hPrS9-zZs"},"source":["### üöÄ TEST COMPLETE SYSTEM WITH OUTPUT GUARDRAIL\n","Test the same message that previously had problems to verify the output guardrail fixes them\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1753719343385,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"aTOmT2kF8X0v"},"outputs":[],"source":["RUN_TEST_CUSTOM_MESSAGE = False"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1753719343407,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"x9AJUUq2-zZs"},"outputs":[],"source":["if RUN_TEST_CUSTOM_MESSAGE:\n","    print(\"üß™ TESTING COMPLETE SYSTEM WITH OUTPUT GUARDRAIL\")\n","    print(\"=\"*80)\n","\n","    # Test the message that previously generated \"the candidate's\" instead of \"my\"\n","    test_message = \"\"\"\n","    Exciting REMOTE GenAI Opportunity ‚Äì Long-Term Contract with Virtusa\n","    Hi Cristopher,\n","\n","    I hope you're doing well!\n","\n","    I'm reaching out regarding an exciting opportunity for a Spanish-speaking GenAI Engineer with GCP on a long-term B2B contract with Virtusa. This is a REMOTE role and it's an urgent requirement.\n","\n","    We're specifically looking for someone with experience in:\n","\n","    Dialogflow CX and GCP\n","    Contact Center AI (CCAI)\n","    Visual flow design\n","    Native GCP integration (BigQuery, Cloud Functions, etc.)\n","    Fluent Spanish ‚Äì This is a must-have\n","\n","    If this sounds like a good fit or if you know someone in your network who might be interested, I'd love to connect and share more details.\n","\n","    Looking forward to hearing from you!!\n","\n","    Best regards,\n","    Priyanka\n","    \"\"\"\n","\n","    print(\"üì® TEST MESSAGE:\")\n","    print(test_message.strip())\n","    print(\"\\n\" + \"=\"*60)\n","\n","    print(\"üöÄ PROCESSING WITH OUTPUT GUARDRAIL...\")\n","    print(\"Expected: Should generate natural first-person response without 'the candidate' references\")\n","    print(\"\\nüîß Processing...\")\n","\n","    try:\n","        # This will now go through the complete pipeline including output guardrail\n","        result = test_custom_message(test_message, \"Output Guardrail Test\")\n","\n","        print(\"\\n‚úÖ TESTING COMPLETE!\")\n","        print(\"üîç Check the response above to verify it uses 'I', 'my', 'me' instead of 'the candidate'\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error during test: {str(e)}\")\n","        print(\"üí° Make sure to run ai_assistant.load_models() first if models aren't loaded\")\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"üéØ OUTPUT GUARDRAIL FUNCTIONALITY:\")\n","    print(\"‚Ä¢ Validates first-person usage (I, my, me)\")\n","    print(\"‚Ä¢ Removes placeholders like [recruiter name]\")\n","    print(\"‚Ä¢ Iterates up to k=5 times for improvement\")\n","    print(\"‚Ä¢ Provides fallback correction if needed\")\n","    print(f\"‚Ä¢ Uses '{output_guardrail_model_name}' for validation\")\n","    print(\"=\"*80)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1753719343428,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"V9gIeEgX-zZs","outputId":"77ba99e9-553c-422c-db24-7afbbd2a2ae1"},"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================================\n","üéâ AI RECRUITER ASSISTANT WITH OUTPUT GUARDRAIL - SYSTEM READY!\n","==========================================================================================\n","\n","üèóÔ∏è SYSTEM ARCHITECTURE IMPLEMENTED:\n","   üì® Recruiter Message\n","      ‚Üì\n","   üõ°Ô∏è Input Guardrail ‚Üí Generic/Concrete Classification\n","      ‚Üì\n","   üß† Main Generator ‚Üí RAG Analysis + Response Generation\n","      ‚Üì\n","   üõ°Ô∏è Output Guardrail ‚Üí Naturalness Validation\n","      ‚Üì\n","   üí¨ Final Natural Response\n","\n","‚úÖ COMPONENTS READY:\n","   üõ°Ô∏è Input Guardrail: microsoft/Phi-3-mini-4k-instruct\n","   üß† Main Generator: mistralai/Mistral-7B-Instruct-v0.3\n","   üõ°Ô∏è Output Guardrail: microsoft/Phi-3-mini-4k-instruct\n","   üß† RAG Embeddings: sentence-transformers/all-MiniLM-L6-v2\n","\n","üöÄ HOW TO USE:\n","==================================================\n","\n","1Ô∏è‚É£ LOAD MODELS (Required first step):\n","   ai_assistant.load_models()\n","\n","2Ô∏è‚É£ TEST INDIVIDUAL MESSAGE:\n","   message = 'Your recruiter message here'\n","   result = test_custom_message(message, 'Test Name')\n","\n","3Ô∏è‚É£ RUN COMPLETE DEMO (All 4 scenarios):\n","   results = run_complete_demo()\n","\n","üõ°Ô∏è OUTPUT GUARDRAIL FEATURES:\n","   ‚Ä¢ Converts 'the candidate' ‚Üí 'I'\n","   ‚Ä¢ Converts 'candidate's' ‚Üí 'my'\n","   ‚Ä¢ Removes '[recruiter name]' placeholders\n","   ‚Ä¢ Iterates up to k=5 times for improvement\n","   ‚Ä¢ Provides fallback correction if needed\n","\n","üí° EXAMPLE USAGE:\n","==================================================\n","# Step 1: Load models\n","ai_assistant.load_models()\n","\n","# Step 2: Test message\n","test_message = '''\n","Hi! We have a Data Engineer position using Python and RAG.\n","‚Ç¨65k salary, 100% remote. Interested?\n","'''\n","result = test_custom_message(test_message, 'Quick Test')\n","\n","==========================================================================================\n","üéØ SYSTEM READY - NO MORE SETUP NEEDED!\n","==========================================================================================\n"]}],"source":["# üéØ FINAL SYSTEM READY - HOW TO USE\n","print(\"=\"*90)\n","print(\"üéâ AI RECRUITER ASSISTANT WITH OUTPUT GUARDRAIL - SYSTEM READY!\")\n","print(\"=\"*90)\n","\n","print(\"\\nüèóÔ∏è SYSTEM ARCHITECTURE IMPLEMENTED:\")\n","print(\"   üì® Recruiter Message\")\n","print(\"      ‚Üì\")\n","print(\"   üõ°Ô∏è Input Guardrail ‚Üí Generic/Concrete Classification\")\n","print(\"      ‚Üì\")\n","print(\"   üß† Main Generator ‚Üí RAG Analysis + Response Generation\")\n","print(\"      ‚Üì\")\n","print(\"   üõ°Ô∏è Output Guardrail ‚Üí Naturalness Validation\")\n","print(\"      ‚Üì\")\n","print(\"   üí¨ Final Natural Response\")\n","\n","print(\"\\n‚úÖ COMPONENTS READY:\")\n","print(f\"   üõ°Ô∏è Input Guardrail: {input_guardrail_model_name}\")\n","print(f\"   üß† Main Generator: {selected_text_generator_model}\")\n","print(f\"   üõ°Ô∏è Output Guardrail: {output_guardrail_model_name}\")\n","print(f\"   üß† RAG Embeddings: {embedding_model_name}\")\n","\n","print(\"\\nüöÄ HOW TO USE:\")\n","print(\"=\"*50)\n","\n","print(\"\\n1Ô∏è‚É£ LOAD MODELS (Required first step):\")\n","print(\"   ai_assistant.load_models()\")\n","\n","print(\"\\n2Ô∏è‚É£ TEST INDIVIDUAL MESSAGE:\")\n","print(\"   message = 'Your recruiter message here'\")\n","print(\"   result = test_custom_message(message, 'Test Name')\")\n","\n","print(\"\\n3Ô∏è‚É£ RUN COMPLETE DEMO (All 4 scenarios):\")\n","print(\"   results = run_complete_demo()\")\n","\n","print(\"\\nüõ°Ô∏è OUTPUT GUARDRAIL FEATURES:\")\n","print(\"   ‚Ä¢ Converts 'the candidate' ‚Üí 'I'\")\n","print(\"   ‚Ä¢ Converts 'candidate's' ‚Üí 'my'\")\n","print(\"   ‚Ä¢ Removes '[recruiter name]' placeholders\")\n","print(\"   ‚Ä¢ Iterates up to k=5 times for improvement\")\n","print(\"   ‚Ä¢ Provides fallback correction if needed\")\n","\n","print(\"\\nüí° EXAMPLE USAGE:\")\n","print(\"=\"*50)\n","print(\"# Step 1: Load models\")\n","print(\"ai_assistant.load_models()\")\n","print(\"\")\n","print(\"# Step 2: Test message\")\n","print(\"test_message = '''\")\n","print(\"Hi! We have a Data Engineer position using Python and RAG.\")\n","print(\"‚Ç¨65k salary, 100% remote. Interested?\")\n","print(\"'''\")\n","print(\"result = test_custom_message(test_message, 'Quick Test')\")\n","\n","print(\"\\n\" + \"=\"*90)\n","print(\"üéØ SYSTEM READY - NO MORE SETUP NEEDED!\")\n","print(\"=\"*90)\n"]},{"cell_type":"markdown","metadata":{"id":"mQJ4MRbJtlyl"},"source":["# **Stage 4: Application Integration**"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3207,"status":"ok","timestamp":1753719346636,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"},"user_tz":-120},"id":"0LKUf1XjP6VB"},"outputs":[],"source":["import gradio as gr\n","\n","def process_chat_message(message, history):\n","    \"\"\"\n","    Wrapper function for Gradio ChatInterface that processes recruiter messages\n","    using the existing ai_assistant object.\n","\n","    Args:\n","        message (str): User's input message\n","        history (list): Chat history (standard Gradio ChatInterface parameter)\n","\n","    Returns:\n","        str: Assistant's response to be displayed in the chat\n","    \"\"\"\n","    try:\n","        # Call the core logic using the existing ai_assistant object\n","        result = ai_assistant.process_recruiter_message(message)\n","\n","        # Extract the final_response from the dictionary returned by the method\n","        final_response = result.get('final_response', 'Sorry, we could not process your message. Try again.')\n","\n","        return final_response\n","\n","    except Exception as e:\n","        # Gracefully handle any potential errors during inference\n","        error_message = f\"\"\"There is an error processing your message.\n","\n","Please try again or contact support if the issue persists.\n","\n","Error details: {str(e)}\"\"\"\n","\n","        return error_message\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4f91e6c4715e4451bbbd3bddd08ec2df","f7c4533b9ebb48c7a03c4cf1642cc84d","374b4da300d44bf0ad113f242cb1f6d0","a02b4da265784d30a21058dcab4f1851","f28bab11f761487d9e3c8d202994e6f2","1ff05b23168c490db7f7dbc34a1e147e","39ead44aa4154b08ba04fc2e7d55b46d","2f2228479fa64d3590a0187870221b2d","1ace33b7f7254865ab2e1a2099b4562f","5ebe70a969964600a3fbcebfe1c0d4d8","0bc2063370a847b196ab495b33210c73","92cc742a70c2411794ac623262073106","c027d005bfff40119c3854653dba807d","62b9e685ff994676a866c4ca160efad7","e3d128d91b764f2b8ea4ba260890f4d9","3a9a90beedb54f33b79f00792bbf094a","cfd0812b8812480495e6e4e91c17f14f","e1ee51cf3d7b4f5e9d9a1abe845d7704","5aa454aefd0a4bd99a743e73e1357319","f00a860ede5f47529006aa050d2f21f1","efaea3df42154e2b8bba51641107a421","68ee45bc06344c1ebaf5979c2ac777c5","91365656fa7245c28a67af9a2ef902ce","dbab22729c4746a49f070fcac6fbdaab","0332f358c13649979741416ee2417ac4","0cd7307fee924796afdc2f9a4913d33d","e42b85e098f34e4bb66bcbb9898b4bf5","1f8c404ee3d941689b323029a578d15d","1d73271e5c3c421daa18c5ba494c45e0","ae5d8795d7814fd29b8a53eeeccf2097","906e2c1ef4304c99912fdb2516ba5570","b9e407452b034c37917180ec066d3873","d84455f765ca4b018bc3c66ce3bdc0a6"]},"id":"xak27Ogz7SHP","outputId":"1976abc5-3958-4efc-9cec-443a6460f102"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Setting up AI Assistant for Gradio interface...\n","üìö Setting up RAG Knowledge Base...\n","   ‚úÖ RAG vectorstore already exists!\n","ü§ñ Loading AI Assistant models...\n","üîß Loading models...\n","\n","üõ°Ô∏è Loading guardrail model: microsoft/Phi-3-mini-4k-instruct\n","‚ö° Loading from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f91e6c4715e4451bbbd3bddd08ec2df"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Guardrail model loaded successfully!\n","\n","üõ°Ô∏è Loading output guardrail model: microsoft/Phi-3-mini-4k-instruct\n","‚ö° Loading from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92cc742a70c2411794ac623262073106"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Output guardrail model loaded successfully!\n","üì• Loading main model: mistralai/Mistral-7B-Instruct-v0.3\n","‚ö° Loading mistralai/Mistral-7B-Instruct-v0.3 from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91365656fa7245c28a67af9a2ef902ce"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\t‚úÖ mistralai/Mistral-7B-Instruct-v0.3 loaded successfully!\n","‚úÖ All models loaded successfully!\n","‚úÖ All models loaded successfully!\n","\n","üéØ Creating Gradio Chat Interface...\n","üåê Launching Gradio interface...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-27-2421113.py:45: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot=gr.Chatbot(\n","/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://5fe6cb02faccda0ef6.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://5fe6cb02faccda0ef6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: generic (confidence: 0.90)\n","   üåç Language: English\n","   üîç Sub-type: opportunity_inquiry\n","   üí¨ Generating natural response for opportunity_inquiry in English\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 1.00)\n","   üåç Language: English\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n","   üéØ Match Score: 85%\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"]},{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 1\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.95)\n","   üåç Language: English\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n","   üéØ Match Score: 85%\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 1\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.95)\n","   üåç Language: English\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n","   üéØ Match Score: 80%\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚ö†Ô∏è Issues found: Contains placeholder: '\\[.*?\\]'\n","‚ö†Ô∏è Correction error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback correction.\n","      üîÑ Iteration 2/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 2\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.90)\n","   üåç Language: English\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n","   üéØ Match Score: 75%\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 1\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: generic (confidence: 0.80)\n","   üåç Language: English\n","   üîç Sub-type: opportunity_inquiry\n","   üí¨ Generating natural response for opportunity_inquiry in English\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: concrete_offer (confidence: 0.80)\n","   üåç Language: [Detected Language Name In English]\n","üîç Proceeding to RAG analysis...\n","üìä Calculating match score...\n","   üéØ Match Score: 85%\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üõ°Ô∏è Processing through output guardrail...\n","   üîç Validating response naturalness...\n","      üîÑ Iteration 1/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚ö†Ô∏è Issues found: Contains placeholder: '\\[.*?\\]'\n","‚ö†Ô∏è Correction error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback correction.\n","      üîÑ Iteration 2/5\n","‚ö†Ô∏è Validation error: 'DynamicCache' object has no attribute 'get_max_length'. Using fallback validation.\n","      ‚úÖ Response passed validation on iteration 2\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Processing recruiter message through complete system...\n","============================================================\n","üõ°Ô∏è Processing message through guardrail...\n","   üìä Classification: generic (confidence: 0.90)\n","   üåç Language: [Detected Language Name In English]\n","   üîç Sub-type: basic_introduction\n","   üí¨ Generating natural response for basic_introduction in [Detected Language Name In English]\n"]}],"source":["# First, ensure RAG knowledge base is properly set up\n","print(\"üöÄ Setting up AI Assistant for Gradio interface...\")\n","try:\n","    # Step 1: Setup RAG Knowledge Base if not already done\n","    print(\"üìö Setting up RAG Knowledge Base...\")\n","    if not rag_kb.vectorstore:\n","        print(\"   üîß RAG vectorstore not found, creating it...\")\n","        rag_kb.setup_embeddings()\n","        if cv_content and expectations_content:\n","            rag_kb.load_and_process_documents(cv_content, expectations_content)\n","            rag_kb.create_vectorstore()\n","            print(\"   ‚úÖ RAG vectorstore created successfully!\")\n","        else:\n","            raise ValueError(\"CV or expectations content not found. Please run earlier cells first.\")\n","    else:\n","        print(\"   ‚úÖ RAG vectorstore already exists!\")\n","\n","    # Step 2: Load all models\n","    print(\"ü§ñ Loading AI Assistant models...\")\n","    ai_assistant.load_models()\n","    print(\"‚úÖ All models loaded successfully!\")\n","\n","    # Create and launch the Gradio ChatInterface\n","    print(\"\\nüéØ Creating Gradio Chat Interface...\")\n","\n","    # Create the ChatInterface with the wrapper function\n","    chat_interface = gr.ChatInterface(\n","        fn=process_chat_message,\n","        title=\"ü§ñ AI Recruiter Assistant\",\n","        description=\"\"\"\n","        **Welcome Recruiter!**\n","\n","        **How it works:**\n","        - Send me a message and I'll analyze if it's a good fit\n","\n","        **Try the examples below or send your message!**\n","        \"\"\",\n","        examples=[\n","            \"Hi, are you open to new opportunities?\" #,\n","        #     \"We have a Senior Data Engineer role with Python and Azure, fully remote, ‚Ç¨60-65k salary. Interested?\",\n","        #     \"Looking for a Java developer, 5 years experience, on-site in London. Competitive salary.\",\n","        #     \"Exciting GenAI opportunity with GCP, Dialogflow, remote work, B2B contract. Spanish required.\"\n","        ],\n","        theme=gr.themes.Soft(),\n","        chatbot=gr.Chatbot(\n","            height=800,\n","            show_label=False,\n","            avatar_images=(None, \"ü§ñ\")\n","        )\n","    )\n","\n","    # Launch the interface with public URL and debugging enabled\n","    print(\"üåê Launching Gradio interface...\")\n","    chat_interface.launch(\n","        share=True,  # Create a public URL\n","        debug=True,  # Enable debugging output\n","        server_name=\"0.0.0.0\",  # Allow connections from any IP (important for Colab)\n","        server_port=7860,  # Default Gradio port\n","        show_error=True  # Show detailed error messages\n","    )\n","\n","except Exception as e:\n","    print(f\"‚ùå Error setting up Gradio interface: {str(e)}\")\n","    print(\"\\nTroubleshooting tips:\")\n","    print(\"1. Make sure all previous cells have been executed successfully\")\n","    print(\"2. Verify that the ai_assistant object exists\")\n","    print(\"3. Check if there are any memory issues\")\n","    print(\"4. Try restarting the runtime if problems persist\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ae5363d9e6864cf6bc57ab7388e0f6a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e979e07b7d3644e5a5940169fedc5b01","IPY_MODEL_fddfbf283e014b0c93aeee02f82e5dab","IPY_MODEL_2b0046f4228a4976ac26c0ab4e22d98a"],"layout":"IPY_MODEL_cddfdbe72fe8470bae4d7ef834a3fae8"}},"e979e07b7d3644e5a5940169fedc5b01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f811ee477ea1422c96122b92378b5cad","placeholder":"‚Äã","style":"IPY_MODEL_bb929a13a5f34c9f808dbaef6dd7c557","value":"Fetching‚Äá3‚Äáfiles:‚Äá100%"}},"fddfbf283e014b0c93aeee02f82e5dab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7baa221ea5b49588024f84ec054a5fb","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_671f0855a0aa414195522f32e7d6e0d6","value":3}},"2b0046f4228a4976ac26c0ab4e22d98a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ffb2d3b81b2401ca30e3ff134748ec1","placeholder":"‚Äã","style":"IPY_MODEL_2f5fd3d43a554701b55db2da4533d4d5","value":"‚Äá3/3‚Äá[01:21&lt;00:00,‚Äá27.13s/it]"}},"cddfdbe72fe8470bae4d7ef834a3fae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f811ee477ea1422c96122b92378b5cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb929a13a5f34c9f808dbaef6dd7c557":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7baa221ea5b49588024f84ec054a5fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"671f0855a0aa414195522f32e7d6e0d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ffb2d3b81b2401ca30e3ff134748ec1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f5fd3d43a554701b55db2da4533d4d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfd66d0303e5470d91e6d3dd0287f7d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6bf3e5c2bc34df6966dc2e67c6ae08b","IPY_MODEL_2c6d9ac1d64949e3a931d49185d733ee","IPY_MODEL_f1ab8550a6db452ab266ca07a9cfe548"],"layout":"IPY_MODEL_87778d01aed24355825f176f4f229b69"}},"f6bf3e5c2bc34df6966dc2e67c6ae08b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1774f83e423c4e8d84183483f62af986","placeholder":"‚Äã","style":"IPY_MODEL_f1a9a6b0550445efb4a1fff88f718233","value":"model-00003-of-00003.safetensors:‚Äá100%"}},"2c6d9ac1d64949e3a931d49185d733ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ab1b5f9295405aa072fabc5733ff1d","max":4546807800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35ca562d7933467f9af4dcfb13e6986b","value":4546807800}},"f1ab8550a6db452ab266ca07a9cfe548":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_601bb573462149f39a45c25419f87072","placeholder":"‚Äã","style":"IPY_MODEL_8a2c0bfca5eb44fcb5999b5d30c6c6a4","value":"‚Äá4.55G/4.55G‚Äá[01:20&lt;00:00,‚Äá98.8MB/s]"}},"87778d01aed24355825f176f4f229b69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1774f83e423c4e8d84183483f62af986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1a9a6b0550445efb4a1fff88f718233":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4ab1b5f9295405aa072fabc5733ff1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35ca562d7933467f9af4dcfb13e6986b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"601bb573462149f39a45c25419f87072":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a2c0bfca5eb44fcb5999b5d30c6c6a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b627524d5efd484681213cc7b05f3f54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29d78e477c6a45759f3cad3bfefb69e1","IPY_MODEL_7e7e0d867d7746068995f6ab9f48eaaf","IPY_MODEL_7616f8c57da24c42887dd46343711754"],"layout":"IPY_MODEL_94afb980bf074dc7b7f788389388593b"}},"29d78e477c6a45759f3cad3bfefb69e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e638d5052c8a4ffd8a8017c9f7006d54","placeholder":"‚Äã","style":"IPY_MODEL_1ea852e1450d4dd6b58389e791921089","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"7e7e0d867d7746068995f6ab9f48eaaf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78076e91951a446aa0a336a397fc3d8a","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b20770fbacb410e933963da95a9efff","value":3}},"7616f8c57da24c42887dd46343711754":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75a2bab78682433ab304c2148e06c144","placeholder":"‚Äã","style":"IPY_MODEL_eed85b22d6d040a6bbd8d3523a0521d1","value":"‚Äá3/3‚Äá[02:58&lt;00:00,‚Äá54.05s/it]"}},"94afb980bf074dc7b7f788389388593b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e638d5052c8a4ffd8a8017c9f7006d54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ea852e1450d4dd6b58389e791921089":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78076e91951a446aa0a336a397fc3d8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b20770fbacb410e933963da95a9efff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75a2bab78682433ab304c2148e06c144":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eed85b22d6d040a6bbd8d3523a0521d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2935801f0ff43acac5d91bb956c0e9e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da70c271f7cd4a998b8a180a6ab6b06d","IPY_MODEL_fa0e2a3bce1a4bceb8f22b11760d51b3","IPY_MODEL_87ddc554e84f43f8a5ef326b7c32a1b6"],"layout":"IPY_MODEL_2a88a3c1c97a4f60889c56ba02488b5e"}},"da70c271f7cd4a998b8a180a6ab6b06d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6435fb20c0484ac4befe66806e7eff11","placeholder":"‚Äã","style":"IPY_MODEL_80e0f032a1b440b8a7c5583705e43538","value":"Fetching‚Äá4‚Äáfiles:‚Äá100%"}},"fa0e2a3bce1a4bceb8f22b11760d51b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe52e7cacf4644208d3af3da9f33d208","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dcba57ad5b78468d845c840bd666ece3","value":4}},"87ddc554e84f43f8a5ef326b7c32a1b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dec2e090c834aff885daef6a176305e","placeholder":"‚Äã","style":"IPY_MODEL_352e37c8efb240b099fafe5af0ec0545","value":"‚Äá4/4‚Äá[02:02&lt;00:00,‚Äá61.07s/it]"}},"2a88a3c1c97a4f60889c56ba02488b5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6435fb20c0484ac4befe66806e7eff11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80e0f032a1b440b8a7c5583705e43538":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe52e7cacf4644208d3af3da9f33d208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcba57ad5b78468d845c840bd666ece3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9dec2e090c834aff885daef6a176305e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"352e37c8efb240b099fafe5af0ec0545":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6dd7c5cef973461fa74ea6021a2c6092":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f9ed69e484f421baadf511c043c30e2","IPY_MODEL_3086d98dcc844c25874d3aae2ab993b4","IPY_MODEL_25def8d855d04fe083e3d48c45f46383"],"layout":"IPY_MODEL_7aae87de00a34c42b1e837ea316eb622"}},"2f9ed69e484f421baadf511c043c30e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd1de4fc8e3a46a7b1c12b6a82c1f4f5","placeholder":"‚Äã","style":"IPY_MODEL_2d4754b8883d4e3a8569f62117977b06","value":"model-00002-of-00004.safetensors:‚Äá100%"}},"3086d98dcc844c25874d3aae2ab993b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4e4f6b52d1e416aadebd43043236a7f","max":4999802720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed0deac82bc3414885b99f38fea9b62c","value":4999802720}},"25def8d855d04fe083e3d48c45f46383":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bd406c509c34a75be063ca727969270","placeholder":"‚Äã","style":"IPY_MODEL_0f459c30526246e3ae5ad8e25341de7d","value":"‚Äá5.00G/5.00G‚Äá[01:58&lt;00:00,‚Äá373MB/s]"}},"7aae87de00a34c42b1e837ea316eb622":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd1de4fc8e3a46a7b1c12b6a82c1f4f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d4754b8883d4e3a8569f62117977b06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4e4f6b52d1e416aadebd43043236a7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed0deac82bc3414885b99f38fea9b62c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bd406c509c34a75be063ca727969270":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f459c30526246e3ae5ad8e25341de7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a02c069cf824c36b6658302c3864926":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d568e9d379e24fed8fd1eec6103e8a9c","IPY_MODEL_ea24f011d5364d1dbc4ff3d72b5e3f6b","IPY_MODEL_d20ec073986640299da70ba99159f43c"],"layout":"IPY_MODEL_2c22dcf687804a6484aa1eefdaa1b3d5"}},"d568e9d379e24fed8fd1eec6103e8a9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0f6a91b4bfe48568176c947b364b6cf","placeholder":"‚Äã","style":"IPY_MODEL_c5cff93404f44afc95942d4ad4a91676","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"ea24f011d5364d1dbc4ff3d72b5e3f6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cfbacb159de426da8e79dd8fbcf2622","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0aaeb596c7614d17ad42b74c8bd127ec","value":4}},"d20ec073986640299da70ba99159f43c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f623623f6c4ebabb6763c1d9b22dcf","placeholder":"‚Äã","style":"IPY_MODEL_b17459cf0530441c873504e5b8f4ac53","value":"‚Äá4/4‚Äá[02:27&lt;00:00,‚Äá34.78s/it]"}},"2c22dcf687804a6484aa1eefdaa1b3d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0f6a91b4bfe48568176c947b364b6cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5cff93404f44afc95942d4ad4a91676":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7cfbacb159de426da8e79dd8fbcf2622":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aaeb596c7614d17ad42b74c8bd127ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4f623623f6c4ebabb6763c1d9b22dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b17459cf0530441c873504e5b8f4ac53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a646d891936432fadc97b871b37d12b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef14da9d636542dcbbb378c58f86c3b6","IPY_MODEL_4e72518518ef47afa79875792653565e","IPY_MODEL_fdfacfe8878d4229bfe7920675f2598a"],"layout":"IPY_MODEL_e3dd7fc279c54b49a1043c8ef76aafb6"}},"ef14da9d636542dcbbb378c58f86c3b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c1be4c2f3fd490e8ee1efbe3894de44","placeholder":"‚Äã","style":"IPY_MODEL_82d845236f22418f9ba9139aa0d63e8e","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"4e72518518ef47afa79875792653565e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d99801afc3924ed5ae94ba29b74baf5c","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60beaeda9f49483b82713e47cac7a389","value":2}},"fdfacfe8878d4229bfe7920675f2598a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25b2db45d3694e5e9071a403e0e04ff5","placeholder":"‚Äã","style":"IPY_MODEL_8849586aaa4641e5a24ab3d4dacb5f53","value":"‚Äá2/2‚Äá[02:23&lt;00:00,‚Äá64.81s/it]"}},"e3dd7fc279c54b49a1043c8ef76aafb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c1be4c2f3fd490e8ee1efbe3894de44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82d845236f22418f9ba9139aa0d63e8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d99801afc3924ed5ae94ba29b74baf5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60beaeda9f49483b82713e47cac7a389":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25b2db45d3694e5e9071a403e0e04ff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8849586aaa4641e5a24ab3d4dacb5f53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83ffc8c857a64a708ea74fa4965a40d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d32f33321a0846db8637cd375ee51a0f","IPY_MODEL_a515e53896494473947e23532c1fd059","IPY_MODEL_db5cd1c7bb2a49e98e90fefff1e2023a"],"layout":"IPY_MODEL_4b1dc54500e64f49ac2849bdf5aa2471"}},"d32f33321a0846db8637cd375ee51a0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64a20c7992c64d0b94a385ba93e6289f","placeholder":"‚Äã","style":"IPY_MODEL_b9872f70163c452bbfb8ae2139638437","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"a515e53896494473947e23532c1fd059":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_330243f7edb04e36b550e959eca04e34","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec22fe9a0f644e0490b9b8c005b010b3","value":2}},"db5cd1c7bb2a49e98e90fefff1e2023a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7037bbd016c4b21b15c08d9c92849c4","placeholder":"‚Äã","style":"IPY_MODEL_7eecfb8ef0e54d05943af7137a00cb7a","value":"‚Äá2/2‚Äá[03:11&lt;00:00,‚Äá94.32s/it]"}},"4b1dc54500e64f49ac2849bdf5aa2471":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64a20c7992c64d0b94a385ba93e6289f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9872f70163c452bbfb8ae2139638437":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"330243f7edb04e36b550e959eca04e34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec22fe9a0f644e0490b9b8c005b010b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7037bbd016c4b21b15c08d9c92849c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eecfb8ef0e54d05943af7137a00cb7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d229179a8af64077b62269ecba533269":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e98d42dbedaf426689a45290decd90b5","IPY_MODEL_a82c589b677148379c93b558fdcd4054","IPY_MODEL_8465a585e90343d8843c88f6965e69b2"],"layout":"IPY_MODEL_c8700cb220594766b1b8d9965a6438c8"}},"e98d42dbedaf426689a45290decd90b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f3022ce587e4efa869f5614addf4244","placeholder":"‚Äã","style":"IPY_MODEL_8d7f65097cd04a71bb57020c0f471552","value":"modules.json:‚Äá100%"}},"a82c589b677148379c93b558fdcd4054":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d09f673babb941ef9de239403a1f8efb","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1dd634680fa6420486f68d7e17c1ac75","value":349}},"8465a585e90343d8843c88f6965e69b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46215043fb624444826b7d4656252524","placeholder":"‚Äã","style":"IPY_MODEL_123abfd1f199449cbff78dbe8853aab9","value":"‚Äá349/349‚Äá[00:00&lt;00:00,‚Äá45.8kB/s]"}},"c8700cb220594766b1b8d9965a6438c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f3022ce587e4efa869f5614addf4244":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d7f65097cd04a71bb57020c0f471552":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d09f673babb941ef9de239403a1f8efb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dd634680fa6420486f68d7e17c1ac75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46215043fb624444826b7d4656252524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"123abfd1f199449cbff78dbe8853aab9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58cba4a8aca24aa09412c2b09946aa75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df206ed716a84de2ac77ab03d2b64cad","IPY_MODEL_f6b0f45a38b4492f93061b26411c5d0d","IPY_MODEL_527851a71382483ca47c8936abf51a4f"],"layout":"IPY_MODEL_93691947a7e04cc28fa90fc1d849ba11"}},"df206ed716a84de2ac77ab03d2b64cad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7804daf4e7842d1a3aa444acb796c38","placeholder":"‚Äã","style":"IPY_MODEL_d22dc4350bac4c9cb9ae7d1fa1f074c4","value":"config_sentence_transformers.json:‚Äá100%"}},"f6b0f45a38b4492f93061b26411c5d0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e203501743fa4454853f98f6894ef7fe","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2675bdf6c37b4787b28829072fd64349","value":116}},"527851a71382483ca47c8936abf51a4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a44d6bf3c00b43dfbbadaea317ecaadc","placeholder":"‚Äã","style":"IPY_MODEL_f0b677e591204a26b77cc24308ba3bd6","value":"‚Äá116/116‚Äá[00:00&lt;00:00,‚Äá13.0kB/s]"}},"93691947a7e04cc28fa90fc1d849ba11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7804daf4e7842d1a3aa444acb796c38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d22dc4350bac4c9cb9ae7d1fa1f074c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e203501743fa4454853f98f6894ef7fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2675bdf6c37b4787b28829072fd64349":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a44d6bf3c00b43dfbbadaea317ecaadc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0b677e591204a26b77cc24308ba3bd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a57dfc9886b4c0bbf76bf852ec3fd94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfd6c6d72b84487c9cc8a1a2884255cf","IPY_MODEL_bf395abc1d41471c91537fd17d3ffdd1","IPY_MODEL_29e3736f5b9e45b792632434390d5bc8"],"layout":"IPY_MODEL_1ce2ad925dc64f8c83243c8238eb0660"}},"bfd6c6d72b84487c9cc8a1a2884255cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db751d494b3446dda7f5fa508267d61f","placeholder":"‚Äã","style":"IPY_MODEL_9b585bb1c7074d1bab0c079fffab08df","value":"README.md:‚Äá"}},"bf395abc1d41471c91537fd17d3ffdd1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea8d113d7fb4f20908daa7a6fa82928","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41f4369bef3743cd87c20a5729e3651b","value":1}},"29e3736f5b9e45b792632434390d5bc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64ff1f5832bd44d8bfdcd5d69e57644a","placeholder":"‚Äã","style":"IPY_MODEL_655cfa6ebac946f19f46597917738734","value":"‚Äá10.5k/?‚Äá[00:00&lt;00:00,‚Äá634kB/s]"}},"1ce2ad925dc64f8c83243c8238eb0660":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db751d494b3446dda7f5fa508267d61f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b585bb1c7074d1bab0c079fffab08df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ea8d113d7fb4f20908daa7a6fa82928":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"41f4369bef3743cd87c20a5729e3651b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64ff1f5832bd44d8bfdcd5d69e57644a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"655cfa6ebac946f19f46597917738734":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee485db51b8746c58385c5be61496552":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b47346c7428543b5b3d82faa4c2b98d9","IPY_MODEL_c532c2e29136491abc046090299b2681","IPY_MODEL_19aafe239ad04221b34cd2b7f929f406"],"layout":"IPY_MODEL_050ba58105b3489da854c8baeedc0243"}},"b47346c7428543b5b3d82faa4c2b98d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c92d71c942a64b338f357d82058589b7","placeholder":"‚Äã","style":"IPY_MODEL_ef0eb891e4544abca37ebe9b115f5598","value":"sentence_bert_config.json:‚Äá100%"}},"c532c2e29136491abc046090299b2681":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13acd98590f640deb5554a4a23e5e910","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7faac343ff84f67aa89df859c4c7529","value":53}},"19aafe239ad04221b34cd2b7f929f406":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b017c88ad2af4ebca4789bf220ee13b7","placeholder":"‚Äã","style":"IPY_MODEL_7dc1c1e2dbc748f689cc0c4dd71e008d","value":"‚Äá53.0/53.0‚Äá[00:00&lt;00:00,‚Äá7.50kB/s]"}},"050ba58105b3489da854c8baeedc0243":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c92d71c942a64b338f357d82058589b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef0eb891e4544abca37ebe9b115f5598":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13acd98590f640deb5554a4a23e5e910":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7faac343ff84f67aa89df859c4c7529":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b017c88ad2af4ebca4789bf220ee13b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dc1c1e2dbc748f689cc0c4dd71e008d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd4e38a1c3d248c5bcb09e6c2a249727":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a366e8dab7d44551a527662287877188","IPY_MODEL_2513b3029c7d452a8a3ec61999732a4c","IPY_MODEL_6512c4e1a2a04cc48134c68ed87b23ae"],"layout":"IPY_MODEL_9007ab634d76477b899d458207c984b6"}},"a366e8dab7d44551a527662287877188":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e3f257b45f24c53835250a1cd9bfe6a","placeholder":"‚Äã","style":"IPY_MODEL_5c44802ed60940a9a04227cd5aa317cb","value":"config.json:‚Äá100%"}},"2513b3029c7d452a8a3ec61999732a4c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a9cf03a159b48feba1585847d114cbf","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36371a5a4d6347699ffbc189ef2c5ac5","value":612}},"6512c4e1a2a04cc48134c68ed87b23ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e5604cead094796b3d58658b9eb8892","placeholder":"‚Äã","style":"IPY_MODEL_6e44d76c39024c74839bcd4f9b9e5279","value":"‚Äá612/612‚Äá[00:00&lt;00:00,‚Äá82.9kB/s]"}},"9007ab634d76477b899d458207c984b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e3f257b45f24c53835250a1cd9bfe6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c44802ed60940a9a04227cd5aa317cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a9cf03a159b48feba1585847d114cbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36371a5a4d6347699ffbc189ef2c5ac5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e5604cead094796b3d58658b9eb8892":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e44d76c39024c74839bcd4f9b9e5279":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3597072fcbd457f99e90991a5288627":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cacdc256ac6d4507a24638dee923e182","IPY_MODEL_edd4beb1b63a40459fbc298ccf310de5","IPY_MODEL_ebed5e15e4ae4206aeffa09c362ea2e9"],"layout":"IPY_MODEL_7d006a29557e4df5a767726e82db396a"}},"cacdc256ac6d4507a24638dee923e182":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_601caff947b743d497ba5539ced32c2e","placeholder":"‚Äã","style":"IPY_MODEL_3e8d67b1f8c44337be9d0ed2b6d27f39","value":"model.safetensors:‚Äá100%"}},"edd4beb1b63a40459fbc298ccf310de5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01732bd371b443c8b11aa87a5fe6130f","max":90868376,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef6d3a5d258e4b138b8c4d18ca231eb7","value":90868376}},"ebed5e15e4ae4206aeffa09c362ea2e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3a7c594791a41208000e3a7230de57f","placeholder":"‚Äã","style":"IPY_MODEL_e04c899686d4425abb72b8abd3eb3b93","value":"‚Äá90.9M/90.9M‚Äá[00:01&lt;00:00,‚Äá105MB/s]"}},"7d006a29557e4df5a767726e82db396a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"601caff947b743d497ba5539ced32c2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e8d67b1f8c44337be9d0ed2b6d27f39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01732bd371b443c8b11aa87a5fe6130f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef6d3a5d258e4b138b8c4d18ca231eb7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3a7c594791a41208000e3a7230de57f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e04c899686d4425abb72b8abd3eb3b93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3a13b3042f64eb3a8a2354d9bf504a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d7219aad2a140a09b86b19237a57d94","IPY_MODEL_2850f6ce8fb142f5a606817183352b75","IPY_MODEL_018bec3f57ff4d58a9ca8fee04ab7911"],"layout":"IPY_MODEL_9b3b413acb3c46008ab52440c986762d"}},"2d7219aad2a140a09b86b19237a57d94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39310bebaea14fd7a880f86e35f25c3b","placeholder":"‚Äã","style":"IPY_MODEL_b5217f28f3514dd2b4de99ac4736bc09","value":"tokenizer_config.json:‚Äá100%"}},"2850f6ce8fb142f5a606817183352b75":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_600a7b76859f42bc9456393422879b62","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17cb928c0f344892a6ab4972ec092ec4","value":350}},"018bec3f57ff4d58a9ca8fee04ab7911":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bba904455a964104a1319a35ce9aa02e","placeholder":"‚Äã","style":"IPY_MODEL_b1f54995be734b108f48c7f9df8505d5","value":"‚Äá350/350‚Äá[00:00&lt;00:00,‚Äá46.5kB/s]"}},"9b3b413acb3c46008ab52440c986762d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39310bebaea14fd7a880f86e35f25c3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5217f28f3514dd2b4de99ac4736bc09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"600a7b76859f42bc9456393422879b62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17cb928c0f344892a6ab4972ec092ec4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bba904455a964104a1319a35ce9aa02e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1f54995be734b108f48c7f9df8505d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c46910c7e88548b1841db19b337dadc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31192fc8086b49ce882945940b8c737b","IPY_MODEL_1da12db4d9d843e4885d4c3ea7699bad","IPY_MODEL_77e5cf9c41f143caa28ce8f83336196d"],"layout":"IPY_MODEL_832ee8fb468e4bf7b1421334516a47d8"}},"31192fc8086b49ce882945940b8c737b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c705bc8413d849c19143a1175a7d698b","placeholder":"‚Äã","style":"IPY_MODEL_98651d8e32924233ba0151d43f71cee3","value":"vocab.txt:‚Äá"}},"1da12db4d9d843e4885d4c3ea7699bad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1853d5f504b045d9b7864a37497bbdf6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_328457c3da574ae3a9b87322ae0bbe48","value":1}},"77e5cf9c41f143caa28ce8f83336196d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2253457343545cbbac71c8ef53d2c8c","placeholder":"‚Äã","style":"IPY_MODEL_4ed9afb9ba1b4a4588368e469d75c50d","value":"‚Äá232k/?‚Äá[00:00&lt;00:00,‚Äá3.31MB/s]"}},"832ee8fb468e4bf7b1421334516a47d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c705bc8413d849c19143a1175a7d698b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98651d8e32924233ba0151d43f71cee3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1853d5f504b045d9b7864a37497bbdf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"328457c3da574ae3a9b87322ae0bbe48":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2253457343545cbbac71c8ef53d2c8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ed9afb9ba1b4a4588368e469d75c50d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9baf16c5dc2849479d41874d5d258428":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_798fc6d106be45b492650ab8264b41d7","IPY_MODEL_b9429e7cc2b040a9af568008718cdbb8","IPY_MODEL_8b3b2f298fb64522890220b075ac45a4"],"layout":"IPY_MODEL_760602bea18b40f999e9338eae8d85da"}},"798fc6d106be45b492650ab8264b41d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c44c3eb1cd3d445894c030c382d6b660","placeholder":"‚Äã","style":"IPY_MODEL_1d57ed77871a46dba96de9ab838a4e5b","value":"tokenizer.json:‚Äá"}},"b9429e7cc2b040a9af568008718cdbb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_763c6a87662840899b1943715007a2be","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3c982ee1c9f46f7953e5f1de7ef9a64","value":1}},"8b3b2f298fb64522890220b075ac45a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_065e47ea990f4d52b7e0fb41b8d3a197","placeholder":"‚Äã","style":"IPY_MODEL_4a4fd9e38dc34bd39c61eb84fbddfead","value":"‚Äá466k/?‚Äá[00:00&lt;00:00,‚Äá13.3MB/s]"}},"760602bea18b40f999e9338eae8d85da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c44c3eb1cd3d445894c030c382d6b660":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d57ed77871a46dba96de9ab838a4e5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"763c6a87662840899b1943715007a2be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d3c982ee1c9f46f7953e5f1de7ef9a64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"065e47ea990f4d52b7e0fb41b8d3a197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a4fd9e38dc34bd39c61eb84fbddfead":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3896419379354c0b85c044ee79d2f2d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c5e77698cfa4a97be3936fcd83f1898","IPY_MODEL_3be0c1e9927447ebba7544a7926fe437","IPY_MODEL_6c85020bf04b4e34bc487f36bde828d0"],"layout":"IPY_MODEL_5b19cfa0139248ffbc40201ddba9907e"}},"7c5e77698cfa4a97be3936fcd83f1898":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fe21de24e7b4575b9c430ee15fc5e83","placeholder":"‚Äã","style":"IPY_MODEL_823c04f3f87a46ca8fb5dc0608843ac6","value":"special_tokens_map.json:‚Äá100%"}},"3be0c1e9927447ebba7544a7926fe437":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88fb5ba75313401585983a82a581939e","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0b80e0992ee4ef49880b73649822714","value":112}},"6c85020bf04b4e34bc487f36bde828d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3222618da85e47b8a63a5c46833168cd","placeholder":"‚Äã","style":"IPY_MODEL_8f4478f2e3d04df2a0d7954ea530e8d5","value":"‚Äá112/112‚Äá[00:00&lt;00:00,‚Äá15.1kB/s]"}},"5b19cfa0139248ffbc40201ddba9907e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fe21de24e7b4575b9c430ee15fc5e83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"823c04f3f87a46ca8fb5dc0608843ac6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88fb5ba75313401585983a82a581939e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0b80e0992ee4ef49880b73649822714":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3222618da85e47b8a63a5c46833168cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f4478f2e3d04df2a0d7954ea530e8d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86822494d24e4b5abe4ede0e1f7d0f23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3897f11ee3c9407093c0c2ee42735d67","IPY_MODEL_4500144222254e038054080e1c48a7c5","IPY_MODEL_738755022c89480fa87e553bcb08a219"],"layout":"IPY_MODEL_845961f7ae4446a2832dadaa77ab4a5e"}},"3897f11ee3c9407093c0c2ee42735d67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6ce9e8691c84cc590a1eeafa5daa489","placeholder":"‚Äã","style":"IPY_MODEL_a6739e928e6644a6af4b557c08b6709f","value":"config.json:‚Äá100%"}},"4500144222254e038054080e1c48a7c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9790df1d21d249f2b9e2adcc54b5ce43","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c2a80b707d249d5b0d565e31e9ad4a0","value":190}},"738755022c89480fa87e553bcb08a219":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8060424c67b1422c8af813dac8b21126","placeholder":"‚Äã","style":"IPY_MODEL_f10bc514f93a4e6c8af12628aeaf762e","value":"‚Äá190/190‚Äá[00:00&lt;00:00,‚Äá22.6kB/s]"}},"845961f7ae4446a2832dadaa77ab4a5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6ce9e8691c84cc590a1eeafa5daa489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6739e928e6644a6af4b557c08b6709f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9790df1d21d249f2b9e2adcc54b5ce43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c2a80b707d249d5b0d565e31e9ad4a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8060424c67b1422c8af813dac8b21126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f10bc514f93a4e6c8af12628aeaf762e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f91e6c4715e4451bbbd3bddd08ec2df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7c4533b9ebb48c7a03c4cf1642cc84d","IPY_MODEL_374b4da300d44bf0ad113f242cb1f6d0","IPY_MODEL_a02b4da265784d30a21058dcab4f1851"],"layout":"IPY_MODEL_f28bab11f761487d9e3c8d202994e6f2"}},"f7c4533b9ebb48c7a03c4cf1642cc84d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ff05b23168c490db7f7dbc34a1e147e","placeholder":"‚Äã","style":"IPY_MODEL_39ead44aa4154b08ba04fc2e7d55b46d","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"374b4da300d44bf0ad113f242cb1f6d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f2228479fa64d3590a0187870221b2d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ace33b7f7254865ab2e1a2099b4562f","value":2}},"a02b4da265784d30a21058dcab4f1851":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ebe70a969964600a3fbcebfe1c0d4d8","placeholder":"‚Äã","style":"IPY_MODEL_0bc2063370a847b196ab495b33210c73","value":"‚Äá2/2‚Äá[00:08&lt;00:00,‚Äá‚Äá4.14s/it]"}},"f28bab11f761487d9e3c8d202994e6f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ff05b23168c490db7f7dbc34a1e147e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39ead44aa4154b08ba04fc2e7d55b46d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f2228479fa64d3590a0187870221b2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ace33b7f7254865ab2e1a2099b4562f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ebe70a969964600a3fbcebfe1c0d4d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bc2063370a847b196ab495b33210c73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92cc742a70c2411794ac623262073106":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c027d005bfff40119c3854653dba807d","IPY_MODEL_62b9e685ff994676a866c4ca160efad7","IPY_MODEL_e3d128d91b764f2b8ea4ba260890f4d9"],"layout":"IPY_MODEL_3a9a90beedb54f33b79f00792bbf094a"}},"c027d005bfff40119c3854653dba807d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfd0812b8812480495e6e4e91c17f14f","placeholder":"‚Äã","style":"IPY_MODEL_e1ee51cf3d7b4f5e9d9a1abe845d7704","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"62b9e685ff994676a866c4ca160efad7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5aa454aefd0a4bd99a743e73e1357319","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f00a860ede5f47529006aa050d2f21f1","value":2}},"e3d128d91b764f2b8ea4ba260890f4d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efaea3df42154e2b8bba51641107a421","placeholder":"‚Äã","style":"IPY_MODEL_68ee45bc06344c1ebaf5979c2ac777c5","value":"‚Äá2/2‚Äá[00:08&lt;00:00,‚Äá‚Äá3.96s/it]"}},"3a9a90beedb54f33b79f00792bbf094a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfd0812b8812480495e6e4e91c17f14f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1ee51cf3d7b4f5e9d9a1abe845d7704":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5aa454aefd0a4bd99a743e73e1357319":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00a860ede5f47529006aa050d2f21f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"efaea3df42154e2b8bba51641107a421":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ee45bc06344c1ebaf5979c2ac777c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91365656fa7245c28a67af9a2ef902ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dbab22729c4746a49f070fcac6fbdaab","IPY_MODEL_0332f358c13649979741416ee2417ac4","IPY_MODEL_0cd7307fee924796afdc2f9a4913d33d"],"layout":"IPY_MODEL_e42b85e098f34e4bb66bcbb9898b4bf5"}},"dbab22729c4746a49f070fcac6fbdaab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f8c404ee3d941689b323029a578d15d","placeholder":"‚Äã","style":"IPY_MODEL_1d73271e5c3c421daa18c5ba494c45e0","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"0332f358c13649979741416ee2417ac4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae5d8795d7814fd29b8a53eeeccf2097","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_906e2c1ef4304c99912fdb2516ba5570","value":3}},"0cd7307fee924796afdc2f9a4913d33d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9e407452b034c37917180ec066d3873","placeholder":"‚Äã","style":"IPY_MODEL_d84455f765ca4b018bc3c66ce3bdc0a6","value":"‚Äá3/3‚Äá[02:24&lt;00:00,‚Äá38.74s/it]"}},"e42b85e098f34e4bb66bcbb9898b4bf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f8c404ee3d941689b323029a578d15d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d73271e5c3c421daa18c5ba494c45e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae5d8795d7814fd29b8a53eeeccf2097":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"906e2c1ef4304c99912fdb2516ba5570":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9e407452b034c37917180ec066d3873":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d84455f765ca4b018bc3c66ce3bdc0a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}