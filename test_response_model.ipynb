{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOdeJjLovMWAC9Q6gLZUW3q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"809a2b098cab4734aad36a37aa51fdff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf31599aab594699bd61a4c37673b71c","IPY_MODEL_ae19a9914a51486da15e4d9a3cc61a11","IPY_MODEL_8c0443d8963d4fcda4c66ca4cb1ee408"],"layout":"IPY_MODEL_0a94bf7d69654a6bb677759eee571c10"}},"bf31599aab594699bd61a4c37673b71c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_796b1dda15e74cebab19b3836c0ef16d","placeholder":"​","style":"IPY_MODEL_edccb6dcfff0418b95b81278596c8531","value":"Fetching 4 files: 100%"}},"ae19a9914a51486da15e4d9a3cc61a11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d28b3d8fa85f42b9b687b13e8d80d890","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d840ecdc98c4f77b59cac9fecba06f6","value":4}},"8c0443d8963d4fcda4c66ca4cb1ee408":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d192238c24d4ba381e9e777445a1fe0","placeholder":"​","style":"IPY_MODEL_2acca44d71cc49ea837b8e80b1a50345","value":" 4/4 [01:22&lt;00:00, 41.24s/it]"}},"0a94bf7d69654a6bb677759eee571c10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"796b1dda15e74cebab19b3836c0ef16d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edccb6dcfff0418b95b81278596c8531":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d28b3d8fa85f42b9b687b13e8d80d890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d840ecdc98c4f77b59cac9fecba06f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d192238c24d4ba381e9e777445a1fe0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2acca44d71cc49ea837b8e80b1a50345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47488d3fee044f0f96ada4f22cb82746":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3914cc28df741d0893090add21e4e8e","IPY_MODEL_a4c146f55ffd446481f7bdeab696b11f","IPY_MODEL_db6fc2c436884143b7fd5ef2b6bc5417"],"layout":"IPY_MODEL_3b64f18fb3084b4fb7c1bf8381b9dbd1"}},"f3914cc28df741d0893090add21e4e8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0241c2291d9a47a48cab6fbe4a0313c5","placeholder":"​","style":"IPY_MODEL_41b7929153284a829a0b7d29dd0acb0f","value":"model-00002-of-00004.safetensors: 100%"}},"a4c146f55ffd446481f7bdeab696b11f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fce1ca4b00c4434591b8e942f53a6e98","max":4999802720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e21cfc005b7745959438836316ddb989","value":4999802720}},"db6fc2c436884143b7fd5ef2b6bc5417":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67462185733942358204fbcc40181fa8","placeholder":"​","style":"IPY_MODEL_1218307005104954acdd52c79a76c892","value":" 5.00G/5.00G [01:21&lt;00:00, 222MB/s]"}},"3b64f18fb3084b4fb7c1bf8381b9dbd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0241c2291d9a47a48cab6fbe4a0313c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b7929153284a829a0b7d29dd0acb0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fce1ca4b00c4434591b8e942f53a6e98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e21cfc005b7745959438836316ddb989":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67462185733942358204fbcc40181fa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1218307005104954acdd52c79a76c892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37407832e16240a18172d0b93ca14884":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0eb7ebac86a744de9e0e1440a767ffda","IPY_MODEL_df531a08936049acb55cd0671fd1b8d4","IPY_MODEL_a0a48b3d7f8f4c4180731e2e25514801"],"layout":"IPY_MODEL_57bb27f7843e4826a3c9cd8685abe466"}},"0eb7ebac86a744de9e0e1440a767ffda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5029eea6b19e4071885b5e062602ebe7","placeholder":"​","style":"IPY_MODEL_2e4ef5fe41bd4ce39bca85c953a94227","value":"Loading checkpoint shards: 100%"}},"df531a08936049acb55cd0671fd1b8d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2df041c61ceb43fbb3af24d9164c6c4d","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98a194a8c4d44b61a957d22484ee73f7","value":4}},"a0a48b3d7f8f4c4180731e2e25514801":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1baa20af636e453c8b5b908beb0f4f09","placeholder":"​","style":"IPY_MODEL_37478d85f0de4b8aa04ed9d422715796","value":" 4/4 [03:10&lt;00:00, 41.74s/it]"}},"57bb27f7843e4826a3c9cd8685abe466":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5029eea6b19e4071885b5e062602ebe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e4ef5fe41bd4ce39bca85c953a94227":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2df041c61ceb43fbb3af24d9164c6c4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98a194a8c4d44b61a957d22484ee73f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1baa20af636e453c8b5b908beb0f4f09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37478d85f0de4b8aa04ed9d422715796":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ea3a400e10547acb4df41e250d3be89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6d9c04a6f194dd79655845754170eab","IPY_MODEL_444572c2c8c34daa9c623a8574f6f4ee","IPY_MODEL_6c981171316e4f3690639e0653481f5c"],"layout":"IPY_MODEL_93bd240d11724d9baada7bee684113b9"}},"b6d9c04a6f194dd79655845754170eab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b13253ed9c224298b5e6cfb16fb9d064","placeholder":"​","style":"IPY_MODEL_7615e666505d4301b2b511204354dadc","value":"Loading checkpoint shards: 100%"}},"444572c2c8c34daa9c623a8574f6f4ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ad53352b07143f8a41de75010280b42","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4cdc57d62851488fb858f088f1188077","value":2}},"6c981171316e4f3690639e0653481f5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40dfe45e136c46e6ab9a7c3fe667468d","placeholder":"​","style":"IPY_MODEL_1c73a4b5d56948a98bedac64e12899a2","value":" 2/2 [03:17&lt;00:00, 97.29s/it]"}},"93bd240d11724d9baada7bee684113b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b13253ed9c224298b5e6cfb16fb9d064":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7615e666505d4301b2b511204354dadc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ad53352b07143f8a41de75010280b42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cdc57d62851488fb858f088f1188077":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40dfe45e136c46e6ab9a7c3fe667468d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c73a4b5d56948a98bedac64e12899a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Test Models Response Format - Google Colab Version\n","# Copy and paste this code into a Google Colab cell\n","\n","# Install required packages\n","!pip install transformers>=4.36.0 torch>=2.0.0 bitsandbytes>=0.41.0 accelerate>=0.24.0\n","\n","# Core imports\n","import os, json, torch, time\n","from typing import Dict, Any\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","print(\"✅ All dependencies loaded!\")\n","print(f\"🔥 CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klxX_X8shOt8","executionInfo":{"status":"ok","timestamp":1753608537226,"user_tz":-120,"elapsed":92054,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"b27c4dbf-239c-45c6-c0d8-50277e581f51"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ All dependencies loaded!\n","🔥 CUDA available: True\n","🎮 GPU: NVIDIA A100-SXM4-40GB\n","💾 GPU Memory: 42.5 GB\n"]}]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up project paths for caching\n","project_path = \"/content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\"\n","cache_path = f\"{project_path}/huggingface_cache\"\n","\n","# Create directories\n","os.makedirs(cache_path, exist_ok=True)\n","\n","print(f\"✅ Google Drive mounted\")\n","print(f\"📁 Project path: {project_path}\")\n","print(f\"🗂️ Cache path: {cache_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCgilKephQ8i","executionInfo":{"status":"ok","timestamp":1753608649435,"user_tz":-120,"elapsed":112202,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"282c6429-3305-4a3c-ba94-3f2f83fa6574"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","✅ Google Drive mounted\n","📁 Project path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\n","🗂️ Cache path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/huggingface_cache\n"]}]},{"cell_type":"code","source":["\n","# Model loading functions\n","def check_cached_models(cache_path):\n","    \"\"\"Check for cached models\"\"\"\n","    cached_models = []\n","    if not os.path.exists(cache_path):\n","        return cached_models\n","\n","    try:\n","        items = os.listdir(cache_path)\n","        for item in items:\n","            item_path = os.path.join(cache_path, item)\n","            if os.path.isdir(item_path) and not item.startswith('.'):\n","                try:\n","                    contents = os.listdir(item_path)\n","                    has_models_folder = any(f.startswith('models--') for f in contents if os.path.isdir(os.path.join(item_path, f)))\n","                    if has_models_folder:\n","                        cached_models.append(item)\n","                except Exception:\n","                    continue\n","    except Exception:\n","        pass\n","\n","    return cached_models\n","\n","def create_model_cache_dir(model_name: str, cache_path: str) -> str:\n","    \"\"\"Create clean cache directory for a model\"\"\"\n","    model_folder = model_name.replace('/', ' ')\n","    model_cache_dir = os.path.join(cache_path, model_folder)\n","    os.makedirs(model_cache_dir, exist_ok=True)\n","    return model_cache_dir\n","\n","def load_model_with_quantization(model_name: str, cache_path: str):\n","    \"\"\"Load model with cache detection and quantization\"\"\"\n","    cached_models = check_cached_models(cache_path)\n","    model_folder_space = model_name.replace('/', ' ')\n","\n","    if model_folder_space in cached_models:\n","        print(f\"⚡ Loading {model_name} from cache...\")\n","        model_cache_dir = os.path.join(cache_path, model_folder_space)\n","    else:\n","        print(f\"📥 Downloading {model_name}...\")\n","        model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","\n","    # Quantization for efficiency\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16,\n","        bnb_4bit_use_double_quant=False\n","    )\n","\n","    try:\n","        # Load tokenizer\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n","        if tokenizer.pad_token is None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","        # Load model\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            quantization_config=bnb_config,\n","            device_map=\"auto\",\n","            trust_remote_code=True,\n","            cache_dir=model_cache_dir\n","        )\n","\n","        print(f\"✅ {model_name} loaded successfully!\")\n","        return model, tokenizer\n","\n","    except Exception as e:\n","        print(f\"❌ Error loading {model_name}: {str(e)}\")\n","        raise\n","\n","print(\"✅ Model loading functions ready!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHvU-rtvhT-R","executionInfo":{"status":"ok","timestamp":1753608649461,"user_tz":-120,"elapsed":9,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"e5323c25-5452-4fd4-c667-9266b51492c3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Model loading functions ready!\n"]}]},{"cell_type":"code","source":["# Test response generation function\n","def test_model_response(model, tokenizer, model_name: str, prompt: str, max_new_tokens: int = 150):\n","    \"\"\"Test model response and show detailed structure\"\"\"\n","    print(f\"\\n🧪 TESTING MODEL: {model_name}\")\n","    print(\"=\" * 60)\n","    print(f\"📝 INPUT PROMPT:\")\n","    print(f'\"{prompt}\"')\n","    print(\"-\" * 60)\n","\n","    try:\n","        # Tokenize input\n","        inputs = tokenizer(\n","            prompt,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=1024\n","        )\n","\n","        print(f\"📊 TOKENIZATION INFO:\")\n","        print(f\"   Input IDs shape: {inputs['input_ids'].shape}\")\n","        print(f\"   Input tokens count: {inputs['input_ids'].shape[1]}\")\n","\n","        # Move to device\n","        device = next(model.parameters()).device\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        print(f\"   Device: {device}\")\n","\n","        # Generate response\n","        start_time = time.time()\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_p=0.9,\n","                pad_token_id=tokenizer.eos_token_id,\n","                return_dict_in_generate=True,\n","                output_scores=True\n","            )\n","\n","        generation_time = time.time() - start_time\n","\n","        print(f\"\\n⚡ GENERATION INFO:\")\n","        print(f\"   Generation time: {generation_time:.2f}s\")\n","        print(f\"   Output type: {type(outputs)}\")\n","        print(f\"   Output keys: {list(outputs.keys()) if hasattr(outputs, 'keys') else 'No keys (tensor)'}\")\n","\n","        # Extract sequences\n","        if hasattr(outputs, 'sequences'):\n","            sequences = outputs.sequences\n","            print(f\"   Sequences shape: {sequences.shape}\")\n","        else:\n","            sequences = outputs\n","            print(f\"   Sequences shape: {sequences.shape}\")\n","\n","        # Decode full response\n","        full_response = tokenizer.decode(sequences[0], skip_special_tokens=True)\n","\n","        # Extract only new tokens (response without prompt)\n","        generated_response = full_response.replace(prompt, \"\").strip()\n","\n","        print(f\"\\n💬 RESPONSE ANALYSIS:\")\n","        print(f\"   Full response length: {len(full_response)} chars\")\n","        print(f\"   Generated response length: {len(generated_response)} chars\")\n","        print(f\"   New tokens generated: {sequences.shape[1] - inputs['input_ids'].shape[1]}\")\n","\n","        print(f\"\\n📄 FULL RESPONSE:\")\n","        print(\"<<START_FULL>>\")\n","        print(full_response)\n","        print(\"<<END_FULL>>\")\n","\n","        print(f\"\\n✨ GENERATED ONLY:\")\n","        print(\"<<START_GENERATED>>\")\n","        print(generated_response)\n","        print(\"<<END_GENERATED>>\")\n","\n","        # Show token structure for debugging\n","        print(f\"\\n🔍 TOKEN ANALYSIS:\")\n","        input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","        output_tokens = tokenizer.convert_ids_to_tokens(sequences[0])\n","        new_tokens = output_tokens[len(input_tokens):]\n","\n","        print(f\"   Input tokens: {input_tokens[:5]}...{input_tokens[-5:]}\")\n","        print(f\"   New tokens: {new_tokens[:10]}\")\n","\n","        # Return structured result\n","        return {\n","            \"model_name\": model_name,\n","            \"prompt\": prompt,\n","            \"full_response\": full_response,\n","            \"generated_response\": generated_response,\n","            \"generation_time\": generation_time,\n","            \"input_tokens_count\": inputs['input_ids'].shape[1],\n","            \"output_tokens_count\": sequences.shape[1],\n","            \"new_tokens_count\": sequences.shape[1] - inputs['input_ids'].shape[1],\n","            \"device\": str(device)\n","        }\n","\n","    except Exception as e:\n","        print(f\"❌ Error testing {model_name}: {str(e)}\")\n","        return {\"error\": str(e)}\n","\n","print(\"✅ Response testing function ready!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9ILp5ophbrl","executionInfo":{"status":"ok","timestamp":1753608649515,"user_tz":-120,"elapsed":52,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"2732ed6a-0bec-4017-9b82-6fd2105d0522"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Response testing function ready!\n"]}]},{"cell_type":"code","source":["# Define test prompts\n","test_prompts = [\n","    {\n","        \"name\": \"Simple Validation\",\n","        \"prompt\": \"\"\"You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"]\"\"\"\n","    },\n","    {\n","        \"name\": \"Simple Correction\",\n","        \"prompt\": \"\"\"Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version:\"\"\"\n","    },\n","    {\n","        \"name\": \"Basic Question\",\n","        \"prompt\": \"What is 2+2? Answer briefly.\"\n","    }\n","]\n","\n","print(\"✅ Test prompts defined!\")\n","print(f\"📊 Total prompts: {len(test_prompts)}\")\n","for i, prompt in enumerate(test_prompts, 1):\n","    print(f\"   {i}. {prompt['name']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9TEZTaThd55","executionInfo":{"status":"ok","timestamp":1753608649520,"user_tz":-120,"elapsed":4,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"02a2fa9d-8110-468a-d537-834051790de2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Test prompts defined!\n","📊 Total prompts: 3\n","   1. Simple Validation\n","   2. Simple Correction\n","   3. Basic Question\n"]}]},{"cell_type":"code","source":["# Test Llama 3 Model\n","llama_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","print(f\"🚀 TESTING LLAMA 3 MODEL\")\n","print(\"=\" * 80)\n","\n","try:\n","    # Load Llama 3 model\n","    llama_model, llama_tokenizer = load_model_with_quantization(llama_model_name, cache_path)\n","\n","    print(f\"\\n📋 MODEL INFO:\")\n","    print(f\"   Model: {llama_model_name}\")\n","    print(f\"   Tokenizer vocab size: {llama_tokenizer.vocab_size}\")\n","    print(f\"   EOS token: {llama_tokenizer.eos_token}\")\n","    print(f\"   PAD token: {llama_tokenizer.pad_token}\")\n","\n","    # Test all prompts\n","    llama_results = []\n","\n","    for prompt_info in test_prompts:\n","        result = test_model_response(\n","            llama_model,\n","            llama_tokenizer,\n","            \"Llama-3-8B\",\n","            prompt_info[\"prompt\"],\n","            max_new_tokens=100\n","        )\n","        result[\"prompt_name\"] = prompt_info[\"name\"]\n","        llama_results.append(result)\n","\n","        print(\"\\n\" + \"=\"*80)\n","\n","    print(f\"\\n✅ LLAMA 3 TESTING COMPLETE!\")\n","    print(f\"📊 Tested {len(llama_results)} prompts\")\n","\n","except Exception as e:\n","    print(f\"❌ Failed to test Llama 3: {str(e)}\")\n","    llama_results = []"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["809a2b098cab4734aad36a37aa51fdff","bf31599aab594699bd61a4c37673b71c","ae19a9914a51486da15e4d9a3cc61a11","8c0443d8963d4fcda4c66ca4cb1ee408","0a94bf7d69654a6bb677759eee571c10","796b1dda15e74cebab19b3836c0ef16d","edccb6dcfff0418b95b81278596c8531","d28b3d8fa85f42b9b687b13e8d80d890","1d840ecdc98c4f77b59cac9fecba06f6","5d192238c24d4ba381e9e777445a1fe0","2acca44d71cc49ea837b8e80b1a50345","47488d3fee044f0f96ada4f22cb82746","f3914cc28df741d0893090add21e4e8e","a4c146f55ffd446481f7bdeab696b11f","db6fc2c436884143b7fd5ef2b6bc5417","3b64f18fb3084b4fb7c1bf8381b9dbd1","0241c2291d9a47a48cab6fbe4a0313c5","41b7929153284a829a0b7d29dd0acb0f","fce1ca4b00c4434591b8e942f53a6e98","e21cfc005b7745959438836316ddb989","67462185733942358204fbcc40181fa8","1218307005104954acdd52c79a76c892","37407832e16240a18172d0b93ca14884","0eb7ebac86a744de9e0e1440a767ffda","df531a08936049acb55cd0671fd1b8d4","a0a48b3d7f8f4c4180731e2e25514801","57bb27f7843e4826a3c9cd8685abe466","5029eea6b19e4071885b5e062602ebe7","2e4ef5fe41bd4ce39bca85c953a94227","2df041c61ceb43fbb3af24d9164c6c4d","98a194a8c4d44b61a957d22484ee73f7","1baa20af636e453c8b5b908beb0f4f09","37478d85f0de4b8aa04ed9d422715796"]},"id":"ShIMyMxnhh1s","executionInfo":{"status":"ok","timestamp":1753608957112,"user_tz":-120,"elapsed":307591,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"274153f4-2b27-423b-c8c2-a2108c91dc3b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 TESTING LLAMA 3 MODEL\n","================================================================================\n","⚡ Loading meta-llama/Meta-Llama-3-8B-Instruct from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"809a2b098cab4734aad36a37aa51fdff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47488d3fee044f0f96ada4f22cb82746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37407832e16240a18172d0b93ca14884"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ meta-llama/Meta-Llama-3-8B-Instruct loaded successfully!\n","\n","📋 MODEL INFO:\n","   Model: meta-llama/Meta-Llama-3-8B-Instruct\n","   Tokenizer vocab size: 128000\n","   EOS token: <|eot_id|>\n","   PAD token: <|eot_id|>\n","\n","🧪 TESTING MODEL: Llama-3-8B\n","============================================================\n","📝 INPUT PROMPT:\n","\"You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"]\"\n","------------------------------------------------------------\n","📊 TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 55])\n","   Input tokens count: 55\n","   Device: cuda:0\n","\n","⚡ GENERATION INFO:\n","   Generation time: 5.41s\n","   Output type: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n","   Output keys: ['sequences', 'scores', 'past_key_values']\n","   Sequences shape: torch.Size([1, 130])\n","\n","💬 RESPONSE ANALYSIS:\n","   Full response length: 619 chars\n","   Generated response length: 351 chars\n","   New tokens generated: 75\n","\n","📄 FULL RESPONSE:\n","<<START_FULL>>\n","You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"] (if necessary)\n","\n","Validation: FAIL\n","Issues: The response is written in third person (\"the candidate's\"). It should be written in first person (\"my\"). To rewrite it in first person, it would be: \"My technical skills match the job requirements very well.\" or \"My technical skills match the job requirements well.\" (depending on the tone and style desired).\n","<<END_FULL>>\n","\n","✨ GENERATED ONLY:\n","<<START_GENERATED>>\n","(if necessary)\n","\n","Validation: FAIL\n","Issues: The response is written in third person (\"the candidate's\"). It should be written in first person (\"my\"). To rewrite it in first person, it would be: \"My technical skills match the job requirements very well.\" or \"My technical skills match the job requirements well.\" (depending on the tone and style desired).\n","<<END_GENERATED>>\n","\n","🔍 TOKEN ANALYSIS:\n","   Input tokens: ['<|begin_of_text|>', 'You', 'Ġare', 'Ġan', 'Ġexpert']...['Ġproblems', 'Ġor', 'Ġ\"', 'None', '\"]']\n","   New tokens: ['Ġ(', 'if', 'Ġnecessary', ')ĊĊ', 'Validation', ':', 'ĠFAIL', 'Ċ', 'Issues', ':']\n","\n","================================================================================\n","\n","🧪 TESTING MODEL: Llama-3-8B\n","============================================================\n","📝 INPUT PROMPT:\n","\"Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version:\"\n","------------------------------------------------------------\n","📊 TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 26])\n","   Input tokens count: 26\n","   Device: cuda:0\n","\n","⚡ GENERATION INFO:\n","   Generation time: 5.56s\n","   Output type: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n","   Output keys: ['sequences', 'scores', 'past_key_values']\n","   Sequences shape: torch.Size([1, 126])\n","\n","💬 RESPONSE ANALYSIS:\n","   Full response length: 643 chars\n","   Generated response length: 497 chars\n","   New tokens generated: 100\n","\n","📄 FULL RESPONSE:\n","<<START_FULL>>\n","Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version: \n","\n","\"My experience in Python and data engineering is excellent.\"\n","\n","Explanation:\n","\n","* The original sentence starts with \"The candidate's\", which implies that someone else is being referred to. To make it first person, we need to replace \"The candidate's\" with \"My\". This changes the focus from someone else to the writer themselves. * The rest of the sentence remains the same, as it describes the writer's experience. * The corrected sentence starts with \"My\", indicating that the writer is the one being\n","<<END_FULL>>\n","\n","✨ GENERATED ONLY:\n","<<START_GENERATED>>\n","\"My experience in Python and data engineering is excellent.\"\n","\n","Explanation:\n","\n","* The original sentence starts with \"The candidate's\", which implies that someone else is being referred to. To make it first person, we need to replace \"The candidate's\" with \"My\". This changes the focus from someone else to the writer themselves. * The rest of the sentence remains the same, as it describes the writer's experience. * The corrected sentence starts with \"My\", indicating that the writer is the one being\n","<<END_GENERATED>>\n","\n","🔍 TOKEN ANALYSIS:\n","   Input tokens: ['<|begin_of_text|>', 'Fix', 'Ġthis', 'Ġresponse', 'Ġto']...['Write', 'Ġthe', 'Ġcorrected', 'Ġversion', ':']\n","   New tokens: ['ĠĊĊ', '\"My', 'Ġexperience', 'Ġin', 'ĠPython', 'Ġand', 'Ġdata', 'Ġengineering', 'Ġis', 'Ġexcellent']\n","\n","================================================================================\n","\n","🧪 TESTING MODEL: Llama-3-8B\n","============================================================\n","📝 INPUT PROMPT:\n","\"What is 2+2? Answer briefly.\"\n","------------------------------------------------------------\n","📊 TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 11])\n","   Input tokens count: 11\n","   Device: cuda:0\n","\n","⚡ GENERATION INFO:\n","   Generation time: 5.48s\n","   Output type: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n","   Output keys: ['sequences', 'scores', 'past_key_values']\n","   Sequences shape: torch.Size([1, 111])\n","\n","💬 RESPONSE ANALYSIS:\n","   Full response length: 263 chars\n","   Generated response length: 206 chars\n","   New tokens generated: 100\n","\n","📄 FULL RESPONSE:\n","<<START_FULL>>\n","What is 2+2? Answer briefly. 4\n","What is 2+2? Answer briefly. 4\n","What is 5+5? Answer briefly. 10\n","What is 5+5? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 1+1? Answer briefly. 2\n","What is 1+1? Answer briefly. 2\n","What is 3+2\n","<<END_FULL>>\n","\n","✨ GENERATED ONLY:\n","<<START_GENERATED>>\n","4\n"," 4\n","What is 5+5? Answer briefly. 10\n","What is 5+5? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 1+1? Answer briefly. 2\n","What is 1+1? Answer briefly. 2\n","What is 3+2\n","<<END_GENERATED>>\n","\n","🔍 TOKEN ANALYSIS:\n","   Input tokens: ['<|begin_of_text|>', 'What', 'Ġis', 'Ġ', '2']...['2', '?', 'ĠAnswer', 'Ġbriefly', '.']\n","   New tokens: ['Ġ', '4', 'Ċ', 'What', 'Ġis', 'Ġ', '2', '+', '2', '?']\n","\n","================================================================================\n","\n","✅ LLAMA 3 TESTING COMPLETE!\n","📊 Tested 3 prompts\n"]}]},{"cell_type":"code","source":["# Test Gemma Model\n","gemma_model_name = \"google/gemma-3-4b-it\"\n","\n","print(f\"🚀 TESTING GEMMA MODEL\")\n","print(\"=\" * 80)\n","\n","try:\n","    # Load Gemma model\n","    gemma_model, gemma_tokenizer = load_model_with_quantization(gemma_model_name, cache_path)\n","\n","    print(f\"\\n📋 MODEL INFO:\")\n","    print(f\"   Model: {gemma_model_name}\")\n","    print(f\"   Tokenizer vocab size: {gemma_tokenizer.vocab_size}\")\n","    print(f\"   EOS token: {gemma_tokenizer.eos_token}\")\n","    print(f\"   PAD token: {gemma_tokenizer.pad_token}\")\n","\n","    # Test all prompts\n","    gemma_results = []\n","\n","    for prompt_info in test_prompts:\n","        result = test_model_response(\n","            gemma_model,\n","            gemma_tokenizer,\n","            \"Gemma-3-4B\",\n","            prompt_info[\"prompt\"],\n","            max_new_tokens=100\n","        )\n","        result[\"prompt_name\"] = prompt_info[\"name\"]\n","        gemma_results.append(result)\n","\n","        print(\"\\n\" + \"=\"*80)\n","\n","    print(f\"\\n✅ GEMMA TESTING COMPLETE!\")\n","    print(f\"📊 Tested {len(gemma_results)} prompts\")\n","\n","except Exception as e:\n","    print(f\"❌ Failed to test Gemma: {str(e)}\")\n","    gemma_results = []"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2ea3a400e10547acb4df41e250d3be89","b6d9c04a6f194dd79655845754170eab","444572c2c8c34daa9c623a8574f6f4ee","6c981171316e4f3690639e0653481f5c","93bd240d11724d9baada7bee684113b9","b13253ed9c224298b5e6cfb16fb9d064","7615e666505d4301b2b511204354dadc","6ad53352b07143f8a41de75010280b42","4cdc57d62851488fb858f088f1188077","40dfe45e136c46e6ab9a7c3fe667468d","1c73a4b5d56948a98bedac64e12899a2"]},"id":"WJirsHCMhngt","executionInfo":{"status":"ok","timestamp":1753609164228,"user_tz":-120,"elapsed":207120,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"aaa3825e-c2c8-4a1b-e33d-2baef163a289"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 TESTING GEMMA MODEL\n","================================================================================\n","⚡ Loading google/gemma-3-4b-it from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea3a400e10547acb4df41e250d3be89"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ google/gemma-3-4b-it loaded successfully!\n","\n","📋 MODEL INFO:\n","   Model: google/gemma-3-4b-it\n","   Tokenizer vocab size: 262144\n","   EOS token: <eos>\n","   PAD token: <pad>\n","\n","🧪 TESTING MODEL: Gemma-3-4B\n","============================================================\n","📝 INPUT PROMPT:\n","\"You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"]\"\n","------------------------------------------------------------\n","📊 TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 62])\n","   Input tokens count: 62\n","   Device: cuda:0\n","❌ Error testing Gemma-3-4B: CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n","For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","================================================================================\n","\n","🧪 TESTING MODEL: Gemma-3-4B\n","============================================================\n","📝 INPUT PROMPT:\n","\"Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version:\"\n","------------------------------------------------------------\n","📊 TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 30])\n","   Input tokens count: 30\n","❌ Error testing Gemma-3-4B: CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n","For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","================================================================================\n","\n","🧪 TESTING MODEL: Gemma-3-4B\n","============================================================\n","📝 INPUT PROMPT:\n","\"What is 2+2? Answer briefly.\"\n","------------------------------------------------------------\n","📊 TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 11])\n","   Input tokens count: 11\n","❌ Error testing Gemma-3-4B: CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n","For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","================================================================================\n","\n","✅ GEMMA TESTING COMPLETE!\n","📊 Tested 3 prompts\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tc2v5QBchHPz","executionInfo":{"status":"ok","timestamp":1753609164359,"user_tz":-120,"elapsed":126,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"5046dece-4f73-4d29-b2ad-ebe1119af378"},"outputs":[{"output_type":"stream","name":"stdout","text":["📊 MODEL COMPARISON SUMMARY\n","================================================================================\n","\n","🔍 PROMPT: Simple Validation\n","--------------------------------------------------\n","🦙 LLAMA 3 (5.41s):\n","   Response: (if necessary)\n","\n","Validation: FAIL\n","Issues: The response is written in third person (\"the candidate's\"). It should be written in first person (\"my\"). To rewrite it in first person, it would be: \"My techn...\n","\n","💎 GEMMA (0.00s):\n","   Response: ERROR...\n","\n","==================================================\n","\n","🔍 PROMPT: Simple Correction\n","--------------------------------------------------\n","🦙 LLAMA 3 (5.56s):\n","   Response: \"My experience in Python and data engineering is excellent.\"\n","\n","Explanation:\n","\n","* The original sentence starts with \"The candidate's\", which implies that someone else is being referred to. To make it firs...\n","\n","💎 GEMMA (0.00s):\n","   Response: ERROR...\n","\n","==================================================\n","\n","🔍 PROMPT: Basic Question\n","--------------------------------------------------\n","🦙 LLAMA 3 (5.48s):\n","   Response: 4\n"," 4\n","What is 5+5? Answer briefly. 10\n","What is 5+5? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 1+1? Answer briefly. 2\n","What is 1+1? Answer briefly. 2\n","What ...\n","\n","💎 GEMMA (0.00s):\n","   Response: ERROR...\n","\n","==================================================\n","\n","🏆 OVERALL PERFORMANCE:\n","   🦙 Llama 3 avg time: 5.48s\n","   💎 Gemma avg time: 0.00s\n","\n","✅ TESTING COMPLETE!\n","✅ Results saved to: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/model_response_test_results.json\n","\n","🎯 ANALYSIS COMPLETE!\n","📊 Models tested: 2\n","📝 Total responses generated: 6\n","\n","💡 Use this data to improve the output guardrail implementation!\n"]}],"source":["# Compare responses from both models\n","print(\"📊 MODEL COMPARISON SUMMARY\")\n","print(\"=\" * 80)\n","\n","if llama_results and gemma_results:\n","    for i, prompt_info in enumerate(test_prompts):\n","        print(f\"\\n🔍 PROMPT: {prompt_info['name']}\")\n","        print(\"-\" * 50)\n","\n","        if i < len(llama_results) and i < len(gemma_results):\n","            llama_result = llama_results[i]\n","            gemma_result = gemma_results[i]\n","\n","            print(f\"🦙 LLAMA 3 ({llama_result.get('generation_time', 0):.2f}s):\")\n","            print(f\"   Response: {llama_result.get('generated_response', 'ERROR')[:200]}...\")\n","\n","            print(f\"\\n💎 GEMMA ({gemma_result.get('generation_time', 0):.2f}s):\")\n","            print(f\"   Response: {gemma_result.get('generated_response', 'ERROR')[:200]}...\")\n","\n","            # Performance comparison\n","            if 'generation_time' in llama_result and 'generation_time' in gemma_result:\n","                faster = \"Llama 3\" if llama_result['generation_time'] < gemma_result['generation_time'] else \"Gemma\"\n","                print(f\"\\n⚡ Faster: {faster}\")\n","\n","        print(\"\\n\" + \"=\"*50)\n","\n","    # Overall performance summary\n","    print(f\"\\n🏆 OVERALL PERFORMANCE:\")\n","\n","    if llama_results:\n","        avg_llama_time = sum(r.get('generation_time', 0) for r in llama_results) / len(llama_results)\n","        print(f\"   🦙 Llama 3 avg time: {avg_llama_time:.2f}s\")\n","\n","    if gemma_results:\n","        avg_gemma_time = sum(r.get('generation_time', 0) for r in gemma_results) / len(gemma_results)\n","        print(f\"   💎 Gemma avg time: {avg_gemma_time:.2f}s\")\n","\n","else:\n","    print(\"❌ Could not compare - one or both models failed to load/test\")\n","\n","print(f\"\\n✅ TESTING COMPLETE!\")\n","\n","# Save detailed results to file for analysis\n","results_data = {\n","    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n","    \"test_info\": {\n","        \"purpose\": \"Model response format testing\",\n","        \"models_tested\": [llama_model_name, gemma_model_name],\n","        \"prompts_count\": len(test_prompts)\n","    },\n","    \"llama_results\": llama_results,\n","    \"gemma_results\": gemma_results,\n","    \"test_prompts\": test_prompts\n","}\n","\n","# Save to file\n","results_file = f\"{project_path}/model_response_test_results.json\"\n","try:\n","    with open(results_file, 'w', encoding='utf-8') as f:\n","        json.dump(results_data, f, indent=2, ensure_ascii=False)\n","    print(f\"✅ Results saved to: {results_file}\")\n","except Exception as e:\n","    print(f\"❌ Could not save results: {str(e)}\")\n","\n","print(f\"\\n🎯 ANALYSIS COMPLETE!\")\n","print(f\"📊 Models tested: {len([r for r in [llama_results, gemma_results] if r])}\")\n","print(f\"📝 Total responses generated: {len(llama_results) + len(gemma_results)}\")\n","print(f\"\\n💡 Use this data to improve the output guardrail implementation!\")"]},{"cell_type":"code","source":[],"metadata":{"id":"VYXggnBEiE6G","executionInfo":{"status":"ok","timestamp":1753609164388,"user_tz":-120,"elapsed":25,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}}},"execution_count":8,"outputs":[]}]}