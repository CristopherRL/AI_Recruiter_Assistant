{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOdeJjLovMWAC9Q6gLZUW3q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"809a2b098cab4734aad36a37aa51fdff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf31599aab594699bd61a4c37673b71c","IPY_MODEL_ae19a9914a51486da15e4d9a3cc61a11","IPY_MODEL_8c0443d8963d4fcda4c66ca4cb1ee408"],"layout":"IPY_MODEL_0a94bf7d69654a6bb677759eee571c10"}},"bf31599aab594699bd61a4c37673b71c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_796b1dda15e74cebab19b3836c0ef16d","placeholder":"‚Äã","style":"IPY_MODEL_edccb6dcfff0418b95b81278596c8531","value":"Fetching‚Äá4‚Äáfiles:‚Äá100%"}},"ae19a9914a51486da15e4d9a3cc61a11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d28b3d8fa85f42b9b687b13e8d80d890","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d840ecdc98c4f77b59cac9fecba06f6","value":4}},"8c0443d8963d4fcda4c66ca4cb1ee408":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d192238c24d4ba381e9e777445a1fe0","placeholder":"‚Äã","style":"IPY_MODEL_2acca44d71cc49ea837b8e80b1a50345","value":"‚Äá4/4‚Äá[01:22&lt;00:00,‚Äá41.24s/it]"}},"0a94bf7d69654a6bb677759eee571c10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"796b1dda15e74cebab19b3836c0ef16d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edccb6dcfff0418b95b81278596c8531":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d28b3d8fa85f42b9b687b13e8d80d890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d840ecdc98c4f77b59cac9fecba06f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d192238c24d4ba381e9e777445a1fe0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2acca44d71cc49ea837b8e80b1a50345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47488d3fee044f0f96ada4f22cb82746":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3914cc28df741d0893090add21e4e8e","IPY_MODEL_a4c146f55ffd446481f7bdeab696b11f","IPY_MODEL_db6fc2c436884143b7fd5ef2b6bc5417"],"layout":"IPY_MODEL_3b64f18fb3084b4fb7c1bf8381b9dbd1"}},"f3914cc28df741d0893090add21e4e8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0241c2291d9a47a48cab6fbe4a0313c5","placeholder":"‚Äã","style":"IPY_MODEL_41b7929153284a829a0b7d29dd0acb0f","value":"model-00002-of-00004.safetensors:‚Äá100%"}},"a4c146f55ffd446481f7bdeab696b11f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fce1ca4b00c4434591b8e942f53a6e98","max":4999802720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e21cfc005b7745959438836316ddb989","value":4999802720}},"db6fc2c436884143b7fd5ef2b6bc5417":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67462185733942358204fbcc40181fa8","placeholder":"‚Äã","style":"IPY_MODEL_1218307005104954acdd52c79a76c892","value":"‚Äá5.00G/5.00G‚Äá[01:21&lt;00:00,‚Äá222MB/s]"}},"3b64f18fb3084b4fb7c1bf8381b9dbd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0241c2291d9a47a48cab6fbe4a0313c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b7929153284a829a0b7d29dd0acb0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fce1ca4b00c4434591b8e942f53a6e98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e21cfc005b7745959438836316ddb989":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67462185733942358204fbcc40181fa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1218307005104954acdd52c79a76c892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37407832e16240a18172d0b93ca14884":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0eb7ebac86a744de9e0e1440a767ffda","IPY_MODEL_df531a08936049acb55cd0671fd1b8d4","IPY_MODEL_a0a48b3d7f8f4c4180731e2e25514801"],"layout":"IPY_MODEL_57bb27f7843e4826a3c9cd8685abe466"}},"0eb7ebac86a744de9e0e1440a767ffda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5029eea6b19e4071885b5e062602ebe7","placeholder":"‚Äã","style":"IPY_MODEL_2e4ef5fe41bd4ce39bca85c953a94227","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"df531a08936049acb55cd0671fd1b8d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2df041c61ceb43fbb3af24d9164c6c4d","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98a194a8c4d44b61a957d22484ee73f7","value":4}},"a0a48b3d7f8f4c4180731e2e25514801":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1baa20af636e453c8b5b908beb0f4f09","placeholder":"‚Äã","style":"IPY_MODEL_37478d85f0de4b8aa04ed9d422715796","value":"‚Äá4/4‚Äá[03:10&lt;00:00,‚Äá41.74s/it]"}},"57bb27f7843e4826a3c9cd8685abe466":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5029eea6b19e4071885b5e062602ebe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e4ef5fe41bd4ce39bca85c953a94227":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2df041c61ceb43fbb3af24d9164c6c4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98a194a8c4d44b61a957d22484ee73f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1baa20af636e453c8b5b908beb0f4f09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37478d85f0de4b8aa04ed9d422715796":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ea3a400e10547acb4df41e250d3be89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6d9c04a6f194dd79655845754170eab","IPY_MODEL_444572c2c8c34daa9c623a8574f6f4ee","IPY_MODEL_6c981171316e4f3690639e0653481f5c"],"layout":"IPY_MODEL_93bd240d11724d9baada7bee684113b9"}},"b6d9c04a6f194dd79655845754170eab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b13253ed9c224298b5e6cfb16fb9d064","placeholder":"‚Äã","style":"IPY_MODEL_7615e666505d4301b2b511204354dadc","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"444572c2c8c34daa9c623a8574f6f4ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ad53352b07143f8a41de75010280b42","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4cdc57d62851488fb858f088f1188077","value":2}},"6c981171316e4f3690639e0653481f5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40dfe45e136c46e6ab9a7c3fe667468d","placeholder":"‚Äã","style":"IPY_MODEL_1c73a4b5d56948a98bedac64e12899a2","value":"‚Äá2/2‚Äá[03:17&lt;00:00,‚Äá97.29s/it]"}},"93bd240d11724d9baada7bee684113b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b13253ed9c224298b5e6cfb16fb9d064":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7615e666505d4301b2b511204354dadc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ad53352b07143f8a41de75010280b42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cdc57d62851488fb858f088f1188077":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40dfe45e136c46e6ab9a7c3fe667468d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c73a4b5d56948a98bedac64e12899a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Test Models Response Format - Google Colab Version\n","# Copy and paste this code into a Google Colab cell\n","\n","# Install required packages\n","!pip install transformers>=4.36.0 torch>=2.0.0 bitsandbytes>=0.41.0 accelerate>=0.24.0\n","\n","# Core imports\n","import os, json, torch, time\n","from typing import Dict, Any\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","print(\"‚úÖ All dependencies loaded!\")\n","print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klxX_X8shOt8","executionInfo":{"status":"ok","timestamp":1753608537226,"user_tz":-120,"elapsed":92054,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"b27c4dbf-239c-45c6-c0d8-50277e581f51"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ All dependencies loaded!\n","üî• CUDA available: True\n","üéÆ GPU: NVIDIA A100-SXM4-40GB\n","üíæ GPU Memory: 42.5 GB\n"]}]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up project paths for caching\n","project_path = \"/content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\"\n","cache_path = f\"{project_path}/huggingface_cache\"\n","\n","# Create directories\n","os.makedirs(cache_path, exist_ok=True)\n","\n","print(f\"‚úÖ Google Drive mounted\")\n","print(f\"üìÅ Project path: {project_path}\")\n","print(f\"üóÇÔ∏è Cache path: {cache_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCgilKephQ8i","executionInfo":{"status":"ok","timestamp":1753608649435,"user_tz":-120,"elapsed":112202,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"282c6429-3305-4a3c-ba94-3f2f83fa6574"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ Google Drive mounted\n","üìÅ Project path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant\n","üóÇÔ∏è Cache path: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/huggingface_cache\n"]}]},{"cell_type":"code","source":["\n","# Model loading functions\n","def check_cached_models(cache_path):\n","    \"\"\"Check for cached models\"\"\"\n","    cached_models = []\n","    if not os.path.exists(cache_path):\n","        return cached_models\n","\n","    try:\n","        items = os.listdir(cache_path)\n","        for item in items:\n","            item_path = os.path.join(cache_path, item)\n","            if os.path.isdir(item_path) and not item.startswith('.'):\n","                try:\n","                    contents = os.listdir(item_path)\n","                    has_models_folder = any(f.startswith('models--') for f in contents if os.path.isdir(os.path.join(item_path, f)))\n","                    if has_models_folder:\n","                        cached_models.append(item)\n","                except Exception:\n","                    continue\n","    except Exception:\n","        pass\n","\n","    return cached_models\n","\n","def create_model_cache_dir(model_name: str, cache_path: str) -> str:\n","    \"\"\"Create clean cache directory for a model\"\"\"\n","    model_folder = model_name.replace('/', ' ')\n","    model_cache_dir = os.path.join(cache_path, model_folder)\n","    os.makedirs(model_cache_dir, exist_ok=True)\n","    return model_cache_dir\n","\n","def load_model_with_quantization(model_name: str, cache_path: str):\n","    \"\"\"Load model with cache detection and quantization\"\"\"\n","    cached_models = check_cached_models(cache_path)\n","    model_folder_space = model_name.replace('/', ' ')\n","\n","    if model_folder_space in cached_models:\n","        print(f\"‚ö° Loading {model_name} from cache...\")\n","        model_cache_dir = os.path.join(cache_path, model_folder_space)\n","    else:\n","        print(f\"üì• Downloading {model_name}...\")\n","        model_cache_dir = create_model_cache_dir(model_name, cache_path)\n","\n","    # Quantization for efficiency\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16,\n","        bnb_4bit_use_double_quant=False\n","    )\n","\n","    try:\n","        # Load tokenizer\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n","        if tokenizer.pad_token is None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","        # Load model\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            quantization_config=bnb_config,\n","            device_map=\"auto\",\n","            trust_remote_code=True,\n","            cache_dir=model_cache_dir\n","        )\n","\n","        print(f\"‚úÖ {model_name} loaded successfully!\")\n","        return model, tokenizer\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error loading {model_name}: {str(e)}\")\n","        raise\n","\n","print(\"‚úÖ Model loading functions ready!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHvU-rtvhT-R","executionInfo":{"status":"ok","timestamp":1753608649461,"user_tz":-120,"elapsed":9,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"e5323c25-5452-4fd4-c667-9266b51492c3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Model loading functions ready!\n"]}]},{"cell_type":"code","source":["# Test response generation function\n","def test_model_response(model, tokenizer, model_name: str, prompt: str, max_new_tokens: int = 150):\n","    \"\"\"Test model response and show detailed structure\"\"\"\n","    print(f\"\\nüß™ TESTING MODEL: {model_name}\")\n","    print(\"=\" * 60)\n","    print(f\"üìù INPUT PROMPT:\")\n","    print(f'\"{prompt}\"')\n","    print(\"-\" * 60)\n","\n","    try:\n","        # Tokenize input\n","        inputs = tokenizer(\n","            prompt,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=1024\n","        )\n","\n","        print(f\"üìä TOKENIZATION INFO:\")\n","        print(f\"   Input IDs shape: {inputs['input_ids'].shape}\")\n","        print(f\"   Input tokens count: {inputs['input_ids'].shape[1]}\")\n","\n","        # Move to device\n","        device = next(model.parameters()).device\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        print(f\"   Device: {device}\")\n","\n","        # Generate response\n","        start_time = time.time()\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_p=0.9,\n","                pad_token_id=tokenizer.eos_token_id,\n","                return_dict_in_generate=True,\n","                output_scores=True\n","            )\n","\n","        generation_time = time.time() - start_time\n","\n","        print(f\"\\n‚ö° GENERATION INFO:\")\n","        print(f\"   Generation time: {generation_time:.2f}s\")\n","        print(f\"   Output type: {type(outputs)}\")\n","        print(f\"   Output keys: {list(outputs.keys()) if hasattr(outputs, 'keys') else 'No keys (tensor)'}\")\n","\n","        # Extract sequences\n","        if hasattr(outputs, 'sequences'):\n","            sequences = outputs.sequences\n","            print(f\"   Sequences shape: {sequences.shape}\")\n","        else:\n","            sequences = outputs\n","            print(f\"   Sequences shape: {sequences.shape}\")\n","\n","        # Decode full response\n","        full_response = tokenizer.decode(sequences[0], skip_special_tokens=True)\n","\n","        # Extract only new tokens (response without prompt)\n","        generated_response = full_response.replace(prompt, \"\").strip()\n","\n","        print(f\"\\nüí¨ RESPONSE ANALYSIS:\")\n","        print(f\"   Full response length: {len(full_response)} chars\")\n","        print(f\"   Generated response length: {len(generated_response)} chars\")\n","        print(f\"   New tokens generated: {sequences.shape[1] - inputs['input_ids'].shape[1]}\")\n","\n","        print(f\"\\nüìÑ FULL RESPONSE:\")\n","        print(\"<<START_FULL>>\")\n","        print(full_response)\n","        print(\"<<END_FULL>>\")\n","\n","        print(f\"\\n‚ú® GENERATED ONLY:\")\n","        print(\"<<START_GENERATED>>\")\n","        print(generated_response)\n","        print(\"<<END_GENERATED>>\")\n","\n","        # Show token structure for debugging\n","        print(f\"\\nüîç TOKEN ANALYSIS:\")\n","        input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","        output_tokens = tokenizer.convert_ids_to_tokens(sequences[0])\n","        new_tokens = output_tokens[len(input_tokens):]\n","\n","        print(f\"   Input tokens: {input_tokens[:5]}...{input_tokens[-5:]}\")\n","        print(f\"   New tokens: {new_tokens[:10]}\")\n","\n","        # Return structured result\n","        return {\n","            \"model_name\": model_name,\n","            \"prompt\": prompt,\n","            \"full_response\": full_response,\n","            \"generated_response\": generated_response,\n","            \"generation_time\": generation_time,\n","            \"input_tokens_count\": inputs['input_ids'].shape[1],\n","            \"output_tokens_count\": sequences.shape[1],\n","            \"new_tokens_count\": sequences.shape[1] - inputs['input_ids'].shape[1],\n","            \"device\": str(device)\n","        }\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error testing {model_name}: {str(e)}\")\n","        return {\"error\": str(e)}\n","\n","print(\"‚úÖ Response testing function ready!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9ILp5ophbrl","executionInfo":{"status":"ok","timestamp":1753608649515,"user_tz":-120,"elapsed":52,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"2732ed6a-0bec-4017-9b82-6fd2105d0522"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Response testing function ready!\n"]}]},{"cell_type":"code","source":["# Define test prompts\n","test_prompts = [\n","    {\n","        \"name\": \"Simple Validation\",\n","        \"prompt\": \"\"\"You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"]\"\"\"\n","    },\n","    {\n","        \"name\": \"Simple Correction\",\n","        \"prompt\": \"\"\"Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version:\"\"\"\n","    },\n","    {\n","        \"name\": \"Basic Question\",\n","        \"prompt\": \"What is 2+2? Answer briefly.\"\n","    }\n","]\n","\n","print(\"‚úÖ Test prompts defined!\")\n","print(f\"üìä Total prompts: {len(test_prompts)}\")\n","for i, prompt in enumerate(test_prompts, 1):\n","    print(f\"   {i}. {prompt['name']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9TEZTaThd55","executionInfo":{"status":"ok","timestamp":1753608649520,"user_tz":-120,"elapsed":4,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"02a2fa9d-8110-468a-d537-834051790de2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Test prompts defined!\n","üìä Total prompts: 3\n","   1. Simple Validation\n","   2. Simple Correction\n","   3. Basic Question\n"]}]},{"cell_type":"code","source":["# Test Llama 3 Model\n","llama_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","print(f\"üöÄ TESTING LLAMA 3 MODEL\")\n","print(\"=\" * 80)\n","\n","try:\n","    # Load Llama 3 model\n","    llama_model, llama_tokenizer = load_model_with_quantization(llama_model_name, cache_path)\n","\n","    print(f\"\\nüìã MODEL INFO:\")\n","    print(f\"   Model: {llama_model_name}\")\n","    print(f\"   Tokenizer vocab size: {llama_tokenizer.vocab_size}\")\n","    print(f\"   EOS token: {llama_tokenizer.eos_token}\")\n","    print(f\"   PAD token: {llama_tokenizer.pad_token}\")\n","\n","    # Test all prompts\n","    llama_results = []\n","\n","    for prompt_info in test_prompts:\n","        result = test_model_response(\n","            llama_model,\n","            llama_tokenizer,\n","            \"Llama-3-8B\",\n","            prompt_info[\"prompt\"],\n","            max_new_tokens=100\n","        )\n","        result[\"prompt_name\"] = prompt_info[\"name\"]\n","        llama_results.append(result)\n","\n","        print(\"\\n\" + \"=\"*80)\n","\n","    print(f\"\\n‚úÖ LLAMA 3 TESTING COMPLETE!\")\n","    print(f\"üìä Tested {len(llama_results)} prompts\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Failed to test Llama 3: {str(e)}\")\n","    llama_results = []"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["809a2b098cab4734aad36a37aa51fdff","bf31599aab594699bd61a4c37673b71c","ae19a9914a51486da15e4d9a3cc61a11","8c0443d8963d4fcda4c66ca4cb1ee408","0a94bf7d69654a6bb677759eee571c10","796b1dda15e74cebab19b3836c0ef16d","edccb6dcfff0418b95b81278596c8531","d28b3d8fa85f42b9b687b13e8d80d890","1d840ecdc98c4f77b59cac9fecba06f6","5d192238c24d4ba381e9e777445a1fe0","2acca44d71cc49ea837b8e80b1a50345","47488d3fee044f0f96ada4f22cb82746","f3914cc28df741d0893090add21e4e8e","a4c146f55ffd446481f7bdeab696b11f","db6fc2c436884143b7fd5ef2b6bc5417","3b64f18fb3084b4fb7c1bf8381b9dbd1","0241c2291d9a47a48cab6fbe4a0313c5","41b7929153284a829a0b7d29dd0acb0f","fce1ca4b00c4434591b8e942f53a6e98","e21cfc005b7745959438836316ddb989","67462185733942358204fbcc40181fa8","1218307005104954acdd52c79a76c892","37407832e16240a18172d0b93ca14884","0eb7ebac86a744de9e0e1440a767ffda","df531a08936049acb55cd0671fd1b8d4","a0a48b3d7f8f4c4180731e2e25514801","57bb27f7843e4826a3c9cd8685abe466","5029eea6b19e4071885b5e062602ebe7","2e4ef5fe41bd4ce39bca85c953a94227","2df041c61ceb43fbb3af24d9164c6c4d","98a194a8c4d44b61a957d22484ee73f7","1baa20af636e453c8b5b908beb0f4f09","37478d85f0de4b8aa04ed9d422715796"]},"id":"ShIMyMxnhh1s","executionInfo":{"status":"ok","timestamp":1753608957112,"user_tz":-120,"elapsed":307591,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"274153f4-2b27-423b-c8c2-a2108c91dc3b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ TESTING LLAMA 3 MODEL\n","================================================================================\n","‚ö° Loading meta-llama/Meta-Llama-3-8B-Instruct from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"809a2b098cab4734aad36a37aa51fdff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47488d3fee044f0f96ada4f22cb82746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37407832e16240a18172d0b93ca14884"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ meta-llama/Meta-Llama-3-8B-Instruct loaded successfully!\n","\n","üìã MODEL INFO:\n","   Model: meta-llama/Meta-Llama-3-8B-Instruct\n","   Tokenizer vocab size: 128000\n","   EOS token: <|eot_id|>\n","   PAD token: <|eot_id|>\n","\n","üß™ TESTING MODEL: Llama-3-8B\n","============================================================\n","üìù INPUT PROMPT:\n","\"You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"]\"\n","------------------------------------------------------------\n","üìä TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 55])\n","   Input tokens count: 55\n","   Device: cuda:0\n","\n","‚ö° GENERATION INFO:\n","   Generation time: 5.41s\n","   Output type: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n","   Output keys: ['sequences', 'scores', 'past_key_values']\n","   Sequences shape: torch.Size([1, 130])\n","\n","üí¨ RESPONSE ANALYSIS:\n","   Full response length: 619 chars\n","   Generated response length: 351 chars\n","   New tokens generated: 75\n","\n","üìÑ FULL RESPONSE:\n","<<START_FULL>>\n","You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"] (if necessary)\n","\n","Validation: FAIL\n","Issues: The response is written in third person (\"the candidate's\"). It should be written in first person (\"my\"). To rewrite it in first person, it would be: \"My technical skills match the job requirements very well.\" or \"My technical skills match the job requirements well.\" (depending on the tone and style desired).\n","<<END_FULL>>\n","\n","‚ú® GENERATED ONLY:\n","<<START_GENERATED>>\n","(if necessary)\n","\n","Validation: FAIL\n","Issues: The response is written in third person (\"the candidate's\"). It should be written in first person (\"my\"). To rewrite it in first person, it would be: \"My technical skills match the job requirements very well.\" or \"My technical skills match the job requirements well.\" (depending on the tone and style desired).\n","<<END_GENERATED>>\n","\n","üîç TOKEN ANALYSIS:\n","   Input tokens: ['<|begin_of_text|>', 'You', 'ƒ†are', 'ƒ†an', 'ƒ†expert']...['ƒ†problems', 'ƒ†or', 'ƒ†\"', 'None', '\"]']\n","   New tokens: ['ƒ†(', 'if', 'ƒ†necessary', ')ƒäƒä', 'Validation', ':', 'ƒ†FAIL', 'ƒä', 'Issues', ':']\n","\n","================================================================================\n","\n","üß™ TESTING MODEL: Llama-3-8B\n","============================================================\n","üìù INPUT PROMPT:\n","\"Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version:\"\n","------------------------------------------------------------\n","üìä TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 26])\n","   Input tokens count: 26\n","   Device: cuda:0\n","\n","‚ö° GENERATION INFO:\n","   Generation time: 5.56s\n","   Output type: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n","   Output keys: ['sequences', 'scores', 'past_key_values']\n","   Sequences shape: torch.Size([1, 126])\n","\n","üí¨ RESPONSE ANALYSIS:\n","   Full response length: 643 chars\n","   Generated response length: 497 chars\n","   New tokens generated: 100\n","\n","üìÑ FULL RESPONSE:\n","<<START_FULL>>\n","Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version: \n","\n","\"My experience in Python and data engineering is excellent.\"\n","\n","Explanation:\n","\n","* The original sentence starts with \"The candidate's\", which implies that someone else is being referred to. To make it first person, we need to replace \"The candidate's\" with \"My\". This changes the focus from someone else to the writer themselves. * The rest of the sentence remains the same, as it describes the writer's experience. * The corrected sentence starts with \"My\", indicating that the writer is the one being\n","<<END_FULL>>\n","\n","‚ú® GENERATED ONLY:\n","<<START_GENERATED>>\n","\"My experience in Python and data engineering is excellent.\"\n","\n","Explanation:\n","\n","* The original sentence starts with \"The candidate's\", which implies that someone else is being referred to. To make it first person, we need to replace \"The candidate's\" with \"My\". This changes the focus from someone else to the writer themselves. * The rest of the sentence remains the same, as it describes the writer's experience. * The corrected sentence starts with \"My\", indicating that the writer is the one being\n","<<END_GENERATED>>\n","\n","üîç TOKEN ANALYSIS:\n","   Input tokens: ['<|begin_of_text|>', 'Fix', 'ƒ†this', 'ƒ†response', 'ƒ†to']...['Write', 'ƒ†the', 'ƒ†corrected', 'ƒ†version', ':']\n","   New tokens: ['ƒ†ƒäƒä', '\"My', 'ƒ†experience', 'ƒ†in', 'ƒ†Python', 'ƒ†and', 'ƒ†data', 'ƒ†engineering', 'ƒ†is', 'ƒ†excellent']\n","\n","================================================================================\n","\n","üß™ TESTING MODEL: Llama-3-8B\n","============================================================\n","üìù INPUT PROMPT:\n","\"What is 2+2? Answer briefly.\"\n","------------------------------------------------------------\n","üìä TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 11])\n","   Input tokens count: 11\n","   Device: cuda:0\n","\n","‚ö° GENERATION INFO:\n","   Generation time: 5.48s\n","   Output type: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n","   Output keys: ['sequences', 'scores', 'past_key_values']\n","   Sequences shape: torch.Size([1, 111])\n","\n","üí¨ RESPONSE ANALYSIS:\n","   Full response length: 263 chars\n","   Generated response length: 206 chars\n","   New tokens generated: 100\n","\n","üìÑ FULL RESPONSE:\n","<<START_FULL>>\n","What is 2+2? Answer briefly. 4\n","What is 2+2? Answer briefly. 4\n","What is 5+5? Answer briefly. 10\n","What is 5+5? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 1+1? Answer briefly. 2\n","What is 1+1? Answer briefly. 2\n","What is 3+2\n","<<END_FULL>>\n","\n","‚ú® GENERATED ONLY:\n","<<START_GENERATED>>\n","4\n"," 4\n","What is 5+5? Answer briefly. 10\n","What is 5+5? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 1+1? Answer briefly. 2\n","What is 1+1? Answer briefly. 2\n","What is 3+2\n","<<END_GENERATED>>\n","\n","üîç TOKEN ANALYSIS:\n","   Input tokens: ['<|begin_of_text|>', 'What', 'ƒ†is', 'ƒ†', '2']...['2', '?', 'ƒ†Answer', 'ƒ†briefly', '.']\n","   New tokens: ['ƒ†', '4', 'ƒä', 'What', 'ƒ†is', 'ƒ†', '2', '+', '2', '?']\n","\n","================================================================================\n","\n","‚úÖ LLAMA 3 TESTING COMPLETE!\n","üìä Tested 3 prompts\n"]}]},{"cell_type":"code","source":["# Test Gemma Model\n","gemma_model_name = \"google/gemma-3-4b-it\"\n","\n","print(f\"üöÄ TESTING GEMMA MODEL\")\n","print(\"=\" * 80)\n","\n","try:\n","    # Load Gemma model\n","    gemma_model, gemma_tokenizer = load_model_with_quantization(gemma_model_name, cache_path)\n","\n","    print(f\"\\nüìã MODEL INFO:\")\n","    print(f\"   Model: {gemma_model_name}\")\n","    print(f\"   Tokenizer vocab size: {gemma_tokenizer.vocab_size}\")\n","    print(f\"   EOS token: {gemma_tokenizer.eos_token}\")\n","    print(f\"   PAD token: {gemma_tokenizer.pad_token}\")\n","\n","    # Test all prompts\n","    gemma_results = []\n","\n","    for prompt_info in test_prompts:\n","        result = test_model_response(\n","            gemma_model,\n","            gemma_tokenizer,\n","            \"Gemma-3-4B\",\n","            prompt_info[\"prompt\"],\n","            max_new_tokens=100\n","        )\n","        result[\"prompt_name\"] = prompt_info[\"name\"]\n","        gemma_results.append(result)\n","\n","        print(\"\\n\" + \"=\"*80)\n","\n","    print(f\"\\n‚úÖ GEMMA TESTING COMPLETE!\")\n","    print(f\"üìä Tested {len(gemma_results)} prompts\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Failed to test Gemma: {str(e)}\")\n","    gemma_results = []"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2ea3a400e10547acb4df41e250d3be89","b6d9c04a6f194dd79655845754170eab","444572c2c8c34daa9c623a8574f6f4ee","6c981171316e4f3690639e0653481f5c","93bd240d11724d9baada7bee684113b9","b13253ed9c224298b5e6cfb16fb9d064","7615e666505d4301b2b511204354dadc","6ad53352b07143f8a41de75010280b42","4cdc57d62851488fb858f088f1188077","40dfe45e136c46e6ab9a7c3fe667468d","1c73a4b5d56948a98bedac64e12899a2"]},"id":"WJirsHCMhngt","executionInfo":{"status":"ok","timestamp":1753609164228,"user_tz":-120,"elapsed":207120,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"aaa3825e-c2c8-4a1b-e33d-2baef163a289"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ TESTING GEMMA MODEL\n","================================================================================\n","‚ö° Loading google/gemma-3-4b-it from cache...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea3a400e10547acb4df41e250d3be89"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ google/gemma-3-4b-it loaded successfully!\n","\n","üìã MODEL INFO:\n","   Model: google/gemma-3-4b-it\n","   Tokenizer vocab size: 262144\n","   EOS token: <eos>\n","   PAD token: <pad>\n","\n","üß™ TESTING MODEL: Gemma-3-4B\n","============================================================\n","üìù INPUT PROMPT:\n","\"You are an expert at validating professional email responses.\n","\n","Check if this response is written in first person:\n","\"The candidate's technical skills match the job requirements very well.\"\n","\n","Respond with ONLY:\n","VALIDATION: [PASS or FAIL]\n","ISSUES: [List problems or \"None\"]\"\n","------------------------------------------------------------\n","üìä TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 62])\n","   Input tokens count: 62\n","   Device: cuda:0\n","‚ùå Error testing Gemma-3-4B: CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n","For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","================================================================================\n","\n","üß™ TESTING MODEL: Gemma-3-4B\n","============================================================\n","üìù INPUT PROMPT:\n","\"Fix this response to use first person:\n","\n","\"The candidate's experience in Python and data engineering is excellent.\"\n","\n","Write the corrected version:\"\n","------------------------------------------------------------\n","üìä TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 30])\n","   Input tokens count: 30\n","‚ùå Error testing Gemma-3-4B: CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n","For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","================================================================================\n","\n","üß™ TESTING MODEL: Gemma-3-4B\n","============================================================\n","üìù INPUT PROMPT:\n","\"What is 2+2? Answer briefly.\"\n","------------------------------------------------------------\n","üìä TOKENIZATION INFO:\n","   Input IDs shape: torch.Size([1, 11])\n","   Input tokens count: 11\n","‚ùå Error testing Gemma-3-4B: CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n","For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","================================================================================\n","\n","‚úÖ GEMMA TESTING COMPLETE!\n","üìä Tested 3 prompts\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tc2v5QBchHPz","executionInfo":{"status":"ok","timestamp":1753609164359,"user_tz":-120,"elapsed":126,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}},"outputId":"5046dece-4f73-4d29-b2ad-ebe1119af378"},"outputs":[{"output_type":"stream","name":"stdout","text":["üìä MODEL COMPARISON SUMMARY\n","================================================================================\n","\n","üîç PROMPT: Simple Validation\n","--------------------------------------------------\n","ü¶ô LLAMA 3 (5.41s):\n","   Response: (if necessary)\n","\n","Validation: FAIL\n","Issues: The response is written in third person (\"the candidate's\"). It should be written in first person (\"my\"). To rewrite it in first person, it would be: \"My techn...\n","\n","üíé GEMMA (0.00s):\n","   Response: ERROR...\n","\n","==================================================\n","\n","üîç PROMPT: Simple Correction\n","--------------------------------------------------\n","ü¶ô LLAMA 3 (5.56s):\n","   Response: \"My experience in Python and data engineering is excellent.\"\n","\n","Explanation:\n","\n","* The original sentence starts with \"The candidate's\", which implies that someone else is being referred to. To make it firs...\n","\n","üíé GEMMA (0.00s):\n","   Response: ERROR...\n","\n","==================================================\n","\n","üîç PROMPT: Basic Question\n","--------------------------------------------------\n","ü¶ô LLAMA 3 (5.48s):\n","   Response: 4\n"," 4\n","What is 5+5? Answer briefly. 10\n","What is 5+5? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 7+3? Answer briefly. 10\n","What is 1+1? Answer briefly. 2\n","What is 1+1? Answer briefly. 2\n","What ...\n","\n","üíé GEMMA (0.00s):\n","   Response: ERROR...\n","\n","==================================================\n","\n","üèÜ OVERALL PERFORMANCE:\n","   ü¶ô Llama 3 avg time: 5.48s\n","   üíé Gemma avg time: 0.00s\n","\n","‚úÖ TESTING COMPLETE!\n","‚úÖ Results saved to: /content/drive/MyDrive/Colab Notebooks/KEEPCODING/PROJECT/AI_Recruiter_Assistant/model_response_test_results.json\n","\n","üéØ ANALYSIS COMPLETE!\n","üìä Models tested: 2\n","üìù Total responses generated: 6\n","\n","üí° Use this data to improve the output guardrail implementation!\n"]}],"source":["# Compare responses from both models\n","print(\"üìä MODEL COMPARISON SUMMARY\")\n","print(\"=\" * 80)\n","\n","if llama_results and gemma_results:\n","    for i, prompt_info in enumerate(test_prompts):\n","        print(f\"\\nüîç PROMPT: {prompt_info['name']}\")\n","        print(\"-\" * 50)\n","\n","        if i < len(llama_results) and i < len(gemma_results):\n","            llama_result = llama_results[i]\n","            gemma_result = gemma_results[i]\n","\n","            print(f\"ü¶ô LLAMA 3 ({llama_result.get('generation_time', 0):.2f}s):\")\n","            print(f\"   Response: {llama_result.get('generated_response', 'ERROR')[:200]}...\")\n","\n","            print(f\"\\nüíé GEMMA ({gemma_result.get('generation_time', 0):.2f}s):\")\n","            print(f\"   Response: {gemma_result.get('generated_response', 'ERROR')[:200]}...\")\n","\n","            # Performance comparison\n","            if 'generation_time' in llama_result and 'generation_time' in gemma_result:\n","                faster = \"Llama 3\" if llama_result['generation_time'] < gemma_result['generation_time'] else \"Gemma\"\n","                print(f\"\\n‚ö° Faster: {faster}\")\n","\n","        print(\"\\n\" + \"=\"*50)\n","\n","    # Overall performance summary\n","    print(f\"\\nüèÜ OVERALL PERFORMANCE:\")\n","\n","    if llama_results:\n","        avg_llama_time = sum(r.get('generation_time', 0) for r in llama_results) / len(llama_results)\n","        print(f\"   ü¶ô Llama 3 avg time: {avg_llama_time:.2f}s\")\n","\n","    if gemma_results:\n","        avg_gemma_time = sum(r.get('generation_time', 0) for r in gemma_results) / len(gemma_results)\n","        print(f\"   üíé Gemma avg time: {avg_gemma_time:.2f}s\")\n","\n","else:\n","    print(\"‚ùå Could not compare - one or both models failed to load/test\")\n","\n","print(f\"\\n‚úÖ TESTING COMPLETE!\")\n","\n","# Save detailed results to file for analysis\n","results_data = {\n","    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n","    \"test_info\": {\n","        \"purpose\": \"Model response format testing\",\n","        \"models_tested\": [llama_model_name, gemma_model_name],\n","        \"prompts_count\": len(test_prompts)\n","    },\n","    \"llama_results\": llama_results,\n","    \"gemma_results\": gemma_results,\n","    \"test_prompts\": test_prompts\n","}\n","\n","# Save to file\n","results_file = f\"{project_path}/model_response_test_results.json\"\n","try:\n","    with open(results_file, 'w', encoding='utf-8') as f:\n","        json.dump(results_data, f, indent=2, ensure_ascii=False)\n","    print(f\"‚úÖ Results saved to: {results_file}\")\n","except Exception as e:\n","    print(f\"‚ùå Could not save results: {str(e)}\")\n","\n","print(f\"\\nüéØ ANALYSIS COMPLETE!\")\n","print(f\"üìä Models tested: {len([r for r in [llama_results, gemma_results] if r])}\")\n","print(f\"üìù Total responses generated: {len(llama_results) + len(gemma_results)}\")\n","print(f\"\\nüí° Use this data to improve the output guardrail implementation!\")"]},{"cell_type":"code","source":[],"metadata":{"id":"VYXggnBEiE6G","executionInfo":{"status":"ok","timestamp":1753609164388,"user_tz":-120,"elapsed":25,"user":{"displayName":"Cristopher Rojas","userId":"00177450930944324397"}}},"execution_count":8,"outputs":[]}]}